{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gmf import GMFEngine\n",
    "from mlp import MLPEngine\n",
    "from neumf import NeuMFEngine\n",
    "from data import SampleGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MovieID  CustomerID  Rating        Date\n",
      "0        1     1488844       3  2005-09-06\n",
      "1        1      822109       5  2005-05-13\n",
      "2        1      885013       4  2005-10-19\n",
      "3        1       30878       4  2005-12-26\n",
      "4        1      823519       3  2004-05-03\n"
     ]
    }
   ],
   "source": [
    "# def load_combined_data(base_path):\n",
    "#     data = []\n",
    "#     for i in range(1, 5):  # Loop through each of the combined_data files\n",
    "#         file_path = os.path.join(base_path, f'combined_data_{i}.txt')\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             movie_id = None\n",
    "#             for line in file:\n",
    "#                 line = line.strip()\n",
    "#                 if line.endswith(\":\"):\n",
    "#                     movie_id = int(line[:-1])\n",
    "#                 else:\n",
    "#                     customer_id, rating, date = line.split(\",\")\n",
    "#                     data.append([movie_id, int(customer_id), int(rating), date])\n",
    "\n",
    "#     df_sub = pd.DataFrame(data, columns=[\"MovieID\", \"CustomerID\", \"Rating\", \"Date\"])\n",
    "#     return df_sub\n",
    "\n",
    "###CURRENTLY JUST USING 1 COMBINED DATASET\n",
    "\n",
    "def load_combined_data(base_path):\n",
    "    data = []\n",
    "    with open(base_path, 'r') as file:\n",
    "        movie_id = None\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.endswith(\":\"):\n",
    "                movie_id = int(line[:-1])\n",
    "            else:\n",
    "                customer_id, rating, date = line.split(\",\")\n",
    "                data.append([movie_id, int(customer_id), int(rating), date])\n",
    "\n",
    "    df_sub = pd.DataFrame(data, columns=[\"MovieID\", \"CustomerID\", \"Rating\", \"Date\"])\n",
    "    return df_sub\n",
    "\n",
    "\n",
    "# Get the base directory path from the user\n",
    "base_path = '/Users/Ryan/Desktop/BT4222/combined_data_1.txt/combined_data_1.txt'\n",
    "\n",
    "# Load the data and create the DataFrame\n",
    "df = load_combined_data(base_path)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MovieID  CustomerID  rating     timestamp\n",
      "0        1     1488844       3  1.125965e+09\n",
      "1        1      822109       5  1.115942e+09\n",
      "2        1      885013       4  1.129680e+09\n",
      "3        1       30878       4  1.135555e+09\n",
      "4        1      823519       3  1.083542e+09\n"
     ]
    }
   ],
   "source": [
    "#Converting date into timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "# Convert 'Date' to timestamp and rename rating column\n",
    "df['timestamp'] = pd.to_datetime(df['Date']).apply(lambda x: x.timestamp())\n",
    "df = df.rename(columns={'Rating': 'rating'})\n",
    "\n",
    "# Drop the original 'Date' column\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# print(type(df['timestamp'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial user_id\n",
      "   CustomerID\n",
      "0     1488844\n",
      "1      822109\n",
      "2      885013\n",
      "3       30878\n",
      "4      823519\n",
      "final user_id\n",
      "   CustomerID  userId\n",
      "0     1488844       0\n",
      "1      822109       1\n",
      "2      885013       2\n",
      "3       30878       3\n",
      "4      823519       4\n",
      "initial df \n",
      "   MovieID  CustomerID  rating     timestamp  userId\n",
      "0        1     1488844       3  1.125965e+09       0\n",
      "1        1      822109       5  1.115942e+09       1\n",
      "2        1      885013       4  1.129680e+09       2\n",
      "3        1       30878       4  1.135555e+09       3\n",
      "4        1      823519       3  1.083542e+09       4\n",
      "initial item_id\n",
      "      MovieID\n",
      "0           1\n",
      "547         2\n",
      "692         3\n",
      "2704        4\n",
      "2846        5\n",
      "final item_id\n",
      "      MovieID  itemId\n",
      "0           1       0\n",
      "547         2       1\n",
      "692         3       2\n",
      "2704        4       3\n",
      "2846        5       4\n",
      "final df \n",
      "   MovieID  CustomerID  rating     timestamp  userId  itemId\n",
      "0        1     1488844       3  1.125965e+09       0       0\n",
      "1        1      822109       5  1.115942e+09       1       0\n",
      "2        1      885013       4  1.129680e+09       2       0\n",
      "3        1       30878       4  1.135555e+09       3       0\n",
      "4        1      823519       3  1.083542e+09       4       0\n",
      "final final df \n",
      "   userId  itemId  rating     timestamp\n",
      "0       0       0       3  1.125965e+09\n",
      "1       1       0       5  1.115942e+09\n",
      "2       2       0       4  1.129680e+09\n",
      "3       3       0       4  1.135555e+09\n",
      "4       4       0       3  1.083542e+09\n",
      "Range of userId is [0, 470757]\n",
      "Range of itemId is [0, 4498]\n"
     ]
    }
   ],
   "source": [
    "### CONVERTING [CustomerID -> userId, MovieID -> itemsId] to match format for data loader\n",
    "\n",
    "user_id = df[['CustomerID']].drop_duplicates().reindex()\n",
    "\n",
    "print(\"initial user_id\")\n",
    "print(user_id.head())\n",
    "\n",
    "user_id['userId'] = np.arange(len(user_id)) \n",
    "\n",
    "print(\"final user_id\")\n",
    "print(user_id.head())\n",
    "\n",
    "df = pd.merge(df, user_id, on=['CustomerID'], how='left')\n",
    "\n",
    "print(\"initial df \")\n",
    "print(df.head())\n",
    "\n",
    "item_id = df[['MovieID']].drop_duplicates()\n",
    "\n",
    "print(\"initial item_id\")\n",
    "print(item_id.head())\n",
    "\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "\n",
    "print(\"final item_id\")\n",
    "print(item_id.head())\n",
    "\n",
    "df = pd.merge(df, item_id, on=['MovieID'], how='left')\n",
    "\n",
    "print(\"final df \")\n",
    "print(df.head())\n",
    "\n",
    "df = df[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "\n",
    "print(\"final final df \")\n",
    "print(df.head())\n",
    "\n",
    "print('Range of userId is [{}, {}]'.format(df.userId.min(), df.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(df.itemId.min(), df.itemId.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 7999]\n",
      "Range of itemId is [0, 4498]\n"
     ]
    }
   ],
   "source": [
    "### DOWNSIZING DATA (original dataset too big - Memory Error)\n",
    "\n",
    "##Reduce by sampling a % of dataset\n",
    "# df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# print(df_sampled.head())\n",
    "\n",
    "# print('Range of userId is [{}, {}]'.format(df_sampled.userId.min(), df_sampled.userId.max()))\n",
    "# print('Range of itemId is [{}, {}]'.format(df_sampled.itemId.min(), df_sampled.itemId.max()))\n",
    "\n",
    "##Reduce by decreasing number of unique users\n",
    "desired_unique_users = 8000\n",
    "\n",
    "# Get a subset of user_ids to keep\n",
    "subset_user_ids = df['userId'].unique()[:desired_unique_users]\n",
    "\n",
    "# Filter the DataFrame to keep only rows with user_ids in the subset\n",
    "df_reduced = df[df['userId'].isin(subset_user_ids)]\n",
    "\n",
    "print('Range of userId is [{}, {}]'.format(df_reduced.userId.min(), df_reduced.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(df_reduced.itemId.min(), df_reduced.itemId.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24053764\n",
      "1152161\n",
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "###Comparing the length of the dataset\n",
    "\n",
    "print(len(df))\n",
    "# print(len(df_sampled))\n",
    "print(len(df_reduced))\n",
    "\n",
    "rating_range = df_reduced['rating'].min(), df_reduced['rating'].max()\n",
    "\n",
    "print(rating_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_config = {'alias': 'mlp_factor8neg4_bz256_166432168_pretrain_reg_0.0000001',\n",
    "              'num_epoch': 1,\n",
    "              'batch_size': 1024,  # 1024,\n",
    "              'optimizer': 'adam',\n",
    "              'adam_lr': 1e-3,\n",
    "              'num_users': 8000,\n",
    "              'num_items': 4499,\n",
    "              'latent_dim': 8,\n",
    "              'num_negative': 4,\n",
    "              'layers': [16,64,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
    "              'l2_regularization': 0.0000001,  # MLP model is sensitive to hyper params\n",
    "              'use_cuda': False,\n",
    "              'device_id': 7,\n",
    "              'pretrain': False,\n",
    "              'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n",
    "              'model_dir':'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_config = {'alias': 'pretrain_neumf_factor8neg4',\n",
    "                'num_epoch': 200,\n",
    "                'batch_size': 1024,\n",
    "                'optimizer': 'adam',\n",
    "                'adam_lr': 1e-3,\n",
    "                'num_users': 6040,\n",
    "                'num_items': 3706,\n",
    "                'latent_dim_mf': 8,\n",
    "                'latent_dim_mlp': 8,\n",
    "                'num_negative': 4,\n",
    "                'layers': [16,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
    "                'l2_regularization': 0.01,\n",
    "                'use_cuda': False,\n",
    "                'device_id': 7,\n",
    "                'pretrain': True,\n",
    "                'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n",
    "                'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_Epoch100_HR0.5606_NDCG0.2463.model'),\n",
    "                'model_dir':'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan\\Desktop\\GitHub_public\\neural-collaborative-filtering\\src\\data.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings['rating'][ratings['rating'] > 0] = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2)    userId                                   interacted_items\n",
      "0       0  {0, 7, 4108, 16, 2068, 2071, 4122, 29, 4134, 4...\n",
      "1       1  {0, 1541, 1797, 1809, 2579, 3859, 1306, 2593, ...\n",
      "2       2  {0, 4097, 4, 2056, 1551, 4114, 1560, 3610, 413...\n",
      "3       3  {0, 2050, 4, 17, 2071, 4122, 27, 29, 4135, 43,...\n",
      "4       4  {0, 1026, 1541, 1542, 7, 3078, 1034, 2571, 16,...\n",
      "-------\n",
      "(8000, 3)    userId                                   interacted_items  \\\n",
      "0       0  {0, 7, 4108, 16, 2068, 2071, 4122, 29, 4134, 4...   \n",
      "1       1  {0, 1541, 1797, 1809, 2579, 3859, 1306, 2593, ...   \n",
      "2       2  {0, 4097, 4, 2056, 1551, 4114, 1560, 3610, 413...   \n",
      "3       3  {0, 2050, 4, 17, 2071, 4122, 27, 29, 4135, 43,...   \n",
      "4       4  {0, 1026, 1541, 1542, 7, 3078, 1034, 2571, 16,...   \n",
      "\n",
      "                                      negative_items  \n",
      "0  {1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
      "1  {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n",
      "2  {1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
      "3  {1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
      "4  {1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
      "--------\n",
      "(8000, 4)    userId                                   interacted_items  \\\n",
      "0       0  {0, 7, 4108, 16, 2068, 2071, 4122, 29, 4134, 4...   \n",
      "1       1  {0, 1541, 1797, 1809, 2579, 3859, 1306, 2593, ...   \n",
      "2       2  {0, 4097, 4, 2056, 1551, 4114, 1560, 3610, 413...   \n",
      "3       3  {0, 2050, 4, 17, 2071, 4122, 27, 29, 4135, 43,...   \n",
      "4       4  {0, 1026, 1541, 1542, 7, 3078, 1034, 2571, 16,...   \n",
      "\n",
      "                                      negative_items  \\\n",
      "0  {1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
      "1  {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "2  {1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
      "3  {1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
      "4  {1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
      "\n",
      "                                    negative_samples  \n",
      "0  [3913, 1782, 3509, 4126, 1950, 181, 1194, 4479...  \n",
      "1  [1802, 4166, 1146, 2327, 1150, 779, 2069, 4396...  \n",
      "2  [3698, 4356, 2185, 528, 118, 785, 3334, 11, 41...  \n",
      "3  [2807, 4467, 4300, 963, 2644, 2554, 1100, 2915...  \n",
      "4  [2219, 4043, 593, 771, 1114, 1278, 332, 687, 3...  \n",
      "7985\n",
      "4499\n",
      "----\n",
      "7985\n",
      "1836\n"
     ]
    }
   ],
   "source": [
    "# DataLoader for training\n",
    "sample_generator1 = SampleGenerator(ratings=df_reduced)\n",
    "evaluate_data1 = sample_generator1.evaluate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (embedding_user): Embedding(8000, 8)\n",
      "  (embedding_item): Embedding(4499, 8)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Specify the exact model\n",
    "# config = gmf_config\n",
    "# engine = GMFEngine(config)\n",
    "config = mlp_config\n",
    "engine = MLPEngine(config)\n",
    "# config = neumf_config\n",
    "# engine = NeuMFEngine(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.6435933113098145\n",
      "[Training Epoch 0] Batch 1, Loss 0.6461410522460938\n",
      "[Training Epoch 0] Batch 2, Loss 0.6417548656463623\n",
      "[Training Epoch 0] Batch 3, Loss 0.6385498642921448\n",
      "[Training Epoch 0] Batch 4, Loss 0.638254702091217\n",
      "[Training Epoch 0] Batch 5, Loss 0.6344251036643982\n",
      "[Training Epoch 0] Batch 6, Loss 0.6290842294692993\n",
      "[Training Epoch 0] Batch 7, Loss 0.6231271028518677\n",
      "[Training Epoch 0] Batch 8, Loss 0.6246495842933655\n",
      "[Training Epoch 0] Batch 9, Loss 0.6205435991287231\n",
      "[Training Epoch 0] Batch 10, Loss 0.6248477697372437\n",
      "[Training Epoch 0] Batch 11, Loss 0.617735743522644\n",
      "[Training Epoch 0] Batch 12, Loss 0.6113344430923462\n",
      "[Training Epoch 0] Batch 13, Loss 0.611133873462677\n",
      "[Training Epoch 0] Batch 14, Loss 0.6082634925842285\n",
      "[Training Epoch 0] Batch 15, Loss 0.6095789670944214\n",
      "[Training Epoch 0] Batch 16, Loss 0.5975694060325623\n",
      "[Training Epoch 0] Batch 17, Loss 0.6052243709564209\n",
      "[Training Epoch 0] Batch 18, Loss 0.5920925140380859\n",
      "[Training Epoch 0] Batch 19, Loss 0.5916081666946411\n",
      "[Training Epoch 0] Batch 20, Loss 0.5823580026626587\n",
      "[Training Epoch 0] Batch 21, Loss 0.5776733160018921\n",
      "[Training Epoch 0] Batch 22, Loss 0.5750679969787598\n",
      "[Training Epoch 0] Batch 23, Loss 0.5713974237442017\n",
      "[Training Epoch 0] Batch 24, Loss 0.5730177164077759\n",
      "[Training Epoch 0] Batch 25, Loss 0.5685456395149231\n",
      "[Training Epoch 0] Batch 26, Loss 0.5613958835601807\n",
      "[Training Epoch 0] Batch 27, Loss 0.5480063557624817\n",
      "[Training Epoch 0] Batch 28, Loss 0.5574612021446228\n",
      "[Training Epoch 0] Batch 29, Loss 0.5476405024528503\n",
      "[Training Epoch 0] Batch 30, Loss 0.5545962452888489\n",
      "[Training Epoch 0] Batch 31, Loss 0.546737551689148\n",
      "[Training Epoch 0] Batch 32, Loss 0.5257084965705872\n",
      "[Training Epoch 0] Batch 33, Loss 0.5227015614509583\n",
      "[Training Epoch 0] Batch 34, Loss 0.5061376690864563\n",
      "[Training Epoch 0] Batch 35, Loss 0.5192659497261047\n",
      "[Training Epoch 0] Batch 36, Loss 0.5124998688697815\n",
      "[Training Epoch 0] Batch 37, Loss 0.511745274066925\n",
      "[Training Epoch 0] Batch 38, Loss 0.5053089261054993\n",
      "[Training Epoch 0] Batch 39, Loss 0.4940192401409149\n",
      "[Training Epoch 0] Batch 40, Loss 0.5111761689186096\n",
      "[Training Epoch 0] Batch 41, Loss 0.49661147594451904\n",
      "[Training Epoch 0] Batch 42, Loss 0.4778582453727722\n",
      "[Training Epoch 0] Batch 43, Loss 0.48628711700439453\n",
      "[Training Epoch 0] Batch 44, Loss 0.5243688225746155\n",
      "[Training Epoch 0] Batch 45, Loss 0.5211900472640991\n",
      "[Training Epoch 0] Batch 46, Loss 0.5328244566917419\n",
      "[Training Epoch 0] Batch 47, Loss 0.5152159929275513\n",
      "[Training Epoch 0] Batch 48, Loss 0.5060425996780396\n",
      "[Training Epoch 0] Batch 49, Loss 0.46850988268852234\n",
      "[Training Epoch 0] Batch 50, Loss 0.5042390823364258\n",
      "[Training Epoch 0] Batch 51, Loss 0.47601452469825745\n",
      "[Training Epoch 0] Batch 52, Loss 0.550775945186615\n",
      "[Training Epoch 0] Batch 53, Loss 0.48898956179618835\n",
      "[Training Epoch 0] Batch 54, Loss 0.4962520897388458\n",
      "[Training Epoch 0] Batch 55, Loss 0.5268732309341431\n",
      "[Training Epoch 0] Batch 56, Loss 0.5251190662384033\n",
      "[Training Epoch 0] Batch 57, Loss 0.5238478779792786\n",
      "[Training Epoch 0] Batch 58, Loss 0.49711403250694275\n",
      "[Training Epoch 0] Batch 59, Loss 0.5074046850204468\n",
      "[Training Epoch 0] Batch 60, Loss 0.512935996055603\n",
      "[Training Epoch 0] Batch 61, Loss 0.5229618549346924\n",
      "[Training Epoch 0] Batch 62, Loss 0.5144163966178894\n",
      "[Training Epoch 0] Batch 63, Loss 0.5155264139175415\n",
      "[Training Epoch 0] Batch 64, Loss 0.4599672257900238\n",
      "[Training Epoch 0] Batch 65, Loss 0.48384159803390503\n",
      "[Training Epoch 0] Batch 66, Loss 0.5204223394393921\n",
      "[Training Epoch 0] Batch 67, Loss 0.504891574382782\n",
      "[Training Epoch 0] Batch 68, Loss 0.5016894340515137\n",
      "[Training Epoch 0] Batch 69, Loss 0.5110838413238525\n",
      "[Training Epoch 0] Batch 70, Loss 0.4915817081928253\n",
      "[Training Epoch 0] Batch 71, Loss 0.4721109867095947\n",
      "[Training Epoch 0] Batch 72, Loss 0.4939286708831787\n",
      "[Training Epoch 0] Batch 73, Loss 0.5048258304595947\n",
      "[Training Epoch 0] Batch 74, Loss 0.49108436703681946\n",
      "[Training Epoch 0] Batch 75, Loss 0.5256412029266357\n",
      "[Training Epoch 0] Batch 76, Loss 0.4917204976081848\n",
      "[Training Epoch 0] Batch 77, Loss 0.5025889277458191\n",
      "[Training Epoch 0] Batch 78, Loss 0.4956902861595154\n",
      "[Training Epoch 0] Batch 79, Loss 0.4880989193916321\n",
      "[Training Epoch 0] Batch 80, Loss 0.49362513422966003\n",
      "[Training Epoch 0] Batch 81, Loss 0.48952826857566833\n",
      "[Training Epoch 0] Batch 82, Loss 0.4771783649921417\n",
      "[Training Epoch 0] Batch 83, Loss 0.5027367472648621\n",
      "[Training Epoch 0] Batch 84, Loss 0.46349620819091797\n",
      "[Training Epoch 0] Batch 85, Loss 0.4606657922267914\n",
      "[Training Epoch 0] Batch 86, Loss 0.4855154752731323\n",
      "[Training Epoch 0] Batch 87, Loss 0.5021932125091553\n",
      "[Training Epoch 0] Batch 88, Loss 0.5048182010650635\n",
      "[Training Epoch 0] Batch 89, Loss 0.5190627574920654\n",
      "[Training Epoch 0] Batch 90, Loss 0.4898604154586792\n",
      "[Training Epoch 0] Batch 91, Loss 0.5348760485649109\n",
      "[Training Epoch 0] Batch 92, Loss 0.46718284487724304\n",
      "[Training Epoch 0] Batch 93, Loss 0.5086055397987366\n",
      "[Training Epoch 0] Batch 94, Loss 0.4835329055786133\n",
      "[Training Epoch 0] Batch 95, Loss 0.4871153235435486\n",
      "[Training Epoch 0] Batch 96, Loss 0.5074849128723145\n",
      "[Training Epoch 0] Batch 97, Loss 0.4774327576160431\n",
      "[Training Epoch 0] Batch 98, Loss 0.4991728663444519\n",
      "[Training Epoch 0] Batch 99, Loss 0.5303748846054077\n",
      "[Training Epoch 0] Batch 100, Loss 0.46319517493247986\n",
      "[Training Epoch 0] Batch 101, Loss 0.4857124984264374\n",
      "[Training Epoch 0] Batch 102, Loss 0.4866287112236023\n",
      "[Training Epoch 0] Batch 103, Loss 0.49349674582481384\n",
      "[Training Epoch 0] Batch 104, Loss 0.5047690868377686\n",
      "[Training Epoch 0] Batch 105, Loss 0.4856749176979065\n",
      "[Training Epoch 0] Batch 106, Loss 0.5043507814407349\n",
      "[Training Epoch 0] Batch 107, Loss 0.51085364818573\n",
      "[Training Epoch 0] Batch 108, Loss 0.5174962282180786\n",
      "[Training Epoch 0] Batch 109, Loss 0.49097639322280884\n",
      "[Training Epoch 0] Batch 110, Loss 0.4948555827140808\n",
      "[Training Epoch 0] Batch 111, Loss 0.516505777835846\n",
      "[Training Epoch 0] Batch 112, Loss 0.48641273379325867\n",
      "[Training Epoch 0] Batch 113, Loss 0.4850349426269531\n",
      "[Training Epoch 0] Batch 114, Loss 0.4919220805168152\n",
      "[Training Epoch 0] Batch 115, Loss 0.4822865426540375\n",
      "[Training Epoch 0] Batch 116, Loss 0.4757254719734192\n",
      "[Training Epoch 0] Batch 117, Loss 0.487606406211853\n",
      "[Training Epoch 0] Batch 118, Loss 0.5101957321166992\n",
      "[Training Epoch 0] Batch 119, Loss 0.5052725672721863\n",
      "[Training Epoch 0] Batch 120, Loss 0.5049019455909729\n",
      "[Training Epoch 0] Batch 121, Loss 0.5211431384086609\n",
      "[Training Epoch 0] Batch 122, Loss 0.4840344786643982\n",
      "[Training Epoch 0] Batch 123, Loss 0.47387760877609253\n",
      "[Training Epoch 0] Batch 124, Loss 0.47586387395858765\n",
      "[Training Epoch 0] Batch 125, Loss 0.5074384212493896\n",
      "[Training Epoch 0] Batch 126, Loss 0.5061453580856323\n",
      "[Training Epoch 0] Batch 127, Loss 0.4775223433971405\n",
      "[Training Epoch 0] Batch 128, Loss 0.5315133333206177\n",
      "[Training Epoch 0] Batch 129, Loss 0.48428836464881897\n",
      "[Training Epoch 0] Batch 130, Loss 0.5038474202156067\n",
      "[Training Epoch 0] Batch 131, Loss 0.4845946133136749\n",
      "[Training Epoch 0] Batch 132, Loss 0.47239944338798523\n",
      "[Training Epoch 0] Batch 133, Loss 0.5007199645042419\n",
      "[Training Epoch 0] Batch 134, Loss 0.5029569864273071\n",
      "[Training Epoch 0] Batch 135, Loss 0.48506471514701843\n",
      "[Training Epoch 0] Batch 136, Loss 0.50284343957901\n",
      "[Training Epoch 0] Batch 137, Loss 0.5065830945968628\n",
      "[Training Epoch 0] Batch 138, Loss 0.4938507080078125\n",
      "[Training Epoch 0] Batch 139, Loss 0.4846738576889038\n",
      "[Training Epoch 0] Batch 140, Loss 0.4867352545261383\n",
      "[Training Epoch 0] Batch 141, Loss 0.4608670473098755\n",
      "[Training Epoch 0] Batch 142, Loss 0.5081030130386353\n",
      "[Training Epoch 0] Batch 143, Loss 0.5044830441474915\n",
      "[Training Epoch 0] Batch 144, Loss 0.5004268884658813\n",
      "[Training Epoch 0] Batch 145, Loss 0.48332086205482483\n",
      "[Training Epoch 0] Batch 146, Loss 0.519559383392334\n",
      "[Training Epoch 0] Batch 147, Loss 0.47482264041900635\n",
      "[Training Epoch 0] Batch 148, Loss 0.5008750557899475\n",
      "[Training Epoch 0] Batch 149, Loss 0.4858775734901428\n",
      "[Training Epoch 0] Batch 150, Loss 0.4702697992324829\n",
      "[Training Epoch 0] Batch 151, Loss 0.5095844864845276\n",
      "[Training Epoch 0] Batch 152, Loss 0.4853460192680359\n",
      "[Training Epoch 0] Batch 153, Loss 0.4796781539916992\n",
      "[Training Epoch 0] Batch 154, Loss 0.5066488981246948\n",
      "[Training Epoch 0] Batch 155, Loss 0.4636204242706299\n",
      "[Training Epoch 0] Batch 156, Loss 0.4996424615383148\n",
      "[Training Epoch 0] Batch 157, Loss 0.5169156789779663\n",
      "[Training Epoch 0] Batch 158, Loss 0.5218974351882935\n",
      "[Training Epoch 0] Batch 159, Loss 0.49464136362075806\n",
      "[Training Epoch 0] Batch 160, Loss 0.4974665343761444\n",
      "[Training Epoch 0] Batch 161, Loss 0.5097674131393433\n",
      "[Training Epoch 0] Batch 162, Loss 0.5072860717773438\n",
      "[Training Epoch 0] Batch 163, Loss 0.4934939742088318\n",
      "[Training Epoch 0] Batch 164, Loss 0.4881892800331116\n",
      "[Training Epoch 0] Batch 165, Loss 0.4801713824272156\n",
      "[Training Epoch 0] Batch 166, Loss 0.5266731977462769\n",
      "[Training Epoch 0] Batch 167, Loss 0.46601375937461853\n",
      "[Training Epoch 0] Batch 168, Loss 0.4995536804199219\n",
      "[Training Epoch 0] Batch 169, Loss 0.5151023268699646\n",
      "[Training Epoch 0] Batch 170, Loss 0.49120229482650757\n",
      "[Training Epoch 0] Batch 171, Loss 0.5028771162033081\n",
      "[Training Epoch 0] Batch 172, Loss 0.495229035615921\n",
      "[Training Epoch 0] Batch 173, Loss 0.4627707004547119\n",
      "[Training Epoch 0] Batch 174, Loss 0.5052719116210938\n",
      "[Training Epoch 0] Batch 175, Loss 0.4857325553894043\n",
      "[Training Epoch 0] Batch 176, Loss 0.5037757158279419\n",
      "[Training Epoch 0] Batch 177, Loss 0.4848681688308716\n",
      "[Training Epoch 0] Batch 178, Loss 0.5097922086715698\n",
      "[Training Epoch 0] Batch 179, Loss 0.507631778717041\n",
      "[Training Epoch 0] Batch 180, Loss 0.4732532501220703\n",
      "[Training Epoch 0] Batch 181, Loss 0.4988458752632141\n",
      "[Training Epoch 0] Batch 182, Loss 0.471554696559906\n",
      "[Training Epoch 0] Batch 183, Loss 0.5321403741836548\n",
      "[Training Epoch 0] Batch 184, Loss 0.5167369842529297\n",
      "[Training Epoch 0] Batch 185, Loss 0.5036908388137817\n",
      "[Training Epoch 0] Batch 186, Loss 0.4764416813850403\n",
      "[Training Epoch 0] Batch 187, Loss 0.46233105659484863\n",
      "[Training Epoch 0] Batch 188, Loss 0.4869921803474426\n",
      "[Training Epoch 0] Batch 189, Loss 0.483176589012146\n",
      "[Training Epoch 0] Batch 190, Loss 0.4796508848667145\n",
      "[Training Epoch 0] Batch 191, Loss 0.528406023979187\n",
      "[Training Epoch 0] Batch 192, Loss 0.49259117245674133\n",
      "[Training Epoch 0] Batch 193, Loss 0.48097002506256104\n",
      "[Training Epoch 0] Batch 194, Loss 0.4964215159416199\n",
      "[Training Epoch 0] Batch 195, Loss 0.4936973750591278\n",
      "[Training Epoch 0] Batch 196, Loss 0.49642592668533325\n",
      "[Training Epoch 0] Batch 197, Loss 0.4669095575809479\n",
      "[Training Epoch 0] Batch 198, Loss 0.48280709981918335\n",
      "[Training Epoch 0] Batch 199, Loss 0.4802578091621399\n",
      "[Training Epoch 0] Batch 200, Loss 0.49534523487091064\n",
      "[Training Epoch 0] Batch 201, Loss 0.4980810582637787\n",
      "[Training Epoch 0] Batch 202, Loss 0.49140793085098267\n",
      "[Training Epoch 0] Batch 203, Loss 0.5137600898742676\n",
      "[Training Epoch 0] Batch 204, Loss 0.47900018095970154\n",
      "[Training Epoch 0] Batch 205, Loss 0.4860558807849884\n",
      "[Training Epoch 0] Batch 206, Loss 0.48420366644859314\n",
      "[Training Epoch 0] Batch 207, Loss 0.507384181022644\n",
      "[Training Epoch 0] Batch 208, Loss 0.4899454712867737\n",
      "[Training Epoch 0] Batch 209, Loss 0.4899706542491913\n",
      "[Training Epoch 0] Batch 210, Loss 0.46047085523605347\n",
      "[Training Epoch 0] Batch 211, Loss 0.4754118025302887\n",
      "[Training Epoch 0] Batch 212, Loss 0.5093559622764587\n",
      "[Training Epoch 0] Batch 213, Loss 0.46970051527023315\n",
      "[Training Epoch 0] Batch 214, Loss 0.47654634714126587\n",
      "[Training Epoch 0] Batch 215, Loss 0.4590374231338501\n",
      "[Training Epoch 0] Batch 216, Loss 0.4961949288845062\n",
      "[Training Epoch 0] Batch 217, Loss 0.4808955788612366\n",
      "[Training Epoch 0] Batch 218, Loss 0.5027489066123962\n",
      "[Training Epoch 0] Batch 219, Loss 0.5136741399765015\n",
      "[Training Epoch 0] Batch 220, Loss 0.4960036277770996\n",
      "[Training Epoch 0] Batch 221, Loss 0.48614510893821716\n",
      "[Training Epoch 0] Batch 222, Loss 0.47013071179389954\n",
      "[Training Epoch 0] Batch 223, Loss 0.4773407280445099\n",
      "[Training Epoch 0] Batch 224, Loss 0.4955270290374756\n",
      "[Training Epoch 0] Batch 225, Loss 0.4641362726688385\n",
      "[Training Epoch 0] Batch 226, Loss 0.4840511381626129\n",
      "[Training Epoch 0] Batch 227, Loss 0.4736621379852295\n",
      "[Training Epoch 0] Batch 228, Loss 0.4782361686229706\n",
      "[Training Epoch 0] Batch 229, Loss 0.488958477973938\n",
      "[Training Epoch 0] Batch 230, Loss 0.4954909682273865\n",
      "[Training Epoch 0] Batch 231, Loss 0.500569760799408\n",
      "[Training Epoch 0] Batch 232, Loss 0.5065870881080627\n",
      "[Training Epoch 0] Batch 233, Loss 0.502639651298523\n",
      "[Training Epoch 0] Batch 234, Loss 0.4977308511734009\n",
      "[Training Epoch 0] Batch 235, Loss 0.4951086640357971\n",
      "[Training Epoch 0] Batch 236, Loss 0.4818453788757324\n",
      "[Training Epoch 0] Batch 237, Loss 0.47087299823760986\n",
      "[Training Epoch 0] Batch 238, Loss 0.44938406348228455\n",
      "[Training Epoch 0] Batch 239, Loss 0.49887949228286743\n",
      "[Training Epoch 0] Batch 240, Loss 0.5113962292671204\n",
      "[Training Epoch 0] Batch 241, Loss 0.48733699321746826\n",
      "[Training Epoch 0] Batch 242, Loss 0.501379132270813\n",
      "[Training Epoch 0] Batch 243, Loss 0.48777636885643005\n",
      "[Training Epoch 0] Batch 244, Loss 0.4942489266395569\n",
      "[Training Epoch 0] Batch 245, Loss 0.48619455099105835\n",
      "[Training Epoch 0] Batch 246, Loss 0.4733508825302124\n",
      "[Training Epoch 0] Batch 247, Loss 0.502498984336853\n",
      "[Training Epoch 0] Batch 248, Loss 0.4509989321231842\n",
      "[Training Epoch 0] Batch 249, Loss 0.46435004472732544\n",
      "[Training Epoch 0] Batch 250, Loss 0.4670281708240509\n",
      "[Training Epoch 0] Batch 251, Loss 0.4739312529563904\n",
      "[Training Epoch 0] Batch 252, Loss 0.4670489430427551\n",
      "[Training Epoch 0] Batch 253, Loss 0.49214470386505127\n",
      "[Training Epoch 0] Batch 254, Loss 0.47188693284988403\n",
      "[Training Epoch 0] Batch 255, Loss 0.49178019165992737\n",
      "[Training Epoch 0] Batch 256, Loss 0.49239474534988403\n",
      "[Training Epoch 0] Batch 257, Loss 0.4448029696941376\n",
      "[Training Epoch 0] Batch 258, Loss 0.5123109221458435\n",
      "[Training Epoch 0] Batch 259, Loss 0.4868129789829254\n",
      "[Training Epoch 0] Batch 260, Loss 0.49016955494880676\n",
      "[Training Epoch 0] Batch 261, Loss 0.49800655245780945\n",
      "[Training Epoch 0] Batch 262, Loss 0.4696763753890991\n",
      "[Training Epoch 0] Batch 263, Loss 0.4867216646671295\n",
      "[Training Epoch 0] Batch 264, Loss 0.4930243194103241\n",
      "[Training Epoch 0] Batch 265, Loss 0.5051423907279968\n",
      "[Training Epoch 0] Batch 266, Loss 0.4838368892669678\n",
      "[Training Epoch 0] Batch 267, Loss 0.4496772885322571\n",
      "[Training Epoch 0] Batch 268, Loss 0.5095557570457458\n",
      "[Training Epoch 0] Batch 269, Loss 0.4894956052303314\n",
      "[Training Epoch 0] Batch 270, Loss 0.4682464599609375\n",
      "[Training Epoch 0] Batch 271, Loss 0.48289385437965393\n",
      "[Training Epoch 0] Batch 272, Loss 0.44285616278648376\n",
      "[Training Epoch 0] Batch 273, Loss 0.4430028200149536\n",
      "[Training Epoch 0] Batch 274, Loss 0.48053205013275146\n",
      "[Training Epoch 0] Batch 275, Loss 0.49025553464889526\n",
      "[Training Epoch 0] Batch 276, Loss 0.48124730587005615\n",
      "[Training Epoch 0] Batch 277, Loss 0.45582541823387146\n",
      "[Training Epoch 0] Batch 278, Loss 0.48701944947242737\n",
      "[Training Epoch 0] Batch 279, Loss 0.48007550835609436\n",
      "[Training Epoch 0] Batch 280, Loss 0.47722116112709045\n",
      "[Training Epoch 0] Batch 281, Loss 0.4635559022426605\n",
      "[Training Epoch 0] Batch 282, Loss 0.49946027994155884\n",
      "[Training Epoch 0] Batch 283, Loss 0.49188411235809326\n",
      "[Training Epoch 0] Batch 284, Loss 0.45201051235198975\n",
      "[Training Epoch 0] Batch 285, Loss 0.4945462942123413\n",
      "[Training Epoch 0] Batch 286, Loss 0.4906441569328308\n",
      "[Training Epoch 0] Batch 287, Loss 0.5057486891746521\n",
      "[Training Epoch 0] Batch 288, Loss 0.475445032119751\n",
      "[Training Epoch 0] Batch 289, Loss 0.47126853466033936\n",
      "[Training Epoch 0] Batch 290, Loss 0.47435081005096436\n",
      "[Training Epoch 0] Batch 291, Loss 0.46604520082473755\n",
      "[Training Epoch 0] Batch 292, Loss 0.5077218413352966\n",
      "[Training Epoch 0] Batch 293, Loss 0.4752155542373657\n",
      "[Training Epoch 0] Batch 294, Loss 0.47233930230140686\n",
      "[Training Epoch 0] Batch 295, Loss 0.47838935256004333\n",
      "[Training Epoch 0] Batch 296, Loss 0.46759799122810364\n",
      "[Training Epoch 0] Batch 297, Loss 0.4877171516418457\n",
      "[Training Epoch 0] Batch 298, Loss 0.49390971660614014\n",
      "[Training Epoch 0] Batch 299, Loss 0.4849439561367035\n",
      "[Training Epoch 0] Batch 300, Loss 0.49075186252593994\n",
      "[Training Epoch 0] Batch 301, Loss 0.4996257424354553\n",
      "[Training Epoch 0] Batch 302, Loss 0.47823888063430786\n",
      "[Training Epoch 0] Batch 303, Loss 0.48160672187805176\n",
      "[Training Epoch 0] Batch 304, Loss 0.5276604294776917\n",
      "[Training Epoch 0] Batch 305, Loss 0.48793548345565796\n",
      "[Training Epoch 0] Batch 306, Loss 0.4770301580429077\n",
      "[Training Epoch 0] Batch 307, Loss 0.46140256524086\n",
      "[Training Epoch 0] Batch 308, Loss 0.5070555806159973\n",
      "[Training Epoch 0] Batch 309, Loss 0.48093730211257935\n",
      "[Training Epoch 0] Batch 310, Loss 0.5018154382705688\n",
      "[Training Epoch 0] Batch 311, Loss 0.482694149017334\n",
      "[Training Epoch 0] Batch 312, Loss 0.5000547766685486\n",
      "[Training Epoch 0] Batch 313, Loss 0.48463356494903564\n",
      "[Training Epoch 0] Batch 314, Loss 0.4848334491252899\n",
      "[Training Epoch 0] Batch 315, Loss 0.49445921182632446\n",
      "[Training Epoch 0] Batch 316, Loss 0.4655832350254059\n",
      "[Training Epoch 0] Batch 317, Loss 0.48948055505752563\n",
      "[Training Epoch 0] Batch 318, Loss 0.46072712540626526\n",
      "[Training Epoch 0] Batch 319, Loss 0.4863475263118744\n",
      "[Training Epoch 0] Batch 320, Loss 0.4926283061504364\n",
      "[Training Epoch 0] Batch 321, Loss 0.45689281821250916\n",
      "[Training Epoch 0] Batch 322, Loss 0.497256875038147\n",
      "[Training Epoch 0] Batch 323, Loss 0.48454761505126953\n",
      "[Training Epoch 0] Batch 324, Loss 0.456553190946579\n",
      "[Training Epoch 0] Batch 325, Loss 0.4791247546672821\n",
      "[Training Epoch 0] Batch 326, Loss 0.4790560007095337\n",
      "[Training Epoch 0] Batch 327, Loss 0.48588794469833374\n",
      "[Training Epoch 0] Batch 328, Loss 0.49068909883499146\n",
      "[Training Epoch 0] Batch 329, Loss 0.47021961212158203\n",
      "[Training Epoch 0] Batch 330, Loss 0.4760150909423828\n",
      "[Training Epoch 0] Batch 331, Loss 0.48708048462867737\n",
      "[Training Epoch 0] Batch 332, Loss 0.47597503662109375\n",
      "[Training Epoch 0] Batch 333, Loss 0.4745434522628784\n",
      "[Training Epoch 0] Batch 334, Loss 0.49721425771713257\n",
      "[Training Epoch 0] Batch 335, Loss 0.47736701369285583\n",
      "[Training Epoch 0] Batch 336, Loss 0.485027015209198\n",
      "[Training Epoch 0] Batch 337, Loss 0.4768981337547302\n",
      "[Training Epoch 0] Batch 338, Loss 0.48882797360420227\n",
      "[Training Epoch 0] Batch 339, Loss 0.45592001080513\n",
      "[Training Epoch 0] Batch 340, Loss 0.43856436014175415\n",
      "[Training Epoch 0] Batch 341, Loss 0.4796876013278961\n",
      "[Training Epoch 0] Batch 342, Loss 0.49833184480667114\n",
      "[Training Epoch 0] Batch 343, Loss 0.4825301766395569\n",
      "[Training Epoch 0] Batch 344, Loss 0.47707170248031616\n",
      "[Training Epoch 0] Batch 345, Loss 0.4736965596675873\n",
      "[Training Epoch 0] Batch 346, Loss 0.46072691679000854\n",
      "[Training Epoch 0] Batch 347, Loss 0.45639172196388245\n",
      "[Training Epoch 0] Batch 348, Loss 0.4598248302936554\n",
      "[Training Epoch 0] Batch 349, Loss 0.4705798327922821\n",
      "[Training Epoch 0] Batch 350, Loss 0.475769966840744\n",
      "[Training Epoch 0] Batch 351, Loss 0.478729248046875\n",
      "[Training Epoch 0] Batch 352, Loss 0.5105090141296387\n",
      "[Training Epoch 0] Batch 353, Loss 0.4335341155529022\n",
      "[Training Epoch 0] Batch 354, Loss 0.4489017724990845\n",
      "[Training Epoch 0] Batch 355, Loss 0.45915237069129944\n",
      "[Training Epoch 0] Batch 356, Loss 0.48688918352127075\n",
      "[Training Epoch 0] Batch 357, Loss 0.4977893829345703\n",
      "[Training Epoch 0] Batch 358, Loss 0.4538748860359192\n",
      "[Training Epoch 0] Batch 359, Loss 0.4855700135231018\n",
      "[Training Epoch 0] Batch 360, Loss 0.44346797466278076\n",
      "[Training Epoch 0] Batch 361, Loss 0.47398149967193604\n",
      "[Training Epoch 0] Batch 362, Loss 0.4562167823314667\n",
      "[Training Epoch 0] Batch 363, Loss 0.4639114737510681\n",
      "[Training Epoch 0] Batch 364, Loss 0.4501234292984009\n",
      "[Training Epoch 0] Batch 365, Loss 0.4762577414512634\n",
      "[Training Epoch 0] Batch 366, Loss 0.47978144884109497\n",
      "[Training Epoch 0] Batch 367, Loss 0.4732619524002075\n",
      "[Training Epoch 0] Batch 368, Loss 0.46804437041282654\n",
      "[Training Epoch 0] Batch 369, Loss 0.47162601351737976\n",
      "[Training Epoch 0] Batch 370, Loss 0.4735563099384308\n",
      "[Training Epoch 0] Batch 371, Loss 0.4701072573661804\n",
      "[Training Epoch 0] Batch 372, Loss 0.4312859773635864\n",
      "[Training Epoch 0] Batch 373, Loss 0.4158872961997986\n",
      "[Training Epoch 0] Batch 374, Loss 0.46994683146476746\n",
      "[Training Epoch 0] Batch 375, Loss 0.4869067072868347\n",
      "[Training Epoch 0] Batch 376, Loss 0.4750748872756958\n",
      "[Training Epoch 0] Batch 377, Loss 0.4666185975074768\n",
      "[Training Epoch 0] Batch 378, Loss 0.4420197308063507\n",
      "[Training Epoch 0] Batch 379, Loss 0.4889031648635864\n",
      "[Training Epoch 0] Batch 380, Loss 0.4724597930908203\n",
      "[Training Epoch 0] Batch 381, Loss 0.4957320988178253\n",
      "[Training Epoch 0] Batch 382, Loss 0.45783162117004395\n",
      "[Training Epoch 0] Batch 383, Loss 0.4518124461174011\n",
      "[Training Epoch 0] Batch 384, Loss 0.43814617395401\n",
      "[Training Epoch 0] Batch 385, Loss 0.44457095861434937\n",
      "[Training Epoch 0] Batch 386, Loss 0.45608973503112793\n",
      "[Training Epoch 0] Batch 387, Loss 0.4497476816177368\n",
      "[Training Epoch 0] Batch 388, Loss 0.4653562903404236\n",
      "[Training Epoch 0] Batch 389, Loss 0.4769135117530823\n",
      "[Training Epoch 0] Batch 390, Loss 0.4567941427230835\n",
      "[Training Epoch 0] Batch 391, Loss 0.4570522904396057\n",
      "[Training Epoch 0] Batch 392, Loss 0.4458872675895691\n",
      "[Training Epoch 0] Batch 393, Loss 0.467319130897522\n",
      "[Training Epoch 0] Batch 394, Loss 0.42884379625320435\n",
      "[Training Epoch 0] Batch 395, Loss 0.4541195034980774\n",
      "[Training Epoch 0] Batch 396, Loss 0.4470479488372803\n",
      "[Training Epoch 0] Batch 397, Loss 0.44742724299430847\n",
      "[Training Epoch 0] Batch 398, Loss 0.4505189061164856\n",
      "[Training Epoch 0] Batch 399, Loss 0.45892173051834106\n",
      "[Training Epoch 0] Batch 400, Loss 0.4626868963241577\n",
      "[Training Epoch 0] Batch 401, Loss 0.4205832779407501\n",
      "[Training Epoch 0] Batch 402, Loss 0.4380374550819397\n",
      "[Training Epoch 0] Batch 403, Loss 0.46437281370162964\n",
      "[Training Epoch 0] Batch 404, Loss 0.4759182333946228\n",
      "[Training Epoch 0] Batch 405, Loss 0.4867361783981323\n",
      "[Training Epoch 0] Batch 406, Loss 0.4418898820877075\n",
      "[Training Epoch 0] Batch 407, Loss 0.4508923292160034\n",
      "[Training Epoch 0] Batch 408, Loss 0.46562492847442627\n",
      "[Training Epoch 0] Batch 409, Loss 0.4605788588523865\n",
      "[Training Epoch 0] Batch 410, Loss 0.47189807891845703\n",
      "[Training Epoch 0] Batch 411, Loss 0.45683005452156067\n",
      "[Training Epoch 0] Batch 412, Loss 0.4538865089416504\n",
      "[Training Epoch 0] Batch 413, Loss 0.45189815759658813\n",
      "[Training Epoch 0] Batch 414, Loss 0.44694626331329346\n",
      "[Training Epoch 0] Batch 415, Loss 0.4394323229789734\n",
      "[Training Epoch 0] Batch 416, Loss 0.46651047468185425\n",
      "[Training Epoch 0] Batch 417, Loss 0.43929991126060486\n",
      "[Training Epoch 0] Batch 418, Loss 0.45096874237060547\n",
      "[Training Epoch 0] Batch 419, Loss 0.4679618179798126\n",
      "[Training Epoch 0] Batch 420, Loss 0.43360331654548645\n",
      "[Training Epoch 0] Batch 421, Loss 0.45543694496154785\n",
      "[Training Epoch 0] Batch 422, Loss 0.4607018530368805\n",
      "[Training Epoch 0] Batch 423, Loss 0.45650479197502136\n",
      "[Training Epoch 0] Batch 424, Loss 0.4627286493778229\n",
      "[Training Epoch 0] Batch 425, Loss 0.464367538690567\n",
      "[Training Epoch 0] Batch 426, Loss 0.45690739154815674\n",
      "[Training Epoch 0] Batch 427, Loss 0.4709886908531189\n",
      "[Training Epoch 0] Batch 428, Loss 0.44464626908302307\n",
      "[Training Epoch 0] Batch 429, Loss 0.43645286560058594\n",
      "[Training Epoch 0] Batch 430, Loss 0.44234350323677063\n",
      "[Training Epoch 0] Batch 431, Loss 0.46796104311943054\n",
      "[Training Epoch 0] Batch 432, Loss 0.4106987714767456\n",
      "[Training Epoch 0] Batch 433, Loss 0.4311288297176361\n",
      "[Training Epoch 0] Batch 434, Loss 0.4536902904510498\n",
      "[Training Epoch 0] Batch 435, Loss 0.4465019404888153\n",
      "[Training Epoch 0] Batch 436, Loss 0.430804967880249\n",
      "[Training Epoch 0] Batch 437, Loss 0.44380873441696167\n",
      "[Training Epoch 0] Batch 438, Loss 0.4507036805152893\n",
      "[Training Epoch 0] Batch 439, Loss 0.45648232102394104\n",
      "[Training Epoch 0] Batch 440, Loss 0.4457526206970215\n",
      "[Training Epoch 0] Batch 441, Loss 0.4280037581920624\n",
      "[Training Epoch 0] Batch 442, Loss 0.45109793543815613\n",
      "[Training Epoch 0] Batch 443, Loss 0.42066046595573425\n",
      "[Training Epoch 0] Batch 444, Loss 0.45316946506500244\n",
      "[Training Epoch 0] Batch 445, Loss 0.4417714774608612\n",
      "[Training Epoch 0] Batch 446, Loss 0.44275760650634766\n",
      "[Training Epoch 0] Batch 447, Loss 0.46940481662750244\n",
      "[Training Epoch 0] Batch 448, Loss 0.4410709738731384\n",
      "[Training Epoch 0] Batch 449, Loss 0.4358903169631958\n",
      "[Training Epoch 0] Batch 450, Loss 0.4257569909095764\n",
      "[Training Epoch 0] Batch 451, Loss 0.41713541746139526\n",
      "[Training Epoch 0] Batch 452, Loss 0.4261048436164856\n",
      "[Training Epoch 0] Batch 453, Loss 0.4539951980113983\n",
      "[Training Epoch 0] Batch 454, Loss 0.44761890172958374\n",
      "[Training Epoch 0] Batch 455, Loss 0.4493522644042969\n",
      "[Training Epoch 0] Batch 456, Loss 0.47420576214790344\n",
      "[Training Epoch 0] Batch 457, Loss 0.4590018093585968\n",
      "[Training Epoch 0] Batch 458, Loss 0.460867702960968\n",
      "[Training Epoch 0] Batch 459, Loss 0.47135424613952637\n",
      "[Training Epoch 0] Batch 460, Loss 0.408502459526062\n",
      "[Training Epoch 0] Batch 461, Loss 0.4187597930431366\n",
      "[Training Epoch 0] Batch 462, Loss 0.437578946352005\n",
      "[Training Epoch 0] Batch 463, Loss 0.42933258414268494\n",
      "[Training Epoch 0] Batch 464, Loss 0.43882107734680176\n",
      "[Training Epoch 0] Batch 465, Loss 0.4418056011199951\n",
      "[Training Epoch 0] Batch 466, Loss 0.4461038410663605\n",
      "[Training Epoch 0] Batch 467, Loss 0.4412984251976013\n",
      "[Training Epoch 0] Batch 468, Loss 0.4051983952522278\n",
      "[Training Epoch 0] Batch 469, Loss 0.4683484733104706\n",
      "[Training Epoch 0] Batch 470, Loss 0.4566991627216339\n",
      "[Training Epoch 0] Batch 471, Loss 0.4192407429218292\n",
      "[Training Epoch 0] Batch 472, Loss 0.4683719277381897\n",
      "[Training Epoch 0] Batch 473, Loss 0.45309126377105713\n",
      "[Training Epoch 0] Batch 474, Loss 0.41087067127227783\n",
      "[Training Epoch 0] Batch 475, Loss 0.44509682059288025\n",
      "[Training Epoch 0] Batch 476, Loss 0.41399577260017395\n",
      "[Training Epoch 0] Batch 477, Loss 0.46872323751449585\n",
      "[Training Epoch 0] Batch 478, Loss 0.43551525473594666\n",
      "[Training Epoch 0] Batch 479, Loss 0.440357506275177\n",
      "[Training Epoch 0] Batch 480, Loss 0.41569405794143677\n",
      "[Training Epoch 0] Batch 481, Loss 0.4140649735927582\n",
      "[Training Epoch 0] Batch 482, Loss 0.440592885017395\n",
      "[Training Epoch 0] Batch 483, Loss 0.4526556134223938\n",
      "[Training Epoch 0] Batch 484, Loss 0.4387829899787903\n",
      "[Training Epoch 0] Batch 485, Loss 0.42183929681777954\n",
      "[Training Epoch 0] Batch 486, Loss 0.44317626953125\n",
      "[Training Epoch 0] Batch 487, Loss 0.43435370922088623\n",
      "[Training Epoch 0] Batch 488, Loss 0.41014227271080017\n",
      "[Training Epoch 0] Batch 489, Loss 0.4325672686100006\n",
      "[Training Epoch 0] Batch 490, Loss 0.4272002875804901\n",
      "[Training Epoch 0] Batch 491, Loss 0.4219493269920349\n",
      "[Training Epoch 0] Batch 492, Loss 0.4384428858757019\n",
      "[Training Epoch 0] Batch 493, Loss 0.4055461287498474\n",
      "[Training Epoch 0] Batch 494, Loss 0.3938080370426178\n",
      "[Training Epoch 0] Batch 495, Loss 0.4297415018081665\n",
      "[Training Epoch 0] Batch 496, Loss 0.40770164132118225\n",
      "[Training Epoch 0] Batch 497, Loss 0.3973052501678467\n",
      "[Training Epoch 0] Batch 498, Loss 0.42438846826553345\n",
      "[Training Epoch 0] Batch 499, Loss 0.4129108190536499\n",
      "[Training Epoch 0] Batch 500, Loss 0.4024014472961426\n",
      "[Training Epoch 0] Batch 501, Loss 0.38990652561187744\n",
      "[Training Epoch 0] Batch 502, Loss 0.41492196917533875\n",
      "[Training Epoch 0] Batch 503, Loss 0.4428088665008545\n",
      "[Training Epoch 0] Batch 504, Loss 0.42467162013053894\n",
      "[Training Epoch 0] Batch 505, Loss 0.424293577671051\n",
      "[Training Epoch 0] Batch 506, Loss 0.40933090448379517\n",
      "[Training Epoch 0] Batch 507, Loss 0.4148435592651367\n",
      "[Training Epoch 0] Batch 508, Loss 0.4373539388179779\n",
      "[Training Epoch 0] Batch 509, Loss 0.42746758460998535\n",
      "[Training Epoch 0] Batch 510, Loss 0.4186713397502899\n",
      "[Training Epoch 0] Batch 511, Loss 0.41242581605911255\n",
      "[Training Epoch 0] Batch 512, Loss 0.4207426607608795\n",
      "[Training Epoch 0] Batch 513, Loss 0.4417496919631958\n",
      "[Training Epoch 0] Batch 514, Loss 0.4396134912967682\n",
      "[Training Epoch 0] Batch 515, Loss 0.44278591871261597\n",
      "[Training Epoch 0] Batch 516, Loss 0.4004220962524414\n",
      "[Training Epoch 0] Batch 517, Loss 0.4183048605918884\n",
      "[Training Epoch 0] Batch 518, Loss 0.4435024857521057\n",
      "[Training Epoch 0] Batch 519, Loss 0.4332636296749115\n",
      "[Training Epoch 0] Batch 520, Loss 0.41298797726631165\n",
      "[Training Epoch 0] Batch 521, Loss 0.42008957266807556\n",
      "[Training Epoch 0] Batch 522, Loss 0.41300079226493835\n",
      "[Training Epoch 0] Batch 523, Loss 0.42115482687950134\n",
      "[Training Epoch 0] Batch 524, Loss 0.4347286522388458\n",
      "[Training Epoch 0] Batch 525, Loss 0.3913626968860626\n",
      "[Training Epoch 0] Batch 526, Loss 0.4070928692817688\n",
      "[Training Epoch 0] Batch 527, Loss 0.4253004193305969\n",
      "[Training Epoch 0] Batch 528, Loss 0.4221172332763672\n",
      "[Training Epoch 0] Batch 529, Loss 0.4007759392261505\n",
      "[Training Epoch 0] Batch 530, Loss 0.39922744035720825\n",
      "[Training Epoch 0] Batch 531, Loss 0.42967355251312256\n",
      "[Training Epoch 0] Batch 532, Loss 0.38960957527160645\n",
      "[Training Epoch 0] Batch 533, Loss 0.39822185039520264\n",
      "[Training Epoch 0] Batch 534, Loss 0.419150173664093\n",
      "[Training Epoch 0] Batch 535, Loss 0.4301328957080841\n",
      "[Training Epoch 0] Batch 536, Loss 0.4106445908546448\n",
      "[Training Epoch 0] Batch 537, Loss 0.42160090804100037\n",
      "[Training Epoch 0] Batch 538, Loss 0.3969912528991699\n",
      "[Training Epoch 0] Batch 539, Loss 0.41267454624176025\n",
      "[Training Epoch 0] Batch 540, Loss 0.41029486060142517\n",
      "[Training Epoch 0] Batch 541, Loss 0.4224351644515991\n",
      "[Training Epoch 0] Batch 542, Loss 0.37518537044525146\n",
      "[Training Epoch 0] Batch 543, Loss 0.4395693242549896\n",
      "[Training Epoch 0] Batch 544, Loss 0.4008353650569916\n",
      "[Training Epoch 0] Batch 545, Loss 0.4100826382637024\n",
      "[Training Epoch 0] Batch 546, Loss 0.39961057901382446\n",
      "[Training Epoch 0] Batch 547, Loss 0.40716487169265747\n",
      "[Training Epoch 0] Batch 548, Loss 0.4260522723197937\n",
      "[Training Epoch 0] Batch 549, Loss 0.40363121032714844\n",
      "[Training Epoch 0] Batch 550, Loss 0.3766181170940399\n",
      "[Training Epoch 0] Batch 551, Loss 0.37821751832962036\n",
      "[Training Epoch 0] Batch 552, Loss 0.40702933073043823\n",
      "[Training Epoch 0] Batch 553, Loss 0.40920117497444153\n",
      "[Training Epoch 0] Batch 554, Loss 0.4247422218322754\n",
      "[Training Epoch 0] Batch 555, Loss 0.3933965563774109\n",
      "[Training Epoch 0] Batch 556, Loss 0.4392271935939789\n",
      "[Training Epoch 0] Batch 557, Loss 0.40823057293891907\n",
      "[Training Epoch 0] Batch 558, Loss 0.3782736659049988\n",
      "[Training Epoch 0] Batch 559, Loss 0.4120667576789856\n",
      "[Training Epoch 0] Batch 560, Loss 0.41416439414024353\n",
      "[Training Epoch 0] Batch 561, Loss 0.4095079302787781\n",
      "[Training Epoch 0] Batch 562, Loss 0.38931769132614136\n",
      "[Training Epoch 0] Batch 563, Loss 0.37493887543678284\n",
      "[Training Epoch 0] Batch 564, Loss 0.40572869777679443\n",
      "[Training Epoch 0] Batch 565, Loss 0.402082622051239\n",
      "[Training Epoch 0] Batch 566, Loss 0.402471661567688\n",
      "[Training Epoch 0] Batch 567, Loss 0.4240812063217163\n",
      "[Training Epoch 0] Batch 568, Loss 0.3883921802043915\n",
      "[Training Epoch 0] Batch 569, Loss 0.3914420008659363\n",
      "[Training Epoch 0] Batch 570, Loss 0.3895132541656494\n",
      "[Training Epoch 0] Batch 571, Loss 0.37559306621551514\n",
      "[Training Epoch 0] Batch 572, Loss 0.369063138961792\n",
      "[Training Epoch 0] Batch 573, Loss 0.40766018629074097\n",
      "[Training Epoch 0] Batch 574, Loss 0.3985408842563629\n",
      "[Training Epoch 0] Batch 575, Loss 0.43323326110839844\n",
      "[Training Epoch 0] Batch 576, Loss 0.41974836587905884\n",
      "[Training Epoch 0] Batch 577, Loss 0.39277195930480957\n",
      "[Training Epoch 0] Batch 578, Loss 0.36234307289123535\n",
      "[Training Epoch 0] Batch 579, Loss 0.39040398597717285\n",
      "[Training Epoch 0] Batch 580, Loss 0.39866238832473755\n",
      "[Training Epoch 0] Batch 581, Loss 0.40905678272247314\n",
      "[Training Epoch 0] Batch 582, Loss 0.3860052525997162\n",
      "[Training Epoch 0] Batch 583, Loss 0.3876798748970032\n",
      "[Training Epoch 0] Batch 584, Loss 0.40657150745391846\n",
      "[Training Epoch 0] Batch 585, Loss 0.39532148838043213\n",
      "[Training Epoch 0] Batch 586, Loss 0.3816337287425995\n",
      "[Training Epoch 0] Batch 587, Loss 0.3555002808570862\n",
      "[Training Epoch 0] Batch 588, Loss 0.41947507858276367\n",
      "[Training Epoch 0] Batch 589, Loss 0.42294061183929443\n",
      "[Training Epoch 0] Batch 590, Loss 0.3943057656288147\n",
      "[Training Epoch 0] Batch 591, Loss 0.3864535391330719\n",
      "[Training Epoch 0] Batch 592, Loss 0.38443222641944885\n",
      "[Training Epoch 0] Batch 593, Loss 0.38535359501838684\n",
      "[Training Epoch 0] Batch 594, Loss 0.4495052099227905\n",
      "[Training Epoch 0] Batch 595, Loss 0.3979237675666809\n",
      "[Training Epoch 0] Batch 596, Loss 0.4014326333999634\n",
      "[Training Epoch 0] Batch 597, Loss 0.3641235828399658\n",
      "[Training Epoch 0] Batch 598, Loss 0.36161696910858154\n",
      "[Training Epoch 0] Batch 599, Loss 0.3976667523384094\n",
      "[Training Epoch 0] Batch 600, Loss 0.4206421673297882\n",
      "[Training Epoch 0] Batch 601, Loss 0.3716065287590027\n",
      "[Training Epoch 0] Batch 602, Loss 0.38563603162765503\n",
      "[Training Epoch 0] Batch 603, Loss 0.37153589725494385\n",
      "[Training Epoch 0] Batch 604, Loss 0.38990119099617004\n",
      "[Training Epoch 0] Batch 605, Loss 0.41208526492118835\n",
      "[Training Epoch 0] Batch 606, Loss 0.4017496705055237\n",
      "[Training Epoch 0] Batch 607, Loss 0.38981491327285767\n",
      "[Training Epoch 0] Batch 608, Loss 0.4282533824443817\n",
      "[Training Epoch 0] Batch 609, Loss 0.393420934677124\n",
      "[Training Epoch 0] Batch 610, Loss 0.37136679887771606\n",
      "[Training Epoch 0] Batch 611, Loss 0.37512317299842834\n",
      "[Training Epoch 0] Batch 612, Loss 0.37692439556121826\n",
      "[Training Epoch 0] Batch 613, Loss 0.3821848928928375\n",
      "[Training Epoch 0] Batch 614, Loss 0.34424296021461487\n",
      "[Training Epoch 0] Batch 615, Loss 0.41675665974617004\n",
      "[Training Epoch 0] Batch 616, Loss 0.41862180829048157\n",
      "[Training Epoch 0] Batch 617, Loss 0.41202306747436523\n",
      "[Training Epoch 0] Batch 618, Loss 0.38401830196380615\n",
      "[Training Epoch 0] Batch 619, Loss 0.4212685823440552\n",
      "[Training Epoch 0] Batch 620, Loss 0.37732431292533875\n",
      "[Training Epoch 0] Batch 621, Loss 0.40903329849243164\n",
      "[Training Epoch 0] Batch 622, Loss 0.4115753769874573\n",
      "[Training Epoch 0] Batch 623, Loss 0.37171947956085205\n",
      "[Training Epoch 0] Batch 624, Loss 0.3809564709663391\n",
      "[Training Epoch 0] Batch 625, Loss 0.37727832794189453\n",
      "[Training Epoch 0] Batch 626, Loss 0.3969392776489258\n",
      "[Training Epoch 0] Batch 627, Loss 0.3759801983833313\n",
      "[Training Epoch 0] Batch 628, Loss 0.39961519837379456\n",
      "[Training Epoch 0] Batch 629, Loss 0.38954055309295654\n",
      "[Training Epoch 0] Batch 630, Loss 0.35118579864501953\n",
      "[Training Epoch 0] Batch 631, Loss 0.34281590580940247\n",
      "[Training Epoch 0] Batch 632, Loss 0.3948354125022888\n",
      "[Training Epoch 0] Batch 633, Loss 0.36510956287384033\n",
      "[Training Epoch 0] Batch 634, Loss 0.39827215671539307\n",
      "[Training Epoch 0] Batch 635, Loss 0.365349143743515\n",
      "[Training Epoch 0] Batch 636, Loss 0.36564433574676514\n",
      "[Training Epoch 0] Batch 637, Loss 0.3765772879123688\n",
      "[Training Epoch 0] Batch 638, Loss 0.3944091200828552\n",
      "[Training Epoch 0] Batch 639, Loss 0.36925068497657776\n",
      "[Training Epoch 0] Batch 640, Loss 0.37628501653671265\n",
      "[Training Epoch 0] Batch 641, Loss 0.34777751564979553\n",
      "[Training Epoch 0] Batch 642, Loss 0.3769802749156952\n",
      "[Training Epoch 0] Batch 643, Loss 0.38295066356658936\n",
      "[Training Epoch 0] Batch 644, Loss 0.3832302689552307\n",
      "[Training Epoch 0] Batch 645, Loss 0.3789188265800476\n",
      "[Training Epoch 0] Batch 646, Loss 0.3860577940940857\n",
      "[Training Epoch 0] Batch 647, Loss 0.4078986644744873\n",
      "[Training Epoch 0] Batch 648, Loss 0.35814356803894043\n",
      "[Training Epoch 0] Batch 649, Loss 0.3808666467666626\n",
      "[Training Epoch 0] Batch 650, Loss 0.3985458314418793\n",
      "[Training Epoch 0] Batch 651, Loss 0.36407867074012756\n",
      "[Training Epoch 0] Batch 652, Loss 0.3973349332809448\n",
      "[Training Epoch 0] Batch 653, Loss 0.3623729944229126\n",
      "[Training Epoch 0] Batch 654, Loss 0.4099103808403015\n",
      "[Training Epoch 0] Batch 655, Loss 0.38890340924263\n",
      "[Training Epoch 0] Batch 656, Loss 0.414899080991745\n",
      "[Training Epoch 0] Batch 657, Loss 0.34927648305892944\n",
      "[Training Epoch 0] Batch 658, Loss 0.3640405535697937\n",
      "[Training Epoch 0] Batch 659, Loss 0.36059123277664185\n",
      "[Training Epoch 0] Batch 660, Loss 0.3802958130836487\n",
      "[Training Epoch 0] Batch 661, Loss 0.36169883608818054\n",
      "[Training Epoch 0] Batch 662, Loss 0.3757925033569336\n",
      "[Training Epoch 0] Batch 663, Loss 0.3922458589076996\n",
      "[Training Epoch 0] Batch 664, Loss 0.37585800886154175\n",
      "[Training Epoch 0] Batch 665, Loss 0.34964579343795776\n",
      "[Training Epoch 0] Batch 666, Loss 0.36928698420524597\n",
      "[Training Epoch 0] Batch 667, Loss 0.39295998215675354\n",
      "[Training Epoch 0] Batch 668, Loss 0.36619502305984497\n",
      "[Training Epoch 0] Batch 669, Loss 0.3912865221500397\n",
      "[Training Epoch 0] Batch 670, Loss 0.3851442337036133\n",
      "[Training Epoch 0] Batch 671, Loss 0.35007166862487793\n",
      "[Training Epoch 0] Batch 672, Loss 0.34718960523605347\n",
      "[Training Epoch 0] Batch 673, Loss 0.3707008361816406\n",
      "[Training Epoch 0] Batch 674, Loss 0.402105450630188\n",
      "[Training Epoch 0] Batch 675, Loss 0.3883615732192993\n",
      "[Training Epoch 0] Batch 676, Loss 0.3602865934371948\n",
      "[Training Epoch 0] Batch 677, Loss 0.3839713931083679\n",
      "[Training Epoch 0] Batch 678, Loss 0.4197978675365448\n",
      "[Training Epoch 0] Batch 679, Loss 0.3644159436225891\n",
      "[Training Epoch 0] Batch 680, Loss 0.37740853428840637\n",
      "[Training Epoch 0] Batch 681, Loss 0.3964235484600067\n",
      "[Training Epoch 0] Batch 682, Loss 0.3800746202468872\n",
      "[Training Epoch 0] Batch 683, Loss 0.3961027264595032\n",
      "[Training Epoch 0] Batch 684, Loss 0.3825950026512146\n",
      "[Training Epoch 0] Batch 685, Loss 0.3633633852005005\n",
      "[Training Epoch 0] Batch 686, Loss 0.3449101448059082\n",
      "[Training Epoch 0] Batch 687, Loss 0.3780917227268219\n",
      "[Training Epoch 0] Batch 688, Loss 0.37271973490715027\n",
      "[Training Epoch 0] Batch 689, Loss 0.38407468795776367\n",
      "[Training Epoch 0] Batch 690, Loss 0.3783666491508484\n",
      "[Training Epoch 0] Batch 691, Loss 0.3801146149635315\n",
      "[Training Epoch 0] Batch 692, Loss 0.350435346364975\n",
      "[Training Epoch 0] Batch 693, Loss 0.3652140200138092\n",
      "[Training Epoch 0] Batch 694, Loss 0.3422500789165497\n",
      "[Training Epoch 0] Batch 695, Loss 0.35870233178138733\n",
      "[Training Epoch 0] Batch 696, Loss 0.3798615038394928\n",
      "[Training Epoch 0] Batch 697, Loss 0.3596188426017761\n",
      "[Training Epoch 0] Batch 698, Loss 0.3633144497871399\n",
      "[Training Epoch 0] Batch 699, Loss 0.3462960720062256\n",
      "[Training Epoch 0] Batch 700, Loss 0.3609667420387268\n",
      "[Training Epoch 0] Batch 701, Loss 0.38058486580848694\n",
      "[Training Epoch 0] Batch 702, Loss 0.37207597494125366\n",
      "[Training Epoch 0] Batch 703, Loss 0.3632314205169678\n",
      "[Training Epoch 0] Batch 704, Loss 0.37486761808395386\n",
      "[Training Epoch 0] Batch 705, Loss 0.37075674533843994\n",
      "[Training Epoch 0] Batch 706, Loss 0.420889288187027\n",
      "[Training Epoch 0] Batch 707, Loss 0.38428694009780884\n",
      "[Training Epoch 0] Batch 708, Loss 0.3636331558227539\n",
      "[Training Epoch 0] Batch 709, Loss 0.3933873474597931\n",
      "[Training Epoch 0] Batch 710, Loss 0.3558996915817261\n",
      "[Training Epoch 0] Batch 711, Loss 0.4052887260913849\n",
      "[Training Epoch 0] Batch 712, Loss 0.376236230134964\n",
      "[Training Epoch 0] Batch 713, Loss 0.3533301055431366\n",
      "[Training Epoch 0] Batch 714, Loss 0.36884671449661255\n",
      "[Training Epoch 0] Batch 715, Loss 0.3595793545246124\n",
      "[Training Epoch 0] Batch 716, Loss 0.37294071912765503\n",
      "[Training Epoch 0] Batch 717, Loss 0.33676379919052124\n",
      "[Training Epoch 0] Batch 718, Loss 0.3639557361602783\n",
      "[Training Epoch 0] Batch 719, Loss 0.3509669601917267\n",
      "[Training Epoch 0] Batch 720, Loss 0.35946357250213623\n",
      "[Training Epoch 0] Batch 721, Loss 0.38598471879959106\n",
      "[Training Epoch 0] Batch 722, Loss 0.3398270606994629\n",
      "[Training Epoch 0] Batch 723, Loss 0.35388216376304626\n",
      "[Training Epoch 0] Batch 724, Loss 0.3983278274536133\n",
      "[Training Epoch 0] Batch 725, Loss 0.3985157012939453\n",
      "[Training Epoch 0] Batch 726, Loss 0.398018479347229\n",
      "[Training Epoch 0] Batch 727, Loss 0.36523517966270447\n",
      "[Training Epoch 0] Batch 728, Loss 0.3741004168987274\n",
      "[Training Epoch 0] Batch 729, Loss 0.3113495111465454\n",
      "[Training Epoch 0] Batch 730, Loss 0.35910850763320923\n",
      "[Training Epoch 0] Batch 731, Loss 0.36450380086898804\n",
      "[Training Epoch 0] Batch 732, Loss 0.3861721158027649\n",
      "[Training Epoch 0] Batch 733, Loss 0.37857162952423096\n",
      "[Training Epoch 0] Batch 734, Loss 0.3976232707500458\n",
      "[Training Epoch 0] Batch 735, Loss 0.3560422360897064\n",
      "[Training Epoch 0] Batch 736, Loss 0.37632450461387634\n",
      "[Training Epoch 0] Batch 737, Loss 0.3936791718006134\n",
      "[Training Epoch 0] Batch 738, Loss 0.3768489360809326\n",
      "[Training Epoch 0] Batch 739, Loss 0.38029035925865173\n",
      "[Training Epoch 0] Batch 740, Loss 0.33669206500053406\n",
      "[Training Epoch 0] Batch 741, Loss 0.37987902760505676\n",
      "[Training Epoch 0] Batch 742, Loss 0.35204017162323\n",
      "[Training Epoch 0] Batch 743, Loss 0.37921538949012756\n",
      "[Training Epoch 0] Batch 744, Loss 0.3734777271747589\n",
      "[Training Epoch 0] Batch 745, Loss 0.3896786570549011\n",
      "[Training Epoch 0] Batch 746, Loss 0.37179261445999146\n",
      "[Training Epoch 0] Batch 747, Loss 0.3608083724975586\n",
      "[Training Epoch 0] Batch 748, Loss 0.3793284595012665\n",
      "[Training Epoch 0] Batch 749, Loss 0.3498770594596863\n",
      "[Training Epoch 0] Batch 750, Loss 0.3832942843437195\n",
      "[Training Epoch 0] Batch 751, Loss 0.39908310770988464\n",
      "[Training Epoch 0] Batch 752, Loss 0.3125733435153961\n",
      "[Training Epoch 0] Batch 753, Loss 0.3619983494281769\n",
      "[Training Epoch 0] Batch 754, Loss 0.38664624094963074\n",
      "[Training Epoch 0] Batch 755, Loss 0.3750246465206146\n",
      "[Training Epoch 0] Batch 756, Loss 0.32951098680496216\n",
      "[Training Epoch 0] Batch 757, Loss 0.3598416745662689\n",
      "[Training Epoch 0] Batch 758, Loss 0.36996883153915405\n",
      "[Training Epoch 0] Batch 759, Loss 0.3842017352581024\n",
      "[Training Epoch 0] Batch 760, Loss 0.37244266271591187\n",
      "[Training Epoch 0] Batch 761, Loss 0.33251309394836426\n",
      "[Training Epoch 0] Batch 762, Loss 0.35180890560150146\n",
      "[Training Epoch 0] Batch 763, Loss 0.35731300711631775\n",
      "[Training Epoch 0] Batch 764, Loss 0.3143339455127716\n",
      "[Training Epoch 0] Batch 765, Loss 0.4040676951408386\n",
      "[Training Epoch 0] Batch 766, Loss 0.34999585151672363\n",
      "[Training Epoch 0] Batch 767, Loss 0.369857519865036\n",
      "[Training Epoch 0] Batch 768, Loss 0.35933369398117065\n",
      "[Training Epoch 0] Batch 769, Loss 0.32589995861053467\n",
      "[Training Epoch 0] Batch 770, Loss 0.35025861859321594\n",
      "[Training Epoch 0] Batch 771, Loss 0.3841903805732727\n",
      "[Training Epoch 0] Batch 772, Loss 0.35248124599456787\n",
      "[Training Epoch 0] Batch 773, Loss 0.35376036167144775\n",
      "[Training Epoch 0] Batch 774, Loss 0.35223984718322754\n",
      "[Training Epoch 0] Batch 775, Loss 0.3441537022590637\n",
      "[Training Epoch 0] Batch 776, Loss 0.3369159400463104\n",
      "[Training Epoch 0] Batch 777, Loss 0.3597354292869568\n",
      "[Training Epoch 0] Batch 778, Loss 0.34197404980659485\n",
      "[Training Epoch 0] Batch 779, Loss 0.3275240659713745\n",
      "[Training Epoch 0] Batch 780, Loss 0.3718736171722412\n",
      "[Training Epoch 0] Batch 781, Loss 0.34707018733024597\n",
      "[Training Epoch 0] Batch 782, Loss 0.3735032379627228\n",
      "[Training Epoch 0] Batch 783, Loss 0.3574306070804596\n",
      "[Training Epoch 0] Batch 784, Loss 0.31838342547416687\n",
      "[Training Epoch 0] Batch 785, Loss 0.37803488969802856\n",
      "[Training Epoch 0] Batch 786, Loss 0.3729724586009979\n",
      "[Training Epoch 0] Batch 787, Loss 0.33521825075149536\n",
      "[Training Epoch 0] Batch 788, Loss 0.34239301085472107\n",
      "[Training Epoch 0] Batch 789, Loss 0.38325780630111694\n",
      "[Training Epoch 0] Batch 790, Loss 0.3538077473640442\n",
      "[Training Epoch 0] Batch 791, Loss 0.32122209668159485\n",
      "[Training Epoch 0] Batch 792, Loss 0.2983011305332184\n",
      "[Training Epoch 0] Batch 793, Loss 0.35478854179382324\n",
      "[Training Epoch 0] Batch 794, Loss 0.3501133620738983\n",
      "[Training Epoch 0] Batch 795, Loss 0.3653688132762909\n",
      "[Training Epoch 0] Batch 796, Loss 0.3680744469165802\n",
      "[Training Epoch 0] Batch 797, Loss 0.34549832344055176\n",
      "[Training Epoch 0] Batch 798, Loss 0.3322654962539673\n",
      "[Training Epoch 0] Batch 799, Loss 0.33217132091522217\n",
      "[Training Epoch 0] Batch 800, Loss 0.3420102000236511\n",
      "[Training Epoch 0] Batch 801, Loss 0.39047515392303467\n",
      "[Training Epoch 0] Batch 802, Loss 0.34582623839378357\n",
      "[Training Epoch 0] Batch 803, Loss 0.35118815302848816\n",
      "[Training Epoch 0] Batch 804, Loss 0.3785364329814911\n",
      "[Training Epoch 0] Batch 805, Loss 0.3295372724533081\n",
      "[Training Epoch 0] Batch 806, Loss 0.3526773452758789\n",
      "[Training Epoch 0] Batch 807, Loss 0.40569573640823364\n",
      "[Training Epoch 0] Batch 808, Loss 0.3577880561351776\n",
      "[Training Epoch 0] Batch 809, Loss 0.3795563280582428\n",
      "[Training Epoch 0] Batch 810, Loss 0.35469576716423035\n",
      "[Training Epoch 0] Batch 811, Loss 0.31971031427383423\n",
      "[Training Epoch 0] Batch 812, Loss 0.36636558175086975\n",
      "[Training Epoch 0] Batch 813, Loss 0.36516073346138\n",
      "[Training Epoch 0] Batch 814, Loss 0.3872697353363037\n",
      "[Training Epoch 0] Batch 815, Loss 0.39118656516075134\n",
      "[Training Epoch 0] Batch 816, Loss 0.3362478017807007\n",
      "[Training Epoch 0] Batch 817, Loss 0.3758898973464966\n",
      "[Training Epoch 0] Batch 818, Loss 0.36473989486694336\n",
      "[Training Epoch 0] Batch 819, Loss 0.366421103477478\n",
      "[Training Epoch 0] Batch 820, Loss 0.3217107653617859\n",
      "[Training Epoch 0] Batch 821, Loss 0.3255816102027893\n",
      "[Training Epoch 0] Batch 822, Loss 0.3793456554412842\n",
      "[Training Epoch 0] Batch 823, Loss 0.36489003896713257\n",
      "[Training Epoch 0] Batch 824, Loss 0.34887826442718506\n",
      "[Training Epoch 0] Batch 825, Loss 0.33326610922813416\n",
      "[Training Epoch 0] Batch 826, Loss 0.35176903009414673\n",
      "[Training Epoch 0] Batch 827, Loss 0.33988383412361145\n",
      "[Training Epoch 0] Batch 828, Loss 0.32852545380592346\n",
      "[Training Epoch 0] Batch 829, Loss 0.3083333671092987\n",
      "[Training Epoch 0] Batch 830, Loss 0.36349552869796753\n",
      "[Training Epoch 0] Batch 831, Loss 0.3736739158630371\n",
      "[Training Epoch 0] Batch 832, Loss 0.33337414264678955\n",
      "[Training Epoch 0] Batch 833, Loss 0.37015727162361145\n",
      "[Training Epoch 0] Batch 834, Loss 0.32945138216018677\n",
      "[Training Epoch 0] Batch 835, Loss 0.36872631311416626\n",
      "[Training Epoch 0] Batch 836, Loss 0.3624951243400574\n",
      "[Training Epoch 0] Batch 837, Loss 0.3492668867111206\n",
      "[Training Epoch 0] Batch 838, Loss 0.36354559659957886\n",
      "[Training Epoch 0] Batch 839, Loss 0.30400946736335754\n",
      "[Training Epoch 0] Batch 840, Loss 0.38272741436958313\n",
      "[Training Epoch 0] Batch 841, Loss 0.3717495799064636\n",
      "[Training Epoch 0] Batch 842, Loss 0.3453162908554077\n",
      "[Training Epoch 0] Batch 843, Loss 0.36469653248786926\n",
      "[Training Epoch 0] Batch 844, Loss 0.3885820508003235\n",
      "[Training Epoch 0] Batch 845, Loss 0.3720753490924835\n",
      "[Training Epoch 0] Batch 846, Loss 0.3533260226249695\n",
      "[Training Epoch 0] Batch 847, Loss 0.34656447172164917\n",
      "[Training Epoch 0] Batch 848, Loss 0.3576229214668274\n",
      "[Training Epoch 0] Batch 849, Loss 0.337105929851532\n",
      "[Training Epoch 0] Batch 850, Loss 0.34482741355895996\n",
      "[Training Epoch 0] Batch 851, Loss 0.36572209000587463\n",
      "[Training Epoch 0] Batch 852, Loss 0.329508513212204\n",
      "[Training Epoch 0] Batch 853, Loss 0.3204202950000763\n",
      "[Training Epoch 0] Batch 854, Loss 0.3479670286178589\n",
      "[Training Epoch 0] Batch 855, Loss 0.3613186776638031\n",
      "[Training Epoch 0] Batch 856, Loss 0.32048556208610535\n",
      "[Training Epoch 0] Batch 857, Loss 0.3269559144973755\n",
      "[Training Epoch 0] Batch 858, Loss 0.37498199939727783\n",
      "[Training Epoch 0] Batch 859, Loss 0.35373997688293457\n",
      "[Training Epoch 0] Batch 860, Loss 0.363084077835083\n",
      "[Training Epoch 0] Batch 861, Loss 0.34901320934295654\n",
      "[Training Epoch 0] Batch 862, Loss 0.3740833103656769\n",
      "[Training Epoch 0] Batch 863, Loss 0.37295570969581604\n",
      "[Training Epoch 0] Batch 864, Loss 0.37590640783309937\n",
      "[Training Epoch 0] Batch 865, Loss 0.32838478684425354\n",
      "[Training Epoch 0] Batch 866, Loss 0.3349378705024719\n",
      "[Training Epoch 0] Batch 867, Loss 0.32668423652648926\n",
      "[Training Epoch 0] Batch 868, Loss 0.36674895882606506\n",
      "[Training Epoch 0] Batch 869, Loss 0.35307058691978455\n",
      "[Training Epoch 0] Batch 870, Loss 0.34410345554351807\n",
      "[Training Epoch 0] Batch 871, Loss 0.3528553247451782\n",
      "[Training Epoch 0] Batch 872, Loss 0.3603375256061554\n",
      "[Training Epoch 0] Batch 873, Loss 0.3558861315250397\n",
      "[Training Epoch 0] Batch 874, Loss 0.35723763704299927\n",
      "[Training Epoch 0] Batch 875, Loss 0.33931487798690796\n",
      "[Training Epoch 0] Batch 876, Loss 0.3651641309261322\n",
      "[Training Epoch 0] Batch 877, Loss 0.36592230200767517\n",
      "[Training Epoch 0] Batch 878, Loss 0.3668147921562195\n",
      "[Training Epoch 0] Batch 879, Loss 0.36998242139816284\n",
      "[Training Epoch 0] Batch 880, Loss 0.35700154304504395\n",
      "[Training Epoch 0] Batch 881, Loss 0.338568776845932\n",
      "[Training Epoch 0] Batch 882, Loss 0.31298571825027466\n",
      "[Training Epoch 0] Batch 883, Loss 0.38000431656837463\n",
      "[Training Epoch 0] Batch 884, Loss 0.3535694181919098\n",
      "[Training Epoch 0] Batch 885, Loss 0.38002586364746094\n",
      "[Training Epoch 0] Batch 886, Loss 0.3225529193878174\n",
      "[Training Epoch 0] Batch 887, Loss 0.33269184827804565\n",
      "[Training Epoch 0] Batch 888, Loss 0.3172570765018463\n",
      "[Training Epoch 0] Batch 889, Loss 0.3336186707019806\n",
      "[Training Epoch 0] Batch 890, Loss 0.3412986993789673\n",
      "[Training Epoch 0] Batch 891, Loss 0.3594854772090912\n",
      "[Training Epoch 0] Batch 892, Loss 0.3332192599773407\n",
      "[Training Epoch 0] Batch 893, Loss 0.3463096022605896\n",
      "[Training Epoch 0] Batch 894, Loss 0.35870736837387085\n",
      "[Training Epoch 0] Batch 895, Loss 0.3875758647918701\n",
      "[Training Epoch 0] Batch 896, Loss 0.3660989999771118\n",
      "[Training Epoch 0] Batch 897, Loss 0.3531090021133423\n",
      "[Training Epoch 0] Batch 898, Loss 0.327348917722702\n",
      "[Training Epoch 0] Batch 899, Loss 0.3223263919353485\n",
      "[Training Epoch 0] Batch 900, Loss 0.3541594445705414\n",
      "[Training Epoch 0] Batch 901, Loss 0.3429625332355499\n",
      "[Training Epoch 0] Batch 902, Loss 0.33819112181663513\n",
      "[Training Epoch 0] Batch 903, Loss 0.3235112428665161\n",
      "[Training Epoch 0] Batch 904, Loss 0.35833653807640076\n",
      "[Training Epoch 0] Batch 905, Loss 0.3770589828491211\n",
      "[Training Epoch 0] Batch 906, Loss 0.33720865845680237\n",
      "[Training Epoch 0] Batch 907, Loss 0.34727004170417786\n",
      "[Training Epoch 0] Batch 908, Loss 0.3305748403072357\n",
      "[Training Epoch 0] Batch 909, Loss 0.365376353263855\n",
      "[Training Epoch 0] Batch 910, Loss 0.35180363059043884\n",
      "[Training Epoch 0] Batch 911, Loss 0.30771616101264954\n",
      "[Training Epoch 0] Batch 912, Loss 0.3708823323249817\n",
      "[Training Epoch 0] Batch 913, Loss 0.36193373799324036\n",
      "[Training Epoch 0] Batch 914, Loss 0.35711196064949036\n",
      "[Training Epoch 0] Batch 915, Loss 0.34262633323669434\n",
      "[Training Epoch 0] Batch 916, Loss 0.3560425639152527\n",
      "[Training Epoch 0] Batch 917, Loss 0.3229375183582306\n",
      "[Training Epoch 0] Batch 918, Loss 0.31860464811325073\n",
      "[Training Epoch 0] Batch 919, Loss 0.32182076573371887\n",
      "[Training Epoch 0] Batch 920, Loss 0.32595038414001465\n",
      "[Training Epoch 0] Batch 921, Loss 0.35922691226005554\n",
      "[Training Epoch 0] Batch 922, Loss 0.3403765857219696\n",
      "[Training Epoch 0] Batch 923, Loss 0.36918655037879944\n",
      "[Training Epoch 0] Batch 924, Loss 0.33713918924331665\n",
      "[Training Epoch 0] Batch 925, Loss 0.3575986623764038\n",
      "[Training Epoch 0] Batch 926, Loss 0.3582654893398285\n",
      "[Training Epoch 0] Batch 927, Loss 0.36370187997817993\n",
      "[Training Epoch 0] Batch 928, Loss 0.3554360866546631\n",
      "[Training Epoch 0] Batch 929, Loss 0.34306713938713074\n",
      "[Training Epoch 0] Batch 930, Loss 0.377750039100647\n",
      "[Training Epoch 0] Batch 931, Loss 0.3744751214981079\n",
      "[Training Epoch 0] Batch 932, Loss 0.36172083020210266\n",
      "[Training Epoch 0] Batch 933, Loss 0.3000405728816986\n",
      "[Training Epoch 0] Batch 934, Loss 0.3699982762336731\n",
      "[Training Epoch 0] Batch 935, Loss 0.35795918107032776\n",
      "[Training Epoch 0] Batch 936, Loss 0.3253193199634552\n",
      "[Training Epoch 0] Batch 937, Loss 0.33835554122924805\n",
      "[Training Epoch 0] Batch 938, Loss 0.346591055393219\n",
      "[Training Epoch 0] Batch 939, Loss 0.3387316167354584\n",
      "[Training Epoch 0] Batch 940, Loss 0.3217816948890686\n",
      "[Training Epoch 0] Batch 941, Loss 0.33792808651924133\n",
      "[Training Epoch 0] Batch 942, Loss 0.3181903064250946\n",
      "[Training Epoch 0] Batch 943, Loss 0.37540847063064575\n",
      "[Training Epoch 0] Batch 944, Loss 0.36333200335502625\n",
      "[Training Epoch 0] Batch 945, Loss 0.36719751358032227\n",
      "[Training Epoch 0] Batch 946, Loss 0.3658823072910309\n",
      "[Training Epoch 0] Batch 947, Loss 0.37293678522109985\n",
      "[Training Epoch 0] Batch 948, Loss 0.3415107727050781\n",
      "[Training Epoch 0] Batch 949, Loss 0.3568975627422333\n",
      "[Training Epoch 0] Batch 950, Loss 0.35254597663879395\n",
      "[Training Epoch 0] Batch 951, Loss 0.33754974603652954\n",
      "[Training Epoch 0] Batch 952, Loss 0.35405951738357544\n",
      "[Training Epoch 0] Batch 953, Loss 0.320180743932724\n",
      "[Training Epoch 0] Batch 954, Loss 0.3353937864303589\n",
      "[Training Epoch 0] Batch 955, Loss 0.33185476064682007\n",
      "[Training Epoch 0] Batch 956, Loss 0.3301613926887512\n",
      "[Training Epoch 0] Batch 957, Loss 0.38346362113952637\n",
      "[Training Epoch 0] Batch 958, Loss 0.3202623128890991\n",
      "[Training Epoch 0] Batch 959, Loss 0.3052133619785309\n",
      "[Training Epoch 0] Batch 960, Loss 0.324291467666626\n",
      "[Training Epoch 0] Batch 961, Loss 0.3601241409778595\n",
      "[Training Epoch 0] Batch 962, Loss 0.36622047424316406\n",
      "[Training Epoch 0] Batch 963, Loss 0.345895379781723\n",
      "[Training Epoch 0] Batch 964, Loss 0.3813890218734741\n",
      "[Training Epoch 0] Batch 965, Loss 0.34951016306877136\n",
      "[Training Epoch 0] Batch 966, Loss 0.3556973338127136\n",
      "[Training Epoch 0] Batch 967, Loss 0.3618617057800293\n",
      "[Training Epoch 0] Batch 968, Loss 0.33920133113861084\n",
      "[Training Epoch 0] Batch 969, Loss 0.33968669176101685\n",
      "[Training Epoch 0] Batch 970, Loss 0.375553697347641\n",
      "[Training Epoch 0] Batch 971, Loss 0.3471500277519226\n",
      "[Training Epoch 0] Batch 972, Loss 0.35567334294319153\n",
      "[Training Epoch 0] Batch 973, Loss 0.36492884159088135\n",
      "[Training Epoch 0] Batch 974, Loss 0.3234959840774536\n",
      "[Training Epoch 0] Batch 975, Loss 0.32456398010253906\n",
      "[Training Epoch 0] Batch 976, Loss 0.3305598795413971\n",
      "[Training Epoch 0] Batch 977, Loss 0.32316455245018005\n",
      "[Training Epoch 0] Batch 978, Loss 0.329496830701828\n",
      "[Training Epoch 0] Batch 979, Loss 0.3445824682712555\n",
      "[Training Epoch 0] Batch 980, Loss 0.35208597779273987\n",
      "[Training Epoch 0] Batch 981, Loss 0.3546645939350128\n",
      "[Training Epoch 0] Batch 982, Loss 0.3086474537849426\n",
      "[Training Epoch 0] Batch 983, Loss 0.33926481008529663\n",
      "[Training Epoch 0] Batch 984, Loss 0.33465802669525146\n",
      "[Training Epoch 0] Batch 985, Loss 0.3443446755409241\n",
      "[Training Epoch 0] Batch 986, Loss 0.3329848647117615\n",
      "[Training Epoch 0] Batch 987, Loss 0.33489784598350525\n",
      "[Training Epoch 0] Batch 988, Loss 0.3363468050956726\n",
      "[Training Epoch 0] Batch 989, Loss 0.33271223306655884\n",
      "[Training Epoch 0] Batch 990, Loss 0.3514884114265442\n",
      "[Training Epoch 0] Batch 991, Loss 0.37031087279319763\n",
      "[Training Epoch 0] Batch 992, Loss 0.31832051277160645\n",
      "[Training Epoch 0] Batch 993, Loss 0.3413466215133667\n",
      "[Training Epoch 0] Batch 994, Loss 0.3469768762588501\n",
      "[Training Epoch 0] Batch 995, Loss 0.316472589969635\n",
      "[Training Epoch 0] Batch 996, Loss 0.3400532603263855\n",
      "[Training Epoch 0] Batch 997, Loss 0.34840303659439087\n",
      "[Training Epoch 0] Batch 998, Loss 0.34206780791282654\n",
      "[Training Epoch 0] Batch 999, Loss 0.34030646085739136\n",
      "[Training Epoch 0] Batch 1000, Loss 0.3438493013381958\n",
      "[Training Epoch 0] Batch 1001, Loss 0.36966100335121155\n",
      "[Training Epoch 0] Batch 1002, Loss 0.3511701226234436\n",
      "[Training Epoch 0] Batch 1003, Loss 0.33260905742645264\n",
      "[Training Epoch 0] Batch 1004, Loss 0.308762788772583\n",
      "[Training Epoch 0] Batch 1005, Loss 0.31841713190078735\n",
      "[Training Epoch 0] Batch 1006, Loss 0.3156338930130005\n",
      "[Training Epoch 0] Batch 1007, Loss 0.3795115351676941\n",
      "[Training Epoch 0] Batch 1008, Loss 0.3789704144001007\n",
      "[Training Epoch 0] Batch 1009, Loss 0.33631590008735657\n",
      "[Training Epoch 0] Batch 1010, Loss 0.36302709579467773\n",
      "[Training Epoch 0] Batch 1011, Loss 0.3506801128387451\n",
      "[Training Epoch 0] Batch 1012, Loss 0.35624632239341736\n",
      "[Training Epoch 0] Batch 1013, Loss 0.336204469203949\n",
      "[Training Epoch 0] Batch 1014, Loss 0.3494083285331726\n",
      "[Training Epoch 0] Batch 1015, Loss 0.37042397260665894\n",
      "[Training Epoch 0] Batch 1016, Loss 0.3048115372657776\n",
      "[Training Epoch 0] Batch 1017, Loss 0.3278728425502777\n",
      "[Training Epoch 0] Batch 1018, Loss 0.3455255329608917\n",
      "[Training Epoch 0] Batch 1019, Loss 0.3364143371582031\n",
      "[Training Epoch 0] Batch 1020, Loss 0.3402865529060364\n",
      "[Training Epoch 0] Batch 1021, Loss 0.33112287521362305\n",
      "[Training Epoch 0] Batch 1022, Loss 0.315591424703598\n",
      "[Training Epoch 0] Batch 1023, Loss 0.37955665588378906\n",
      "[Training Epoch 0] Batch 1024, Loss 0.3346920907497406\n",
      "[Training Epoch 0] Batch 1025, Loss 0.3812124729156494\n",
      "[Training Epoch 0] Batch 1026, Loss 0.33940473198890686\n",
      "[Training Epoch 0] Batch 1027, Loss 0.35925495624542236\n",
      "[Training Epoch 0] Batch 1028, Loss 0.3752758204936981\n",
      "[Training Epoch 0] Batch 1029, Loss 0.3400718569755554\n",
      "[Training Epoch 0] Batch 1030, Loss 0.3812379240989685\n",
      "[Training Epoch 0] Batch 1031, Loss 0.33938148617744446\n",
      "[Training Epoch 0] Batch 1032, Loss 0.3350563049316406\n",
      "[Training Epoch 0] Batch 1033, Loss 0.3178996443748474\n",
      "[Training Epoch 0] Batch 1034, Loss 0.34100863337516785\n",
      "[Training Epoch 0] Batch 1035, Loss 0.3543355166912079\n",
      "[Training Epoch 0] Batch 1036, Loss 0.3420824408531189\n",
      "[Training Epoch 0] Batch 1037, Loss 0.32640340924263\n",
      "[Training Epoch 0] Batch 1038, Loss 0.3329594135284424\n",
      "[Training Epoch 0] Batch 1039, Loss 0.3536682724952698\n",
      "[Training Epoch 0] Batch 1040, Loss 0.3496588170528412\n",
      "[Training Epoch 0] Batch 1041, Loss 0.32495757937431335\n",
      "[Training Epoch 0] Batch 1042, Loss 0.3218325972557068\n",
      "[Training Epoch 0] Batch 1043, Loss 0.34814056754112244\n",
      "[Training Epoch 0] Batch 1044, Loss 0.3322114944458008\n",
      "[Training Epoch 0] Batch 1045, Loss 0.35163548588752747\n",
      "[Training Epoch 0] Batch 1046, Loss 0.32412734627723694\n",
      "[Training Epoch 0] Batch 1047, Loss 0.32975244522094727\n",
      "[Training Epoch 0] Batch 1048, Loss 0.34831303358078003\n",
      "[Training Epoch 0] Batch 1049, Loss 0.3343659043312073\n",
      "[Training Epoch 0] Batch 1050, Loss 0.32953518629074097\n",
      "[Training Epoch 0] Batch 1051, Loss 0.3478386402130127\n",
      "[Training Epoch 0] Batch 1052, Loss 0.3288317322731018\n",
      "[Training Epoch 0] Batch 1053, Loss 0.3450251817703247\n",
      "[Training Epoch 0] Batch 1054, Loss 0.33454060554504395\n",
      "[Training Epoch 0] Batch 1055, Loss 0.3450806736946106\n",
      "[Training Epoch 0] Batch 1056, Loss 0.34136009216308594\n",
      "[Training Epoch 0] Batch 1057, Loss 0.314805269241333\n",
      "[Training Epoch 0] Batch 1058, Loss 0.32443752884864807\n",
      "[Training Epoch 0] Batch 1059, Loss 0.3480200469493866\n",
      "[Training Epoch 0] Batch 1060, Loss 0.3453650176525116\n",
      "[Training Epoch 0] Batch 1061, Loss 0.3261294364929199\n",
      "[Training Epoch 0] Batch 1062, Loss 0.3355228900909424\n",
      "[Training Epoch 0] Batch 1063, Loss 0.332217276096344\n",
      "[Training Epoch 0] Batch 1064, Loss 0.36360421776771545\n",
      "[Training Epoch 0] Batch 1065, Loss 0.32987648248672485\n",
      "[Training Epoch 0] Batch 1066, Loss 0.33367836475372314\n",
      "[Training Epoch 0] Batch 1067, Loss 0.3178473711013794\n",
      "[Training Epoch 0] Batch 1068, Loss 0.3480914831161499\n",
      "[Training Epoch 0] Batch 1069, Loss 0.33662450313568115\n",
      "[Training Epoch 0] Batch 1070, Loss 0.3574627935886383\n",
      "[Training Epoch 0] Batch 1071, Loss 0.32167866826057434\n",
      "[Training Epoch 0] Batch 1072, Loss 0.3577478528022766\n",
      "[Training Epoch 0] Batch 1073, Loss 0.3520612418651581\n",
      "[Training Epoch 0] Batch 1074, Loss 0.35258886218070984\n",
      "[Training Epoch 0] Batch 1075, Loss 0.3666445016860962\n",
      "[Training Epoch 0] Batch 1076, Loss 0.32489731907844543\n",
      "[Training Epoch 0] Batch 1077, Loss 0.30446189641952515\n",
      "[Training Epoch 0] Batch 1078, Loss 0.3218700587749481\n",
      "[Training Epoch 0] Batch 1079, Loss 0.3578537404537201\n",
      "[Training Epoch 0] Batch 1080, Loss 0.3074928820133209\n",
      "[Training Epoch 0] Batch 1081, Loss 0.39414554834365845\n",
      "[Training Epoch 0] Batch 1082, Loss 0.3330601751804352\n",
      "[Training Epoch 0] Batch 1083, Loss 0.33240506052970886\n",
      "[Training Epoch 0] Batch 1084, Loss 0.35876867175102234\n",
      "[Training Epoch 0] Batch 1085, Loss 0.3347187340259552\n",
      "[Training Epoch 0] Batch 1086, Loss 0.34923991560935974\n",
      "[Training Epoch 0] Batch 1087, Loss 0.3557887375354767\n",
      "[Training Epoch 0] Batch 1088, Loss 0.3568553328514099\n",
      "[Training Epoch 0] Batch 1089, Loss 0.30702391266822815\n",
      "[Training Epoch 0] Batch 1090, Loss 0.34535008668899536\n",
      "[Training Epoch 0] Batch 1091, Loss 0.333421915769577\n",
      "[Training Epoch 0] Batch 1092, Loss 0.29982131719589233\n",
      "[Training Epoch 0] Batch 1093, Loss 0.3637065887451172\n",
      "[Training Epoch 0] Batch 1094, Loss 0.3353046774864197\n",
      "[Training Epoch 0] Batch 1095, Loss 0.32863619923591614\n",
      "[Training Epoch 0] Batch 1096, Loss 0.3221285939216614\n",
      "[Training Epoch 0] Batch 1097, Loss 0.3260325789451599\n",
      "[Training Epoch 0] Batch 1098, Loss 0.33522987365722656\n",
      "[Training Epoch 0] Batch 1099, Loss 0.333909273147583\n",
      "[Training Epoch 0] Batch 1100, Loss 0.3551316261291504\n",
      "[Training Epoch 0] Batch 1101, Loss 0.35908180475234985\n",
      "[Training Epoch 0] Batch 1102, Loss 0.3139040470123291\n",
      "[Training Epoch 0] Batch 1103, Loss 0.35619327425956726\n",
      "[Training Epoch 0] Batch 1104, Loss 0.34394800662994385\n",
      "[Training Epoch 0] Batch 1105, Loss 0.3076385259628296\n",
      "[Training Epoch 0] Batch 1106, Loss 0.3487342596054077\n",
      "[Training Epoch 0] Batch 1107, Loss 0.34978392720222473\n",
      "[Training Epoch 0] Batch 1108, Loss 0.3062778115272522\n",
      "[Training Epoch 0] Batch 1109, Loss 0.3766840696334839\n",
      "[Training Epoch 0] Batch 1110, Loss 0.321494996547699\n",
      "[Training Epoch 0] Batch 1111, Loss 0.34068888425827026\n",
      "[Training Epoch 0] Batch 1112, Loss 0.37573760747909546\n",
      "[Training Epoch 0] Batch 1113, Loss 0.3285866379737854\n",
      "[Training Epoch 0] Batch 1114, Loss 0.32537105679512024\n",
      "[Training Epoch 0] Batch 1115, Loss 0.2921162247657776\n",
      "[Training Epoch 0] Batch 1116, Loss 0.3727082312107086\n",
      "[Training Epoch 0] Batch 1117, Loss 0.36189892888069153\n",
      "[Training Epoch 0] Batch 1118, Loss 0.31346195936203003\n",
      "[Training Epoch 0] Batch 1119, Loss 0.3231421113014221\n",
      "[Training Epoch 0] Batch 1120, Loss 0.3173423409461975\n",
      "[Training Epoch 0] Batch 1121, Loss 0.379609078168869\n",
      "[Training Epoch 0] Batch 1122, Loss 0.3162866532802582\n",
      "[Training Epoch 0] Batch 1123, Loss 0.3297392427921295\n",
      "[Training Epoch 0] Batch 1124, Loss 0.3643610179424286\n",
      "[Training Epoch 0] Batch 1125, Loss 0.3261077105998993\n",
      "[Training Epoch 0] Batch 1126, Loss 0.35292187333106995\n",
      "[Training Epoch 0] Batch 1127, Loss 0.35512545704841614\n",
      "[Training Epoch 0] Batch 1128, Loss 0.3134133517742157\n",
      "[Training Epoch 0] Batch 1129, Loss 0.31847476959228516\n",
      "[Training Epoch 0] Batch 1130, Loss 0.3533862233161926\n",
      "[Training Epoch 0] Batch 1131, Loss 0.33531659841537476\n",
      "[Training Epoch 0] Batch 1132, Loss 0.341434121131897\n",
      "[Training Epoch 0] Batch 1133, Loss 0.3297737240791321\n",
      "[Training Epoch 0] Batch 1134, Loss 0.3279978632926941\n",
      "[Training Epoch 0] Batch 1135, Loss 0.33097735047340393\n",
      "[Training Epoch 0] Batch 1136, Loss 0.3151700794696808\n",
      "[Training Epoch 0] Batch 1137, Loss 0.32809245586395264\n",
      "[Training Epoch 0] Batch 1138, Loss 0.36086535453796387\n",
      "[Training Epoch 0] Batch 1139, Loss 0.34106266498565674\n",
      "[Training Epoch 0] Batch 1140, Loss 0.3331713378429413\n",
      "[Training Epoch 0] Batch 1141, Loss 0.3776797652244568\n",
      "[Training Epoch 0] Batch 1142, Loss 0.31927549839019775\n",
      "[Training Epoch 0] Batch 1143, Loss 0.3107740581035614\n",
      "[Training Epoch 0] Batch 1144, Loss 0.316991925239563\n",
      "[Training Epoch 0] Batch 1145, Loss 0.3296564817428589\n",
      "[Training Epoch 0] Batch 1146, Loss 0.34899142384529114\n",
      "[Training Epoch 0] Batch 1147, Loss 0.3206450939178467\n",
      "[Training Epoch 0] Batch 1148, Loss 0.34354591369628906\n",
      "[Training Epoch 0] Batch 1149, Loss 0.3220874071121216\n",
      "[Training Epoch 0] Batch 1150, Loss 0.3534497618675232\n",
      "[Training Epoch 0] Batch 1151, Loss 0.331261545419693\n",
      "[Training Epoch 0] Batch 1152, Loss 0.32640841603279114\n",
      "[Training Epoch 0] Batch 1153, Loss 0.34304091334342957\n",
      "[Training Epoch 0] Batch 1154, Loss 0.3273318409919739\n",
      "[Training Epoch 0] Batch 1155, Loss 0.3566752076148987\n",
      "[Training Epoch 0] Batch 1156, Loss 0.3059256672859192\n",
      "[Training Epoch 0] Batch 1157, Loss 0.3460773825645447\n",
      "[Training Epoch 0] Batch 1158, Loss 0.3101750910282135\n",
      "[Training Epoch 0] Batch 1159, Loss 0.35519880056381226\n",
      "[Training Epoch 0] Batch 1160, Loss 0.33576831221580505\n",
      "[Training Epoch 0] Batch 1161, Loss 0.3301866352558136\n",
      "[Training Epoch 0] Batch 1162, Loss 0.3083469271659851\n",
      "[Training Epoch 0] Batch 1163, Loss 0.3343982398509979\n",
      "[Training Epoch 0] Batch 1164, Loss 0.31613439321517944\n",
      "[Training Epoch 0] Batch 1165, Loss 0.344955176115036\n",
      "[Training Epoch 0] Batch 1166, Loss 0.3287108242511749\n",
      "[Training Epoch 0] Batch 1167, Loss 0.33546197414398193\n",
      "[Training Epoch 0] Batch 1168, Loss 0.33695605397224426\n",
      "[Training Epoch 0] Batch 1169, Loss 0.34578898549079895\n",
      "[Training Epoch 0] Batch 1170, Loss 0.32361388206481934\n",
      "[Training Epoch 0] Batch 1171, Loss 0.3733062148094177\n",
      "[Training Epoch 0] Batch 1172, Loss 0.3026931583881378\n",
      "[Training Epoch 0] Batch 1173, Loss 0.3611632287502289\n",
      "[Training Epoch 0] Batch 1174, Loss 0.3079087436199188\n",
      "[Training Epoch 0] Batch 1175, Loss 0.336840957403183\n",
      "[Training Epoch 0] Batch 1176, Loss 0.32491791248321533\n",
      "[Training Epoch 0] Batch 1177, Loss 0.31901490688323975\n",
      "[Training Epoch 0] Batch 1178, Loss 0.3147823214530945\n",
      "[Training Epoch 0] Batch 1179, Loss 0.3311159610748291\n",
      "[Training Epoch 0] Batch 1180, Loss 0.3260669708251953\n",
      "[Training Epoch 0] Batch 1181, Loss 0.34015756845474243\n",
      "[Training Epoch 0] Batch 1182, Loss 0.3523469865322113\n",
      "[Training Epoch 0] Batch 1183, Loss 0.32126733660697937\n",
      "[Training Epoch 0] Batch 1184, Loss 0.35473206639289856\n",
      "[Training Epoch 0] Batch 1185, Loss 0.35060107707977295\n",
      "[Training Epoch 0] Batch 1186, Loss 0.32456618547439575\n",
      "[Training Epoch 0] Batch 1187, Loss 0.3208826184272766\n",
      "[Training Epoch 0] Batch 1188, Loss 0.3625112771987915\n",
      "[Training Epoch 0] Batch 1189, Loss 0.33104246854782104\n",
      "[Training Epoch 0] Batch 1190, Loss 0.3263070583343506\n",
      "[Training Epoch 0] Batch 1191, Loss 0.32187697291374207\n",
      "[Training Epoch 0] Batch 1192, Loss 0.35437431931495667\n",
      "[Training Epoch 0] Batch 1193, Loss 0.33899447321891785\n",
      "[Training Epoch 0] Batch 1194, Loss 0.31378450989723206\n",
      "[Training Epoch 0] Batch 1195, Loss 0.3309885263442993\n",
      "[Training Epoch 0] Batch 1196, Loss 0.3395736813545227\n",
      "[Training Epoch 0] Batch 1197, Loss 0.3275302052497864\n",
      "[Training Epoch 0] Batch 1198, Loss 0.32247668504714966\n",
      "[Training Epoch 0] Batch 1199, Loss 0.31048011779785156\n",
      "[Training Epoch 0] Batch 1200, Loss 0.3629336357116699\n",
      "[Training Epoch 0] Batch 1201, Loss 0.31910061836242676\n",
      "[Training Epoch 0] Batch 1202, Loss 0.3485966622829437\n",
      "[Training Epoch 0] Batch 1203, Loss 0.3306577801704407\n",
      "[Training Epoch 0] Batch 1204, Loss 0.3299424648284912\n",
      "[Training Epoch 0] Batch 1205, Loss 0.3029290437698364\n",
      "[Training Epoch 0] Batch 1206, Loss 0.3217690587043762\n",
      "[Training Epoch 0] Batch 1207, Loss 0.3376447558403015\n",
      "[Training Epoch 0] Batch 1208, Loss 0.33666351437568665\n",
      "[Training Epoch 0] Batch 1209, Loss 0.3279635012149811\n",
      "[Training Epoch 0] Batch 1210, Loss 0.3352413773536682\n",
      "[Training Epoch 0] Batch 1211, Loss 0.34415653347969055\n",
      "[Training Epoch 0] Batch 1212, Loss 0.37126120924949646\n",
      "[Training Epoch 0] Batch 1213, Loss 0.34191566705703735\n",
      "[Training Epoch 0] Batch 1214, Loss 0.33484986424446106\n",
      "[Training Epoch 0] Batch 1215, Loss 0.32823753356933594\n",
      "[Training Epoch 0] Batch 1216, Loss 0.30937620997428894\n",
      "[Training Epoch 0] Batch 1217, Loss 0.31825685501098633\n",
      "[Training Epoch 0] Batch 1218, Loss 0.35707002878189087\n",
      "[Training Epoch 0] Batch 1219, Loss 0.3323802649974823\n",
      "[Training Epoch 0] Batch 1220, Loss 0.3213277757167816\n",
      "[Training Epoch 0] Batch 1221, Loss 0.3521658778190613\n",
      "[Training Epoch 0] Batch 1222, Loss 0.3390233516693115\n",
      "[Training Epoch 0] Batch 1223, Loss 0.3320689797401428\n",
      "[Training Epoch 0] Batch 1224, Loss 0.3310897648334503\n",
      "[Training Epoch 0] Batch 1225, Loss 0.3166154623031616\n",
      "[Training Epoch 0] Batch 1226, Loss 0.3413776159286499\n",
      "[Training Epoch 0] Batch 1227, Loss 0.39553871750831604\n",
      "[Training Epoch 0] Batch 1228, Loss 0.32904064655303955\n",
      "[Training Epoch 0] Batch 1229, Loss 0.3071645200252533\n",
      "[Training Epoch 0] Batch 1230, Loss 0.32501110434532166\n",
      "[Training Epoch 0] Batch 1231, Loss 0.35924798250198364\n",
      "[Training Epoch 0] Batch 1232, Loss 0.32073238492012024\n",
      "[Training Epoch 0] Batch 1233, Loss 0.37177062034606934\n",
      "[Training Epoch 0] Batch 1234, Loss 0.32134994864463806\n",
      "[Training Epoch 0] Batch 1235, Loss 0.31614893674850464\n",
      "[Training Epoch 0] Batch 1236, Loss 0.34373271465301514\n",
      "[Training Epoch 0] Batch 1237, Loss 0.3418593108654022\n",
      "[Training Epoch 0] Batch 1238, Loss 0.3448832333087921\n",
      "[Training Epoch 0] Batch 1239, Loss 0.3284057080745697\n",
      "[Training Epoch 0] Batch 1240, Loss 0.33854782581329346\n",
      "[Training Epoch 0] Batch 1241, Loss 0.3428402841091156\n",
      "[Training Epoch 0] Batch 1242, Loss 0.3265629708766937\n",
      "[Training Epoch 0] Batch 1243, Loss 0.3428006172180176\n",
      "[Training Epoch 0] Batch 1244, Loss 0.3232649564743042\n",
      "[Training Epoch 0] Batch 1245, Loss 0.33635228872299194\n",
      "[Training Epoch 0] Batch 1246, Loss 0.330943763256073\n",
      "[Training Epoch 0] Batch 1247, Loss 0.30152446031570435\n",
      "[Training Epoch 0] Batch 1248, Loss 0.33029788732528687\n",
      "[Training Epoch 0] Batch 1249, Loss 0.34354397654533386\n",
      "[Training Epoch 0] Batch 1250, Loss 0.31161704659461975\n",
      "[Training Epoch 0] Batch 1251, Loss 0.40039169788360596\n",
      "[Training Epoch 0] Batch 1252, Loss 0.35853931307792664\n",
      "[Training Epoch 0] Batch 1253, Loss 0.32043689489364624\n",
      "[Training Epoch 0] Batch 1254, Loss 0.32662105560302734\n",
      "[Training Epoch 0] Batch 1255, Loss 0.34539538621902466\n",
      "[Training Epoch 0] Batch 1256, Loss 0.35751521587371826\n",
      "[Training Epoch 0] Batch 1257, Loss 0.3374525308609009\n",
      "[Training Epoch 0] Batch 1258, Loss 0.34070897102355957\n",
      "[Training Epoch 0] Batch 1259, Loss 0.3636481463909149\n",
      "[Training Epoch 0] Batch 1260, Loss 0.31568437814712524\n",
      "[Training Epoch 0] Batch 1261, Loss 0.34472739696502686\n",
      "[Training Epoch 0] Batch 1262, Loss 0.29276424646377563\n",
      "[Training Epoch 0] Batch 1263, Loss 0.34493178129196167\n",
      "[Training Epoch 0] Batch 1264, Loss 0.3491631746292114\n",
      "[Training Epoch 0] Batch 1265, Loss 0.33994778990745544\n",
      "[Training Epoch 0] Batch 1266, Loss 0.3132837414741516\n",
      "[Training Epoch 0] Batch 1267, Loss 0.33147549629211426\n",
      "[Training Epoch 0] Batch 1268, Loss 0.3651093542575836\n",
      "[Training Epoch 0] Batch 1269, Loss 0.3073289394378662\n",
      "[Training Epoch 0] Batch 1270, Loss 0.3060753047466278\n",
      "[Training Epoch 0] Batch 1271, Loss 0.3295027017593384\n",
      "[Training Epoch 0] Batch 1272, Loss 0.33442002534866333\n",
      "[Training Epoch 0] Batch 1273, Loss 0.3367149233818054\n",
      "[Training Epoch 0] Batch 1274, Loss 0.33832114934921265\n",
      "[Training Epoch 0] Batch 1275, Loss 0.3631223738193512\n",
      "[Training Epoch 0] Batch 1276, Loss 0.31921300292015076\n",
      "[Training Epoch 0] Batch 1277, Loss 0.3241042196750641\n",
      "[Training Epoch 0] Batch 1278, Loss 0.33748841285705566\n",
      "[Training Epoch 0] Batch 1279, Loss 0.32647448778152466\n",
      "[Training Epoch 0] Batch 1280, Loss 0.3065389394760132\n",
      "[Training Epoch 0] Batch 1281, Loss 0.3284623920917511\n",
      "[Training Epoch 0] Batch 1282, Loss 0.2877325415611267\n",
      "[Training Epoch 0] Batch 1283, Loss 0.33074676990509033\n",
      "[Training Epoch 0] Batch 1284, Loss 0.32606232166290283\n",
      "[Training Epoch 0] Batch 1285, Loss 0.3449796140193939\n",
      "[Training Epoch 0] Batch 1286, Loss 0.33759361505508423\n",
      "[Training Epoch 0] Batch 1287, Loss 0.3290340304374695\n",
      "[Training Epoch 0] Batch 1288, Loss 0.3664197325706482\n",
      "[Training Epoch 0] Batch 1289, Loss 0.3537450134754181\n",
      "[Training Epoch 0] Batch 1290, Loss 0.31488221883773804\n",
      "[Training Epoch 0] Batch 1291, Loss 0.3271436393260956\n",
      "[Training Epoch 0] Batch 1292, Loss 0.3443083167076111\n",
      "[Training Epoch 0] Batch 1293, Loss 0.34754329919815063\n",
      "[Training Epoch 0] Batch 1294, Loss 0.32059264183044434\n",
      "[Training Epoch 0] Batch 1295, Loss 0.31773361563682556\n",
      "[Training Epoch 0] Batch 1296, Loss 0.3170119822025299\n",
      "[Training Epoch 0] Batch 1297, Loss 0.31999605894088745\n",
      "[Training Epoch 0] Batch 1298, Loss 0.33140304684638977\n",
      "[Training Epoch 0] Batch 1299, Loss 0.3295558989048004\n",
      "[Training Epoch 0] Batch 1300, Loss 0.3081494867801666\n",
      "[Training Epoch 0] Batch 1301, Loss 0.32535916566848755\n",
      "[Training Epoch 0] Batch 1302, Loss 0.3198603689670563\n",
      "[Training Epoch 0] Batch 1303, Loss 0.33477604389190674\n",
      "[Training Epoch 0] Batch 1304, Loss 0.3195292055606842\n",
      "[Training Epoch 0] Batch 1305, Loss 0.35927847027778625\n",
      "[Training Epoch 0] Batch 1306, Loss 0.32853055000305176\n",
      "[Training Epoch 0] Batch 1307, Loss 0.36537712812423706\n",
      "[Training Epoch 0] Batch 1308, Loss 0.36512815952301025\n",
      "[Training Epoch 0] Batch 1309, Loss 0.3560468256473541\n",
      "[Training Epoch 0] Batch 1310, Loss 0.3121103346347809\n",
      "[Training Epoch 0] Batch 1311, Loss 0.34495678544044495\n",
      "[Training Epoch 0] Batch 1312, Loss 0.32300910353660583\n",
      "[Training Epoch 0] Batch 1313, Loss 0.34253495931625366\n",
      "[Training Epoch 0] Batch 1314, Loss 0.3343513011932373\n",
      "[Training Epoch 0] Batch 1315, Loss 0.33999356627464294\n",
      "[Training Epoch 0] Batch 1316, Loss 0.3292938768863678\n",
      "[Training Epoch 0] Batch 1317, Loss 0.36972135305404663\n",
      "[Training Epoch 0] Batch 1318, Loss 0.36961138248443604\n",
      "[Training Epoch 0] Batch 1319, Loss 0.3520149886608124\n",
      "[Training Epoch 0] Batch 1320, Loss 0.3081227242946625\n",
      "[Training Epoch 0] Batch 1321, Loss 0.28768718242645264\n",
      "[Training Epoch 0] Batch 1322, Loss 0.34945791959762573\n",
      "[Training Epoch 0] Batch 1323, Loss 0.33336296677589417\n",
      "[Training Epoch 0] Batch 1324, Loss 0.3471328616142273\n",
      "[Training Epoch 0] Batch 1325, Loss 0.3247750997543335\n",
      "[Training Epoch 0] Batch 1326, Loss 0.34266865253448486\n",
      "[Training Epoch 0] Batch 1327, Loss 0.32881689071655273\n",
      "[Training Epoch 0] Batch 1328, Loss 0.31623873114585876\n",
      "[Training Epoch 0] Batch 1329, Loss 0.34057408571243286\n",
      "[Training Epoch 0] Batch 1330, Loss 0.34114158153533936\n",
      "[Training Epoch 0] Batch 1331, Loss 0.35606348514556885\n",
      "[Training Epoch 0] Batch 1332, Loss 0.30365076661109924\n",
      "[Training Epoch 0] Batch 1333, Loss 0.3592565655708313\n",
      "[Training Epoch 0] Batch 1334, Loss 0.32073256373405457\n",
      "[Training Epoch 0] Batch 1335, Loss 0.3365817964076996\n",
      "[Training Epoch 0] Batch 1336, Loss 0.34135931730270386\n",
      "[Training Epoch 0] Batch 1337, Loss 0.3341907858848572\n",
      "[Training Epoch 0] Batch 1338, Loss 0.32941800355911255\n",
      "[Training Epoch 0] Batch 1339, Loss 0.35534170269966125\n",
      "[Training Epoch 0] Batch 1340, Loss 0.30787837505340576\n",
      "[Training Epoch 0] Batch 1341, Loss 0.34004178643226624\n",
      "[Training Epoch 0] Batch 1342, Loss 0.3247404992580414\n",
      "[Training Epoch 0] Batch 1343, Loss 0.3185986280441284\n",
      "[Training Epoch 0] Batch 1344, Loss 0.3290826678276062\n",
      "[Training Epoch 0] Batch 1345, Loss 0.3060949444770813\n",
      "[Training Epoch 0] Batch 1346, Loss 0.3272022008895874\n",
      "[Training Epoch 0] Batch 1347, Loss 0.3130892515182495\n",
      "[Training Epoch 0] Batch 1348, Loss 0.3352145254611969\n",
      "[Training Epoch 0] Batch 1349, Loss 0.33549392223358154\n",
      "[Training Epoch 0] Batch 1350, Loss 0.32448258996009827\n",
      "[Training Epoch 0] Batch 1351, Loss 0.34216588735580444\n",
      "[Training Epoch 0] Batch 1352, Loss 0.31959956884384155\n",
      "[Training Epoch 0] Batch 1353, Loss 0.33077284693717957\n",
      "[Training Epoch 0] Batch 1354, Loss 0.29556387662887573\n",
      "[Training Epoch 0] Batch 1355, Loss 0.31904566287994385\n",
      "[Training Epoch 0] Batch 1356, Loss 0.33758074045181274\n",
      "[Training Epoch 0] Batch 1357, Loss 0.35421672463417053\n",
      "[Training Epoch 0] Batch 1358, Loss 0.36176276206970215\n",
      "[Training Epoch 0] Batch 1359, Loss 0.30140742659568787\n",
      "[Training Epoch 0] Batch 1360, Loss 0.3322916626930237\n",
      "[Training Epoch 0] Batch 1361, Loss 0.3273998200893402\n",
      "[Training Epoch 0] Batch 1362, Loss 0.2837165594100952\n",
      "[Training Epoch 0] Batch 1363, Loss 0.31669771671295166\n",
      "[Training Epoch 0] Batch 1364, Loss 0.36569514870643616\n",
      "[Training Epoch 0] Batch 1365, Loss 0.3639254570007324\n",
      "[Training Epoch 0] Batch 1366, Loss 0.3398798406124115\n",
      "[Training Epoch 0] Batch 1367, Loss 0.34223324060440063\n",
      "[Training Epoch 0] Batch 1368, Loss 0.3333243131637573\n",
      "[Training Epoch 0] Batch 1369, Loss 0.32701200246810913\n",
      "[Training Epoch 0] Batch 1370, Loss 0.3392452299594879\n",
      "[Training Epoch 0] Batch 1371, Loss 0.317573219537735\n",
      "[Training Epoch 0] Batch 1372, Loss 0.2987063527107239\n",
      "[Training Epoch 0] Batch 1373, Loss 0.32968565821647644\n",
      "[Training Epoch 0] Batch 1374, Loss 0.33895352482795715\n",
      "[Training Epoch 0] Batch 1375, Loss 0.33464205265045166\n",
      "[Training Epoch 0] Batch 1376, Loss 0.3116576373577118\n",
      "[Training Epoch 0] Batch 1377, Loss 0.34487485885620117\n",
      "[Training Epoch 0] Batch 1378, Loss 0.29911985993385315\n",
      "[Training Epoch 0] Batch 1379, Loss 0.3354571759700775\n",
      "[Training Epoch 0] Batch 1380, Loss 0.3578636646270752\n",
      "[Training Epoch 0] Batch 1381, Loss 0.3432740569114685\n",
      "[Training Epoch 0] Batch 1382, Loss 0.34984707832336426\n",
      "[Training Epoch 0] Batch 1383, Loss 0.3110470175743103\n",
      "[Training Epoch 0] Batch 1384, Loss 0.35857272148132324\n",
      "[Training Epoch 0] Batch 1385, Loss 0.37819263339042664\n",
      "[Training Epoch 0] Batch 1386, Loss 0.3223198354244232\n",
      "[Training Epoch 0] Batch 1387, Loss 0.3363558351993561\n",
      "[Training Epoch 0] Batch 1388, Loss 0.2961147427558899\n",
      "[Training Epoch 0] Batch 1389, Loss 0.3422178328037262\n",
      "[Training Epoch 0] Batch 1390, Loss 0.3174990117549896\n",
      "[Training Epoch 0] Batch 1391, Loss 0.35215067863464355\n",
      "[Training Epoch 0] Batch 1392, Loss 0.3522225618362427\n",
      "[Training Epoch 0] Batch 1393, Loss 0.33144283294677734\n",
      "[Training Epoch 0] Batch 1394, Loss 0.3318687975406647\n",
      "[Training Epoch 0] Batch 1395, Loss 0.3076481819152832\n",
      "[Training Epoch 0] Batch 1396, Loss 0.3500516712665558\n",
      "[Training Epoch 0] Batch 1397, Loss 0.3288189768791199\n",
      "[Training Epoch 0] Batch 1398, Loss 0.31060901284217834\n",
      "[Training Epoch 0] Batch 1399, Loss 0.31811049580574036\n",
      "[Training Epoch 0] Batch 1400, Loss 0.3136129379272461\n",
      "[Training Epoch 0] Batch 1401, Loss 0.3215167820453644\n",
      "[Training Epoch 0] Batch 1402, Loss 0.34683915972709656\n",
      "[Training Epoch 0] Batch 1403, Loss 0.32441702485084534\n",
      "[Training Epoch 0] Batch 1404, Loss 0.3351786732673645\n",
      "[Training Epoch 0] Batch 1405, Loss 0.3394699692726135\n",
      "[Training Epoch 0] Batch 1406, Loss 0.32199427485466003\n",
      "[Training Epoch 0] Batch 1407, Loss 0.3219769299030304\n",
      "[Training Epoch 0] Batch 1408, Loss 0.37178927659988403\n",
      "[Training Epoch 0] Batch 1409, Loss 0.3006976246833801\n",
      "[Training Epoch 0] Batch 1410, Loss 0.3307643532752991\n",
      "[Training Epoch 0] Batch 1411, Loss 0.33819320797920227\n",
      "[Training Epoch 0] Batch 1412, Loss 0.3141120672225952\n",
      "[Training Epoch 0] Batch 1413, Loss 0.3083542287349701\n",
      "[Training Epoch 0] Batch 1414, Loss 0.3161732256412506\n",
      "[Training Epoch 0] Batch 1415, Loss 0.31540393829345703\n",
      "[Training Epoch 0] Batch 1416, Loss 0.30284062027931213\n",
      "[Training Epoch 0] Batch 1417, Loss 0.29833102226257324\n",
      "[Training Epoch 0] Batch 1418, Loss 0.28795748949050903\n",
      "[Training Epoch 0] Batch 1419, Loss 0.32388362288475037\n",
      "[Training Epoch 0] Batch 1420, Loss 0.3394733965396881\n",
      "[Training Epoch 0] Batch 1421, Loss 0.33432722091674805\n",
      "[Training Epoch 0] Batch 1422, Loss 0.3439709544181824\n",
      "[Training Epoch 0] Batch 1423, Loss 0.322971910238266\n",
      "[Training Epoch 0] Batch 1424, Loss 0.34984296560287476\n",
      "[Training Epoch 0] Batch 1425, Loss 0.30403009057044983\n",
      "[Training Epoch 0] Batch 1426, Loss 0.3236929178237915\n",
      "[Training Epoch 0] Batch 1427, Loss 0.3119412362575531\n",
      "[Training Epoch 0] Batch 1428, Loss 0.33340951800346375\n",
      "[Training Epoch 0] Batch 1429, Loss 0.3639267385005951\n",
      "[Training Epoch 0] Batch 1430, Loss 0.33385375142097473\n",
      "[Training Epoch 0] Batch 1431, Loss 0.34259968996047974\n",
      "[Training Epoch 0] Batch 1432, Loss 0.3138163685798645\n",
      "[Training Epoch 0] Batch 1433, Loss 0.35493069887161255\n",
      "[Training Epoch 0] Batch 1434, Loss 0.3520841598510742\n",
      "[Training Epoch 0] Batch 1435, Loss 0.3050854206085205\n",
      "[Training Epoch 0] Batch 1436, Loss 0.3431014120578766\n",
      "[Training Epoch 0] Batch 1437, Loss 0.3540729880332947\n",
      "[Training Epoch 0] Batch 1438, Loss 0.3515792787075043\n",
      "[Training Epoch 0] Batch 1439, Loss 0.3306826949119568\n",
      "[Training Epoch 0] Batch 1440, Loss 0.3137173652648926\n",
      "[Training Epoch 0] Batch 1441, Loss 0.3576667904853821\n",
      "[Training Epoch 0] Batch 1442, Loss 0.33751359581947327\n",
      "[Training Epoch 0] Batch 1443, Loss 0.30307310819625854\n",
      "[Training Epoch 0] Batch 1444, Loss 0.3142685890197754\n",
      "[Training Epoch 0] Batch 1445, Loss 0.3137053847312927\n",
      "[Training Epoch 0] Batch 1446, Loss 0.3367094397544861\n",
      "[Training Epoch 0] Batch 1447, Loss 0.3276691436767578\n",
      "[Training Epoch 0] Batch 1448, Loss 0.3103533089160919\n",
      "[Training Epoch 0] Batch 1449, Loss 0.31864166259765625\n",
      "[Training Epoch 0] Batch 1450, Loss 0.31616029143333435\n",
      "[Training Epoch 0] Batch 1451, Loss 0.28854501247406006\n",
      "[Training Epoch 0] Batch 1452, Loss 0.32214051485061646\n",
      "[Training Epoch 0] Batch 1453, Loss 0.35963037610054016\n",
      "[Training Epoch 0] Batch 1454, Loss 0.3296818435192108\n",
      "[Training Epoch 0] Batch 1455, Loss 0.3200460970401764\n",
      "[Training Epoch 0] Batch 1456, Loss 0.3286411166191101\n",
      "[Training Epoch 0] Batch 1457, Loss 0.36645835638046265\n",
      "[Training Epoch 0] Batch 1458, Loss 0.3376259505748749\n",
      "[Training Epoch 0] Batch 1459, Loss 0.30912041664123535\n",
      "[Training Epoch 0] Batch 1460, Loss 0.32314229011535645\n",
      "[Training Epoch 0] Batch 1461, Loss 0.3549768924713135\n",
      "[Training Epoch 0] Batch 1462, Loss 0.29925331473350525\n",
      "[Training Epoch 0] Batch 1463, Loss 0.3345925211906433\n",
      "[Training Epoch 0] Batch 1464, Loss 0.3219892382621765\n",
      "[Training Epoch 0] Batch 1465, Loss 0.3218807280063629\n",
      "[Training Epoch 0] Batch 1466, Loss 0.33080920577049255\n",
      "[Training Epoch 0] Batch 1467, Loss 0.33474522829055786\n",
      "[Training Epoch 0] Batch 1468, Loss 0.36050015687942505\n",
      "[Training Epoch 0] Batch 1469, Loss 0.31676095724105835\n",
      "[Training Epoch 0] Batch 1470, Loss 0.3357910215854645\n",
      "[Training Epoch 0] Batch 1471, Loss 0.30447742342948914\n",
      "[Training Epoch 0] Batch 1472, Loss 0.341260701417923\n",
      "[Training Epoch 0] Batch 1473, Loss 0.3472793400287628\n",
      "[Training Epoch 0] Batch 1474, Loss 0.31899699568748474\n",
      "[Training Epoch 0] Batch 1475, Loss 0.3165143132209778\n",
      "[Training Epoch 0] Batch 1476, Loss 0.34243375062942505\n",
      "[Training Epoch 0] Batch 1477, Loss 0.3571900725364685\n",
      "[Training Epoch 0] Batch 1478, Loss 0.3375833034515381\n",
      "[Training Epoch 0] Batch 1479, Loss 0.34760481119155884\n",
      "[Training Epoch 0] Batch 1480, Loss 0.33041518926620483\n",
      "[Training Epoch 0] Batch 1481, Loss 0.33207178115844727\n",
      "[Training Epoch 0] Batch 1482, Loss 0.36667123436927795\n",
      "[Training Epoch 0] Batch 1483, Loss 0.34718963503837585\n",
      "[Training Epoch 0] Batch 1484, Loss 0.3080796003341675\n",
      "[Training Epoch 0] Batch 1485, Loss 0.3356945812702179\n",
      "[Training Epoch 0] Batch 1486, Loss 0.32483404874801636\n",
      "[Training Epoch 0] Batch 1487, Loss 0.32529813051223755\n",
      "[Training Epoch 0] Batch 1488, Loss 0.3506726026535034\n",
      "[Training Epoch 0] Batch 1489, Loss 0.3255762457847595\n",
      "[Training Epoch 0] Batch 1490, Loss 0.3208543658256531\n",
      "[Training Epoch 0] Batch 1491, Loss 0.3295620083808899\n",
      "[Training Epoch 0] Batch 1492, Loss 0.3374405801296234\n",
      "[Training Epoch 0] Batch 1493, Loss 0.3389049172401428\n",
      "[Training Epoch 0] Batch 1494, Loss 0.375570684671402\n",
      "[Training Epoch 0] Batch 1495, Loss 0.39393284916877747\n",
      "[Training Epoch 0] Batch 1496, Loss 0.3302101194858551\n",
      "[Training Epoch 0] Batch 1497, Loss 0.33597296476364136\n",
      "[Training Epoch 0] Batch 1498, Loss 0.30775412917137146\n",
      "[Training Epoch 0] Batch 1499, Loss 0.31125181913375854\n",
      "[Training Epoch 0] Batch 1500, Loss 0.34113365411758423\n",
      "[Training Epoch 0] Batch 1501, Loss 0.33404290676116943\n",
      "[Training Epoch 0] Batch 1502, Loss 0.31493255496025085\n",
      "[Training Epoch 0] Batch 1503, Loss 0.3366696536540985\n",
      "[Training Epoch 0] Batch 1504, Loss 0.31116026639938354\n",
      "[Training Epoch 0] Batch 1505, Loss 0.34337949752807617\n",
      "[Training Epoch 0] Batch 1506, Loss 0.3315799832344055\n",
      "[Training Epoch 0] Batch 1507, Loss 0.32543620467185974\n",
      "[Training Epoch 0] Batch 1508, Loss 0.3152642846107483\n",
      "[Training Epoch 0] Batch 1509, Loss 0.3516967296600342\n",
      "[Training Epoch 0] Batch 1510, Loss 0.29625964164733887\n",
      "[Training Epoch 0] Batch 1511, Loss 0.3576503396034241\n",
      "[Training Epoch 0] Batch 1512, Loss 0.33088183403015137\n",
      "[Training Epoch 0] Batch 1513, Loss 0.3238771855831146\n",
      "[Training Epoch 0] Batch 1514, Loss 0.3229042589664459\n",
      "[Training Epoch 0] Batch 1515, Loss 0.2724415361881256\n",
      "[Training Epoch 0] Batch 1516, Loss 0.33735552430152893\n",
      "[Training Epoch 0] Batch 1517, Loss 0.34226250648498535\n",
      "[Training Epoch 0] Batch 1518, Loss 0.34247803688049316\n",
      "[Training Epoch 0] Batch 1519, Loss 0.3117194175720215\n",
      "[Training Epoch 0] Batch 1520, Loss 0.31564298272132874\n",
      "[Training Epoch 0] Batch 1521, Loss 0.33511894941329956\n",
      "[Training Epoch 0] Batch 1522, Loss 0.3385809063911438\n",
      "[Training Epoch 0] Batch 1523, Loss 0.29969656467437744\n",
      "[Training Epoch 0] Batch 1524, Loss 0.29647231101989746\n",
      "[Training Epoch 0] Batch 1525, Loss 0.31029564142227173\n",
      "[Training Epoch 0] Batch 1526, Loss 0.31153470277786255\n",
      "[Training Epoch 0] Batch 1527, Loss 0.3248119354248047\n",
      "[Training Epoch 0] Batch 1528, Loss 0.38946107029914856\n",
      "[Training Epoch 0] Batch 1529, Loss 0.2959144413471222\n",
      "[Training Epoch 0] Batch 1530, Loss 0.34216126799583435\n",
      "[Training Epoch 0] Batch 1531, Loss 0.33419349789619446\n",
      "[Training Epoch 0] Batch 1532, Loss 0.29386773705482483\n",
      "[Training Epoch 0] Batch 1533, Loss 0.30997785925865173\n",
      "[Training Epoch 0] Batch 1534, Loss 0.32869407534599304\n",
      "[Training Epoch 0] Batch 1535, Loss 0.36014822125434875\n",
      "[Training Epoch 0] Batch 1536, Loss 0.33252543210983276\n",
      "[Training Epoch 0] Batch 1537, Loss 0.31169477105140686\n",
      "[Training Epoch 0] Batch 1538, Loss 0.32609131932258606\n",
      "[Training Epoch 0] Batch 1539, Loss 0.3253862261772156\n",
      "[Training Epoch 0] Batch 1540, Loss 0.32462120056152344\n",
      "[Training Epoch 0] Batch 1541, Loss 0.3237290382385254\n",
      "[Training Epoch 0] Batch 1542, Loss 0.2984989285469055\n",
      "[Training Epoch 0] Batch 1543, Loss 0.30479657649993896\n",
      "[Training Epoch 0] Batch 1544, Loss 0.3487280607223511\n",
      "[Training Epoch 0] Batch 1545, Loss 0.3215307593345642\n",
      "[Training Epoch 0] Batch 1546, Loss 0.31001394987106323\n",
      "[Training Epoch 0] Batch 1547, Loss 0.35148802399635315\n",
      "[Training Epoch 0] Batch 1548, Loss 0.32416900992393494\n",
      "[Training Epoch 0] Batch 1549, Loss 0.31044095754623413\n",
      "[Training Epoch 0] Batch 1550, Loss 0.3414514660835266\n",
      "[Training Epoch 0] Batch 1551, Loss 0.3122898042201996\n",
      "[Training Epoch 0] Batch 1552, Loss 0.29502391815185547\n",
      "[Training Epoch 0] Batch 1553, Loss 0.3328700065612793\n",
      "[Training Epoch 0] Batch 1554, Loss 0.2966631352901459\n",
      "[Training Epoch 0] Batch 1555, Loss 0.33011314272880554\n",
      "[Training Epoch 0] Batch 1556, Loss 0.3225820064544678\n",
      "[Training Epoch 0] Batch 1557, Loss 0.33249378204345703\n",
      "[Training Epoch 0] Batch 1558, Loss 0.30886292457580566\n",
      "[Training Epoch 0] Batch 1559, Loss 0.322501003742218\n",
      "[Training Epoch 0] Batch 1560, Loss 0.3252142071723938\n",
      "[Training Epoch 0] Batch 1561, Loss 0.3232397437095642\n",
      "[Training Epoch 0] Batch 1562, Loss 0.29463863372802734\n",
      "[Training Epoch 0] Batch 1563, Loss 0.3188447058200836\n",
      "[Training Epoch 0] Batch 1564, Loss 0.3299955427646637\n",
      "[Training Epoch 0] Batch 1565, Loss 0.3218868672847748\n",
      "[Training Epoch 0] Batch 1566, Loss 0.32080039381980896\n",
      "[Training Epoch 0] Batch 1567, Loss 0.31591013073921204\n",
      "[Training Epoch 0] Batch 1568, Loss 0.3124341070652008\n",
      "[Training Epoch 0] Batch 1569, Loss 0.32182878255844116\n",
      "[Training Epoch 0] Batch 1570, Loss 0.31809771060943604\n",
      "[Training Epoch 0] Batch 1571, Loss 0.35072654485702515\n",
      "[Training Epoch 0] Batch 1572, Loss 0.30604034662246704\n",
      "[Training Epoch 0] Batch 1573, Loss 0.3570396304130554\n",
      "[Training Epoch 0] Batch 1574, Loss 0.33078593015670776\n",
      "[Training Epoch 0] Batch 1575, Loss 0.3261384069919586\n",
      "[Training Epoch 0] Batch 1576, Loss 0.30489158630371094\n",
      "[Training Epoch 0] Batch 1577, Loss 0.34135735034942627\n",
      "[Training Epoch 0] Batch 1578, Loss 0.27928319573402405\n",
      "[Training Epoch 0] Batch 1579, Loss 0.33388060331344604\n",
      "[Training Epoch 0] Batch 1580, Loss 0.3187708556652069\n",
      "[Training Epoch 0] Batch 1581, Loss 0.31921282410621643\n",
      "[Training Epoch 0] Batch 1582, Loss 0.3314878046512604\n",
      "[Training Epoch 0] Batch 1583, Loss 0.3219107389450073\n",
      "[Training Epoch 0] Batch 1584, Loss 0.30360284447669983\n",
      "[Training Epoch 0] Batch 1585, Loss 0.28197941184043884\n",
      "[Training Epoch 0] Batch 1586, Loss 0.3051747977733612\n",
      "[Training Epoch 0] Batch 1587, Loss 0.3182608485221863\n",
      "[Training Epoch 0] Batch 1588, Loss 0.32181742787361145\n",
      "[Training Epoch 0] Batch 1589, Loss 0.3146769404411316\n",
      "[Training Epoch 0] Batch 1590, Loss 0.3382600247859955\n",
      "[Training Epoch 0] Batch 1591, Loss 0.3140624761581421\n",
      "[Training Epoch 0] Batch 1592, Loss 0.3104945719242096\n",
      "[Training Epoch 0] Batch 1593, Loss 0.314911425113678\n",
      "[Training Epoch 0] Batch 1594, Loss 0.3502914607524872\n",
      "[Training Epoch 0] Batch 1595, Loss 0.30039259791374207\n",
      "[Training Epoch 0] Batch 1596, Loss 0.31911569833755493\n",
      "[Training Epoch 0] Batch 1597, Loss 0.3279123604297638\n",
      "[Training Epoch 0] Batch 1598, Loss 0.3302430510520935\n",
      "[Training Epoch 0] Batch 1599, Loss 0.32078421115875244\n",
      "[Training Epoch 0] Batch 1600, Loss 0.31084853410720825\n",
      "[Training Epoch 0] Batch 1601, Loss 0.3072964549064636\n",
      "[Training Epoch 0] Batch 1602, Loss 0.31925544142723083\n",
      "[Training Epoch 0] Batch 1603, Loss 0.3129982352256775\n",
      "[Training Epoch 0] Batch 1604, Loss 0.327473908662796\n",
      "[Training Epoch 0] Batch 1605, Loss 0.35216066241264343\n",
      "[Training Epoch 0] Batch 1606, Loss 0.3214722275733948\n",
      "[Training Epoch 0] Batch 1607, Loss 0.3167494833469391\n",
      "[Training Epoch 0] Batch 1608, Loss 0.34474533796310425\n",
      "[Training Epoch 0] Batch 1609, Loss 0.30995213985443115\n",
      "[Training Epoch 0] Batch 1610, Loss 0.2911495864391327\n",
      "[Training Epoch 0] Batch 1611, Loss 0.33988866209983826\n",
      "[Training Epoch 0] Batch 1612, Loss 0.334987998008728\n",
      "[Training Epoch 0] Batch 1613, Loss 0.30890151858329773\n",
      "[Training Epoch 0] Batch 1614, Loss 0.34122174978256226\n",
      "[Training Epoch 0] Batch 1615, Loss 0.33631715178489685\n",
      "[Training Epoch 0] Batch 1616, Loss 0.3302212953567505\n",
      "[Training Epoch 0] Batch 1617, Loss 0.3583946228027344\n",
      "[Training Epoch 0] Batch 1618, Loss 0.31276383996009827\n",
      "[Training Epoch 0] Batch 1619, Loss 0.3380148708820343\n",
      "[Training Epoch 0] Batch 1620, Loss 0.28553032875061035\n",
      "[Training Epoch 0] Batch 1621, Loss 0.33857014775276184\n",
      "[Training Epoch 0] Batch 1622, Loss 0.27117863297462463\n",
      "[Training Epoch 0] Batch 1623, Loss 0.36440789699554443\n",
      "[Training Epoch 0] Batch 1624, Loss 0.29610681533813477\n",
      "[Training Epoch 0] Batch 1625, Loss 0.29356980323791504\n",
      "[Training Epoch 0] Batch 1626, Loss 0.3068159520626068\n",
      "[Training Epoch 0] Batch 1627, Loss 0.366760790348053\n",
      "[Training Epoch 0] Batch 1628, Loss 0.3386423587799072\n",
      "[Training Epoch 0] Batch 1629, Loss 0.33270642161369324\n",
      "[Training Epoch 0] Batch 1630, Loss 0.3169251084327698\n",
      "[Training Epoch 0] Batch 1631, Loss 0.34252455830574036\n",
      "[Training Epoch 0] Batch 1632, Loss 0.29593154788017273\n",
      "[Training Epoch 0] Batch 1633, Loss 0.3318006694316864\n",
      "[Training Epoch 0] Batch 1634, Loss 0.3485002815723419\n",
      "[Training Epoch 0] Batch 1635, Loss 0.3530470132827759\n",
      "[Training Epoch 0] Batch 1636, Loss 0.3336062431335449\n",
      "[Training Epoch 0] Batch 1637, Loss 0.29377979040145874\n",
      "[Training Epoch 0] Batch 1638, Loss 0.30516088008880615\n",
      "[Training Epoch 0] Batch 1639, Loss 0.30005666613578796\n",
      "[Training Epoch 0] Batch 1640, Loss 0.3252193331718445\n",
      "[Training Epoch 0] Batch 1641, Loss 0.3258607089519501\n",
      "[Training Epoch 0] Batch 1642, Loss 0.3371894657611847\n",
      "[Training Epoch 0] Batch 1643, Loss 0.32019707560539246\n",
      "[Training Epoch 0] Batch 1644, Loss 0.34697532653808594\n",
      "[Training Epoch 0] Batch 1645, Loss 0.32789352536201477\n",
      "[Training Epoch 0] Batch 1646, Loss 0.29649460315704346\n",
      "[Training Epoch 0] Batch 1647, Loss 0.3592616319656372\n",
      "[Training Epoch 0] Batch 1648, Loss 0.3331560790538788\n",
      "[Training Epoch 0] Batch 1649, Loss 0.3204810321331024\n",
      "[Training Epoch 0] Batch 1650, Loss 0.33001476526260376\n",
      "[Training Epoch 0] Batch 1651, Loss 0.3064405918121338\n",
      "[Training Epoch 0] Batch 1652, Loss 0.2919626832008362\n",
      "[Training Epoch 0] Batch 1653, Loss 0.3734753131866455\n",
      "[Training Epoch 0] Batch 1654, Loss 0.31873202323913574\n",
      "[Training Epoch 0] Batch 1655, Loss 0.31580835580825806\n",
      "[Training Epoch 0] Batch 1656, Loss 0.3348582088947296\n",
      "[Training Epoch 0] Batch 1657, Loss 0.3426530361175537\n",
      "[Training Epoch 0] Batch 1658, Loss 0.33617496490478516\n",
      "[Training Epoch 0] Batch 1659, Loss 0.325203537940979\n",
      "[Training Epoch 0] Batch 1660, Loss 0.305245965719223\n",
      "[Training Epoch 0] Batch 1661, Loss 0.308165967464447\n",
      "[Training Epoch 0] Batch 1662, Loss 0.32543811202049255\n",
      "[Training Epoch 0] Batch 1663, Loss 0.3143094480037689\n",
      "[Training Epoch 0] Batch 1664, Loss 0.3093322515487671\n",
      "[Training Epoch 0] Batch 1665, Loss 0.3097067177295685\n",
      "[Training Epoch 0] Batch 1666, Loss 0.3500146269798279\n",
      "[Training Epoch 0] Batch 1667, Loss 0.33010348677635193\n",
      "[Training Epoch 0] Batch 1668, Loss 0.3379514813423157\n",
      "[Training Epoch 0] Batch 1669, Loss 0.3173925578594208\n",
      "[Training Epoch 0] Batch 1670, Loss 0.306600958108902\n",
      "[Training Epoch 0] Batch 1671, Loss 0.3154529631137848\n",
      "[Training Epoch 0] Batch 1672, Loss 0.33786988258361816\n",
      "[Training Epoch 0] Batch 1673, Loss 0.3323669135570526\n",
      "[Training Epoch 0] Batch 1674, Loss 0.30147579312324524\n",
      "[Training Epoch 0] Batch 1675, Loss 0.308107852935791\n",
      "[Training Epoch 0] Batch 1676, Loss 0.31773489713668823\n",
      "[Training Epoch 0] Batch 1677, Loss 0.3413757085800171\n",
      "[Training Epoch 0] Batch 1678, Loss 0.33586424589157104\n",
      "[Training Epoch 0] Batch 1679, Loss 0.31372004747390747\n",
      "[Training Epoch 0] Batch 1680, Loss 0.2954126000404358\n",
      "[Training Epoch 0] Batch 1681, Loss 0.34477877616882324\n",
      "[Training Epoch 0] Batch 1682, Loss 0.3825552761554718\n",
      "[Training Epoch 0] Batch 1683, Loss 0.3302026391029358\n",
      "[Training Epoch 0] Batch 1684, Loss 0.3441131114959717\n",
      "[Training Epoch 0] Batch 1685, Loss 0.3041399419307709\n",
      "[Training Epoch 0] Batch 1686, Loss 0.3214627802371979\n",
      "[Training Epoch 0] Batch 1687, Loss 0.3351036012172699\n",
      "[Training Epoch 0] Batch 1688, Loss 0.32079458236694336\n",
      "[Training Epoch 0] Batch 1689, Loss 0.3179032802581787\n",
      "[Training Epoch 0] Batch 1690, Loss 0.30787235498428345\n",
      "[Training Epoch 0] Batch 1691, Loss 0.3304734230041504\n",
      "[Training Epoch 0] Batch 1692, Loss 0.3335795998573303\n",
      "[Training Epoch 0] Batch 1693, Loss 0.304047554731369\n",
      "[Training Epoch 0] Batch 1694, Loss 0.33373236656188965\n",
      "[Training Epoch 0] Batch 1695, Loss 0.34083572030067444\n",
      "[Training Epoch 0] Batch 1696, Loss 0.3144162893295288\n",
      "[Training Epoch 0] Batch 1697, Loss 0.31414419412612915\n",
      "[Training Epoch 0] Batch 1698, Loss 0.32600918412208557\n",
      "[Training Epoch 0] Batch 1699, Loss 0.2946842908859253\n",
      "[Training Epoch 0] Batch 1700, Loss 0.3330049514770508\n",
      "[Training Epoch 0] Batch 1701, Loss 0.33262112736701965\n",
      "[Training Epoch 0] Batch 1702, Loss 0.2958706021308899\n",
      "[Training Epoch 0] Batch 1703, Loss 0.329623818397522\n",
      "[Training Epoch 0] Batch 1704, Loss 0.35001233220100403\n",
      "[Training Epoch 0] Batch 1705, Loss 0.3315013349056244\n",
      "[Training Epoch 0] Batch 1706, Loss 0.3130400776863098\n",
      "[Training Epoch 0] Batch 1707, Loss 0.3072783648967743\n",
      "[Training Epoch 0] Batch 1708, Loss 0.3082205355167389\n",
      "[Training Epoch 0] Batch 1709, Loss 0.3131166100502014\n",
      "[Training Epoch 0] Batch 1710, Loss 0.32447803020477295\n",
      "[Training Epoch 0] Batch 1711, Loss 0.3591950535774231\n",
      "[Training Epoch 0] Batch 1712, Loss 0.3466064929962158\n",
      "[Training Epoch 0] Batch 1713, Loss 0.3094930350780487\n",
      "[Training Epoch 0] Batch 1714, Loss 0.31756293773651123\n",
      "[Training Epoch 0] Batch 1715, Loss 0.3398841321468353\n",
      "[Training Epoch 0] Batch 1716, Loss 0.30545157194137573\n",
      "[Training Epoch 0] Batch 1717, Loss 0.3316109776496887\n",
      "[Training Epoch 0] Batch 1718, Loss 0.3238467276096344\n",
      "[Training Epoch 0] Batch 1719, Loss 0.34651505947113037\n",
      "[Training Epoch 0] Batch 1720, Loss 0.34982454776763916\n",
      "[Training Epoch 0] Batch 1721, Loss 0.33168163895606995\n",
      "[Training Epoch 0] Batch 1722, Loss 0.29301199316978455\n",
      "[Training Epoch 0] Batch 1723, Loss 0.331631064414978\n",
      "[Training Epoch 0] Batch 1724, Loss 0.3514890968799591\n",
      "[Training Epoch 0] Batch 1725, Loss 0.3308582901954651\n",
      "[Training Epoch 0] Batch 1726, Loss 0.3046518862247467\n",
      "[Training Epoch 0] Batch 1727, Loss 0.3074952960014343\n",
      "[Training Epoch 0] Batch 1728, Loss 0.3427506685256958\n",
      "[Training Epoch 0] Batch 1729, Loss 0.29755809903144836\n",
      "[Training Epoch 0] Batch 1730, Loss 0.32965099811553955\n",
      "[Training Epoch 0] Batch 1731, Loss 0.3132442235946655\n",
      "[Training Epoch 0] Batch 1732, Loss 0.3332345187664032\n",
      "[Training Epoch 0] Batch 1733, Loss 0.33267131447792053\n",
      "[Training Epoch 0] Batch 1734, Loss 0.34106147289276123\n",
      "[Training Epoch 0] Batch 1735, Loss 0.32526177167892456\n",
      "[Training Epoch 0] Batch 1736, Loss 0.29412201046943665\n",
      "[Training Epoch 0] Batch 1737, Loss 0.33224502205848694\n",
      "[Training Epoch 0] Batch 1738, Loss 0.34460434317588806\n",
      "[Training Epoch 0] Batch 1739, Loss 0.3184252381324768\n",
      "[Training Epoch 0] Batch 1740, Loss 0.3632930517196655\n",
      "[Training Epoch 0] Batch 1741, Loss 0.31054553389549255\n",
      "[Training Epoch 0] Batch 1742, Loss 0.3139439523220062\n",
      "[Training Epoch 0] Batch 1743, Loss 0.31362205743789673\n",
      "[Training Epoch 0] Batch 1744, Loss 0.3191317021846771\n",
      "[Training Epoch 0] Batch 1745, Loss 0.33055320382118225\n",
      "[Training Epoch 0] Batch 1746, Loss 0.3409085273742676\n",
      "[Training Epoch 0] Batch 1747, Loss 0.3252735435962677\n",
      "[Training Epoch 0] Batch 1748, Loss 0.3090915381908417\n",
      "[Training Epoch 0] Batch 1749, Loss 0.3353789746761322\n",
      "[Training Epoch 0] Batch 1750, Loss 0.3244776725769043\n",
      "[Training Epoch 0] Batch 1751, Loss 0.31426143646240234\n",
      "[Training Epoch 0] Batch 1752, Loss 0.31260946393013\n",
      "[Training Epoch 0] Batch 1753, Loss 0.32802435755729675\n",
      "[Training Epoch 0] Batch 1754, Loss 0.32788318395614624\n",
      "[Training Epoch 0] Batch 1755, Loss 0.3185054659843445\n",
      "[Training Epoch 0] Batch 1756, Loss 0.3473978340625763\n",
      "[Training Epoch 0] Batch 1757, Loss 0.305736780166626\n",
      "[Training Epoch 0] Batch 1758, Loss 0.33767470717430115\n",
      "[Training Epoch 0] Batch 1759, Loss 0.3412182033061981\n",
      "[Training Epoch 0] Batch 1760, Loss 0.3223653733730316\n",
      "[Training Epoch 0] Batch 1761, Loss 0.3459887206554413\n",
      "[Training Epoch 0] Batch 1762, Loss 0.3583310842514038\n",
      "[Training Epoch 0] Batch 1763, Loss 0.3069348633289337\n",
      "[Training Epoch 0] Batch 1764, Loss 0.31556814908981323\n",
      "[Training Epoch 0] Batch 1765, Loss 0.345942884683609\n",
      "[Training Epoch 0] Batch 1766, Loss 0.3433953821659088\n",
      "[Training Epoch 0] Batch 1767, Loss 0.3208547830581665\n",
      "[Training Epoch 0] Batch 1768, Loss 0.3257158398628235\n",
      "[Training Epoch 0] Batch 1769, Loss 0.30091869831085205\n",
      "[Training Epoch 0] Batch 1770, Loss 0.31453824043273926\n",
      "[Training Epoch 0] Batch 1771, Loss 0.32504138350486755\n",
      "[Training Epoch 0] Batch 1772, Loss 0.3115135729312897\n",
      "[Training Epoch 0] Batch 1773, Loss 0.3095039427280426\n",
      "[Training Epoch 0] Batch 1774, Loss 0.29983341693878174\n",
      "[Training Epoch 0] Batch 1775, Loss 0.36659061908721924\n",
      "[Training Epoch 0] Batch 1776, Loss 0.3441668748855591\n",
      "[Training Epoch 0] Batch 1777, Loss 0.33885908126831055\n",
      "[Training Epoch 0] Batch 1778, Loss 0.33819323778152466\n",
      "[Training Epoch 0] Batch 1779, Loss 0.3508703112602234\n",
      "[Training Epoch 0] Batch 1780, Loss 0.34020936489105225\n",
      "[Training Epoch 0] Batch 1781, Loss 0.3174336552619934\n",
      "[Training Epoch 0] Batch 1782, Loss 0.29414814710617065\n",
      "[Training Epoch 0] Batch 1783, Loss 0.3047243058681488\n",
      "[Training Epoch 0] Batch 1784, Loss 0.3019828796386719\n",
      "[Training Epoch 0] Batch 1785, Loss 0.3186807930469513\n",
      "[Training Epoch 0] Batch 1786, Loss 0.3268424868583679\n",
      "[Training Epoch 0] Batch 1787, Loss 0.3471898138523102\n",
      "[Training Epoch 0] Batch 1788, Loss 0.33376798033714294\n",
      "[Training Epoch 0] Batch 1789, Loss 0.30975061655044556\n",
      "[Training Epoch 0] Batch 1790, Loss 0.3339880108833313\n",
      "[Training Epoch 0] Batch 1791, Loss 0.35248538851737976\n",
      "[Training Epoch 0] Batch 1792, Loss 0.31788739562034607\n",
      "[Training Epoch 0] Batch 1793, Loss 0.33635571599006653\n",
      "[Training Epoch 0] Batch 1794, Loss 0.30987748503685\n",
      "[Training Epoch 0] Batch 1795, Loss 0.30705249309539795\n",
      "[Training Epoch 0] Batch 1796, Loss 0.331098735332489\n",
      "[Training Epoch 0] Batch 1797, Loss 0.33188027143478394\n",
      "[Training Epoch 0] Batch 1798, Loss 0.315516859292984\n",
      "[Training Epoch 0] Batch 1799, Loss 0.3221275806427002\n",
      "[Training Epoch 0] Batch 1800, Loss 0.3417919874191284\n",
      "[Training Epoch 0] Batch 1801, Loss 0.31388038396835327\n",
      "[Training Epoch 0] Batch 1802, Loss 0.3295755684375763\n",
      "[Training Epoch 0] Batch 1803, Loss 0.30376410484313965\n",
      "[Training Epoch 0] Batch 1804, Loss 0.3155573606491089\n",
      "[Training Epoch 0] Batch 1805, Loss 0.33191582560539246\n",
      "[Training Epoch 0] Batch 1806, Loss 0.32038745284080505\n",
      "[Training Epoch 0] Batch 1807, Loss 0.33797597885131836\n",
      "[Training Epoch 0] Batch 1808, Loss 0.3279573321342468\n",
      "[Training Epoch 0] Batch 1809, Loss 0.3283002972602844\n",
      "[Training Epoch 0] Batch 1810, Loss 0.3134463131427765\n",
      "[Training Epoch 0] Batch 1811, Loss 0.3165813982486725\n",
      "[Training Epoch 0] Batch 1812, Loss 0.3430177867412567\n",
      "[Training Epoch 0] Batch 1813, Loss 0.3293367028236389\n",
      "[Training Epoch 0] Batch 1814, Loss 0.307424932718277\n",
      "[Training Epoch 0] Batch 1815, Loss 0.32212966680526733\n",
      "[Training Epoch 0] Batch 1816, Loss 0.35076507925987244\n",
      "[Training Epoch 0] Batch 1817, Loss 0.3127245306968689\n",
      "[Training Epoch 0] Batch 1818, Loss 0.35749030113220215\n",
      "[Training Epoch 0] Batch 1819, Loss 0.30303075909614563\n",
      "[Training Epoch 0] Batch 1820, Loss 0.3091796040534973\n",
      "[Training Epoch 0] Batch 1821, Loss 0.3291921019554138\n",
      "[Training Epoch 0] Batch 1822, Loss 0.3111758530139923\n",
      "[Training Epoch 0] Batch 1823, Loss 0.3048097789287567\n",
      "[Training Epoch 0] Batch 1824, Loss 0.32928749918937683\n",
      "[Training Epoch 0] Batch 1825, Loss 0.344139963388443\n",
      "[Training Epoch 0] Batch 1826, Loss 0.28317975997924805\n",
      "[Training Epoch 0] Batch 1827, Loss 0.32211050391197205\n",
      "[Training Epoch 0] Batch 1828, Loss 0.32185664772987366\n",
      "[Training Epoch 0] Batch 1829, Loss 0.3081532418727875\n",
      "[Training Epoch 0] Batch 1830, Loss 0.3320063352584839\n",
      "[Training Epoch 0] Batch 1831, Loss 0.32914999127388\n",
      "[Training Epoch 0] Batch 1832, Loss 0.3257085680961609\n",
      "[Training Epoch 0] Batch 1833, Loss 0.33123087882995605\n",
      "[Training Epoch 0] Batch 1834, Loss 0.35173529386520386\n",
      "[Training Epoch 0] Batch 1835, Loss 0.3108290433883667\n",
      "[Training Epoch 0] Batch 1836, Loss 0.35969844460487366\n",
      "[Training Epoch 0] Batch 1837, Loss 0.3195076286792755\n",
      "[Training Epoch 0] Batch 1838, Loss 0.33692389726638794\n",
      "[Training Epoch 0] Batch 1839, Loss 0.3093973994255066\n",
      "[Training Epoch 0] Batch 1840, Loss 0.3576171398162842\n",
      "[Training Epoch 0] Batch 1841, Loss 0.31977713108062744\n",
      "[Training Epoch 0] Batch 1842, Loss 0.3072243928909302\n",
      "[Training Epoch 0] Batch 1843, Loss 0.32132235169410706\n",
      "[Training Epoch 0] Batch 1844, Loss 0.3207343816757202\n",
      "[Training Epoch 0] Batch 1845, Loss 0.3060677945613861\n",
      "[Training Epoch 0] Batch 1846, Loss 0.3365831971168518\n",
      "[Training Epoch 0] Batch 1847, Loss 0.3388543725013733\n",
      "[Training Epoch 0] Batch 1848, Loss 0.3082248866558075\n",
      "[Training Epoch 0] Batch 1849, Loss 0.29733574390411377\n",
      "[Training Epoch 0] Batch 1850, Loss 0.32853615283966064\n",
      "[Training Epoch 0] Batch 1851, Loss 0.31708061695098877\n",
      "[Training Epoch 0] Batch 1852, Loss 0.3152521848678589\n",
      "[Training Epoch 0] Batch 1853, Loss 0.3105320930480957\n",
      "[Training Epoch 0] Batch 1854, Loss 0.3213617205619812\n",
      "[Training Epoch 0] Batch 1855, Loss 0.3016021251678467\n",
      "[Training Epoch 0] Batch 1856, Loss 0.32496926188468933\n",
      "[Training Epoch 0] Batch 1857, Loss 0.31272804737091064\n",
      "[Training Epoch 0] Batch 1858, Loss 0.2839083969593048\n",
      "[Training Epoch 0] Batch 1859, Loss 0.3358837366104126\n",
      "[Training Epoch 0] Batch 1860, Loss 0.32166725397109985\n",
      "[Training Epoch 0] Batch 1861, Loss 0.3384615182876587\n",
      "[Training Epoch 0] Batch 1862, Loss 0.31382060050964355\n",
      "[Training Epoch 0] Batch 1863, Loss 0.3554864525794983\n",
      "[Training Epoch 0] Batch 1864, Loss 0.3368067741394043\n",
      "[Training Epoch 0] Batch 1865, Loss 0.3163657486438751\n",
      "[Training Epoch 0] Batch 1866, Loss 0.35084933042526245\n",
      "[Training Epoch 0] Batch 1867, Loss 0.29723235964775085\n",
      "[Training Epoch 0] Batch 1868, Loss 0.325760155916214\n",
      "[Training Epoch 0] Batch 1869, Loss 0.35499051213264465\n",
      "[Training Epoch 0] Batch 1870, Loss 0.32264453172683716\n",
      "[Training Epoch 0] Batch 1871, Loss 0.3122333586215973\n",
      "[Training Epoch 0] Batch 1872, Loss 0.3252585530281067\n",
      "[Training Epoch 0] Batch 1873, Loss 0.3213997483253479\n",
      "[Training Epoch 0] Batch 1874, Loss 0.3485354781150818\n",
      "[Training Epoch 0] Batch 1875, Loss 0.3396573066711426\n",
      "[Training Epoch 0] Batch 1876, Loss 0.31772223114967346\n",
      "[Training Epoch 0] Batch 1877, Loss 0.32163453102111816\n",
      "[Training Epoch 0] Batch 1878, Loss 0.3157319128513336\n",
      "[Training Epoch 0] Batch 1879, Loss 0.3398023843765259\n",
      "[Training Epoch 0] Batch 1880, Loss 0.3422907888889313\n",
      "[Training Epoch 0] Batch 1881, Loss 0.3325687050819397\n",
      "[Training Epoch 0] Batch 1882, Loss 0.2991720736026764\n",
      "[Training Epoch 0] Batch 1883, Loss 0.3322465121746063\n",
      "[Training Epoch 0] Batch 1884, Loss 0.29932230710983276\n",
      "[Training Epoch 0] Batch 1885, Loss 0.30840760469436646\n",
      "[Training Epoch 0] Batch 1886, Loss 0.30956923961639404\n",
      "[Training Epoch 0] Batch 1887, Loss 0.32426393032073975\n",
      "[Training Epoch 0] Batch 1888, Loss 0.29865777492523193\n",
      "[Training Epoch 0] Batch 1889, Loss 0.3369196057319641\n",
      "[Training Epoch 0] Batch 1890, Loss 0.33457180857658386\n",
      "[Training Epoch 0] Batch 1891, Loss 0.2979951798915863\n",
      "[Training Epoch 0] Batch 1892, Loss 0.33933353424072266\n",
      "[Training Epoch 0] Batch 1893, Loss 0.35379940271377563\n",
      "[Training Epoch 0] Batch 1894, Loss 0.32727938890457153\n",
      "[Training Epoch 0] Batch 1895, Loss 0.31510674953460693\n",
      "[Training Epoch 0] Batch 1896, Loss 0.3450111746788025\n",
      "[Training Epoch 0] Batch 1897, Loss 0.31921082735061646\n",
      "[Training Epoch 0] Batch 1898, Loss 0.3527725040912628\n",
      "[Training Epoch 0] Batch 1899, Loss 0.31013256311416626\n",
      "[Training Epoch 0] Batch 1900, Loss 0.3322643041610718\n",
      "[Training Epoch 0] Batch 1901, Loss 0.32375332713127136\n",
      "[Training Epoch 0] Batch 1902, Loss 0.30328959226608276\n",
      "[Training Epoch 0] Batch 1903, Loss 0.3060189485549927\n",
      "[Training Epoch 0] Batch 1904, Loss 0.2941228151321411\n",
      "[Training Epoch 0] Batch 1905, Loss 0.3404935300350189\n",
      "[Training Epoch 0] Batch 1906, Loss 0.3071630001068115\n",
      "[Training Epoch 0] Batch 1907, Loss 0.2993205189704895\n",
      "[Training Epoch 0] Batch 1908, Loss 0.30238232016563416\n",
      "[Training Epoch 0] Batch 1909, Loss 0.31861868500709534\n",
      "[Training Epoch 0] Batch 1910, Loss 0.3395769000053406\n",
      "[Training Epoch 0] Batch 1911, Loss 0.32562774419784546\n",
      "[Training Epoch 0] Batch 1912, Loss 0.32750409841537476\n",
      "[Training Epoch 0] Batch 1913, Loss 0.3282066285610199\n",
      "[Training Epoch 0] Batch 1914, Loss 0.322022944688797\n",
      "[Training Epoch 0] Batch 1915, Loss 0.3144633173942566\n",
      "[Training Epoch 0] Batch 1916, Loss 0.3309304118156433\n",
      "[Training Epoch 0] Batch 1917, Loss 0.33117571473121643\n",
      "[Training Epoch 0] Batch 1918, Loss 0.35123395919799805\n",
      "[Training Epoch 0] Batch 1919, Loss 0.3113318383693695\n",
      "[Training Epoch 0] Batch 1920, Loss 0.3048977255821228\n",
      "[Training Epoch 0] Batch 1921, Loss 0.3315828740596771\n",
      "[Training Epoch 0] Batch 1922, Loss 0.3395099937915802\n",
      "[Training Epoch 0] Batch 1923, Loss 0.3227298855781555\n",
      "[Training Epoch 0] Batch 1924, Loss 0.2929840087890625\n",
      "[Training Epoch 0] Batch 1925, Loss 0.32154756784439087\n",
      "[Training Epoch 0] Batch 1926, Loss 0.3047889471054077\n",
      "[Training Epoch 0] Batch 1927, Loss 0.3221512734889984\n",
      "[Training Epoch 0] Batch 1928, Loss 0.2885211706161499\n",
      "[Training Epoch 0] Batch 1929, Loss 0.3249780833721161\n",
      "[Training Epoch 0] Batch 1930, Loss 0.30142688751220703\n",
      "[Training Epoch 0] Batch 1931, Loss 0.3193713128566742\n",
      "[Training Epoch 0] Batch 1932, Loss 0.298864483833313\n",
      "[Training Epoch 0] Batch 1933, Loss 0.34150806069374084\n",
      "[Training Epoch 0] Batch 1934, Loss 0.28123071789741516\n",
      "[Training Epoch 0] Batch 1935, Loss 0.3183307349681854\n",
      "[Training Epoch 0] Batch 1936, Loss 0.3384696841239929\n",
      "[Training Epoch 0] Batch 1937, Loss 0.355857253074646\n",
      "[Training Epoch 0] Batch 1938, Loss 0.31664204597473145\n",
      "[Training Epoch 0] Batch 1939, Loss 0.30000320076942444\n",
      "[Training Epoch 0] Batch 1940, Loss 0.32499057054519653\n",
      "[Training Epoch 0] Batch 1941, Loss 0.35141435265541077\n",
      "[Training Epoch 0] Batch 1942, Loss 0.3253387212753296\n",
      "[Training Epoch 0] Batch 1943, Loss 0.3402552008628845\n",
      "[Training Epoch 0] Batch 1944, Loss 0.3212704658508301\n",
      "[Training Epoch 0] Batch 1945, Loss 0.3520360589027405\n",
      "[Training Epoch 0] Batch 1946, Loss 0.3367443084716797\n",
      "[Training Epoch 0] Batch 1947, Loss 0.3331226110458374\n",
      "[Training Epoch 0] Batch 1948, Loss 0.3646008372306824\n",
      "[Training Epoch 0] Batch 1949, Loss 0.3345334231853485\n",
      "[Training Epoch 0] Batch 1950, Loss 0.3241666853427887\n",
      "[Training Epoch 0] Batch 1951, Loss 0.35939526557922363\n",
      "[Training Epoch 0] Batch 1952, Loss 0.3239227831363678\n",
      "[Training Epoch 0] Batch 1953, Loss 0.30431312322616577\n",
      "[Training Epoch 0] Batch 1954, Loss 0.3202785551548004\n",
      "[Training Epoch 0] Batch 1955, Loss 0.3134240508079529\n",
      "[Training Epoch 0] Batch 1956, Loss 0.30479100346565247\n",
      "[Training Epoch 0] Batch 1957, Loss 0.3131392300128937\n",
      "[Training Epoch 0] Batch 1958, Loss 0.3266643285751343\n",
      "[Training Epoch 0] Batch 1959, Loss 0.3088800013065338\n",
      "[Training Epoch 0] Batch 1960, Loss 0.33932608366012573\n",
      "[Training Epoch 0] Batch 1961, Loss 0.33038997650146484\n",
      "[Training Epoch 0] Batch 1962, Loss 0.3092884421348572\n",
      "[Training Epoch 0] Batch 1963, Loss 0.3022013008594513\n",
      "[Training Epoch 0] Batch 1964, Loss 0.3032527267932892\n",
      "[Training Epoch 0] Batch 1965, Loss 0.32475680112838745\n",
      "[Training Epoch 0] Batch 1966, Loss 0.3436809182167053\n",
      "[Training Epoch 0] Batch 1967, Loss 0.3160044252872467\n",
      "[Training Epoch 0] Batch 1968, Loss 0.3025689125061035\n",
      "[Training Epoch 0] Batch 1969, Loss 0.3299473226070404\n",
      "[Training Epoch 0] Batch 1970, Loss 0.31186148524284363\n",
      "[Training Epoch 0] Batch 1971, Loss 0.3304674029350281\n",
      "[Training Epoch 0] Batch 1972, Loss 0.3182751536369324\n",
      "[Training Epoch 0] Batch 1973, Loss 0.348190575838089\n",
      "[Training Epoch 0] Batch 1974, Loss 0.32726505398750305\n",
      "[Training Epoch 0] Batch 1975, Loss 0.3217798173427582\n",
      "[Training Epoch 0] Batch 1976, Loss 0.3123907148838043\n",
      "[Training Epoch 0] Batch 1977, Loss 0.34747615456581116\n",
      "[Training Epoch 0] Batch 1978, Loss 0.30847713351249695\n",
      "[Training Epoch 0] Batch 1979, Loss 0.3082885146141052\n",
      "[Training Epoch 0] Batch 1980, Loss 0.31038904190063477\n",
      "[Training Epoch 0] Batch 1981, Loss 0.3315892219543457\n",
      "[Training Epoch 0] Batch 1982, Loss 0.3079046905040741\n",
      "[Training Epoch 0] Batch 1983, Loss 0.3343521058559418\n",
      "[Training Epoch 0] Batch 1984, Loss 0.3728848695755005\n",
      "[Training Epoch 0] Batch 1985, Loss 0.31468236446380615\n",
      "[Training Epoch 0] Batch 1986, Loss 0.30326899886131287\n",
      "[Training Epoch 0] Batch 1987, Loss 0.32297638058662415\n",
      "[Training Epoch 0] Batch 1988, Loss 0.32772311568260193\n",
      "[Training Epoch 0] Batch 1989, Loss 0.32480013370513916\n",
      "[Training Epoch 0] Batch 1990, Loss 0.3041355609893799\n",
      "[Training Epoch 0] Batch 1991, Loss 0.31595438718795776\n",
      "[Training Epoch 0] Batch 1992, Loss 0.331523597240448\n",
      "[Training Epoch 0] Batch 1993, Loss 0.3353082239627838\n",
      "[Training Epoch 0] Batch 1994, Loss 0.3335944414138794\n",
      "[Training Epoch 0] Batch 1995, Loss 0.3357398509979248\n",
      "[Training Epoch 0] Batch 1996, Loss 0.32682257890701294\n",
      "[Training Epoch 0] Batch 1997, Loss 0.3356977701187134\n",
      "[Training Epoch 0] Batch 1998, Loss 0.3093389570713043\n",
      "[Training Epoch 0] Batch 1999, Loss 0.3536989688873291\n",
      "[Training Epoch 0] Batch 2000, Loss 0.31657877564430237\n",
      "[Training Epoch 0] Batch 2001, Loss 0.2968090772628784\n",
      "[Training Epoch 0] Batch 2002, Loss 0.31014037132263184\n",
      "[Training Epoch 0] Batch 2003, Loss 0.3144929111003876\n",
      "[Training Epoch 0] Batch 2004, Loss 0.30791568756103516\n",
      "[Training Epoch 0] Batch 2005, Loss 0.31976622343063354\n",
      "[Training Epoch 0] Batch 2006, Loss 0.30192238092422485\n",
      "[Training Epoch 0] Batch 2007, Loss 0.3226872384548187\n",
      "[Training Epoch 0] Batch 2008, Loss 0.29345738887786865\n",
      "[Training Epoch 0] Batch 2009, Loss 0.31215935945510864\n",
      "[Training Epoch 0] Batch 2010, Loss 0.34401950240135193\n",
      "[Training Epoch 0] Batch 2011, Loss 0.32832008600234985\n",
      "[Training Epoch 0] Batch 2012, Loss 0.3057235777378082\n",
      "[Training Epoch 0] Batch 2013, Loss 0.27937623858451843\n",
      "[Training Epoch 0] Batch 2014, Loss 0.33064547181129456\n",
      "[Training Epoch 0] Batch 2015, Loss 0.33531051874160767\n",
      "[Training Epoch 0] Batch 2016, Loss 0.34226399660110474\n",
      "[Training Epoch 0] Batch 2017, Loss 0.3463003635406494\n",
      "[Training Epoch 0] Batch 2018, Loss 0.3131517171859741\n",
      "[Training Epoch 0] Batch 2019, Loss 0.31963416934013367\n",
      "[Training Epoch 0] Batch 2020, Loss 0.33084264397621155\n",
      "[Training Epoch 0] Batch 2021, Loss 0.3207743167877197\n",
      "[Training Epoch 0] Batch 2022, Loss 0.3248104155063629\n",
      "[Training Epoch 0] Batch 2023, Loss 0.35321152210235596\n",
      "[Training Epoch 0] Batch 2024, Loss 0.2902194857597351\n",
      "[Training Epoch 0] Batch 2025, Loss 0.29091736674308777\n",
      "[Training Epoch 0] Batch 2026, Loss 0.31634020805358887\n",
      "[Training Epoch 0] Batch 2027, Loss 0.3211447298526764\n",
      "[Training Epoch 0] Batch 2028, Loss 0.30940115451812744\n",
      "[Training Epoch 0] Batch 2029, Loss 0.293196439743042\n",
      "[Training Epoch 0] Batch 2030, Loss 0.3299294710159302\n",
      "[Training Epoch 0] Batch 2031, Loss 0.3291863799095154\n",
      "[Training Epoch 0] Batch 2032, Loss 0.33586275577545166\n",
      "[Training Epoch 0] Batch 2033, Loss 0.3270324468612671\n",
      "[Training Epoch 0] Batch 2034, Loss 0.3464316725730896\n",
      "[Training Epoch 0] Batch 2035, Loss 0.3258785605430603\n",
      "[Training Epoch 0] Batch 2036, Loss 0.30569595098495483\n",
      "[Training Epoch 0] Batch 2037, Loss 0.2931205630302429\n",
      "[Training Epoch 0] Batch 2038, Loss 0.33988523483276367\n",
      "[Training Epoch 0] Batch 2039, Loss 0.333028107881546\n",
      "[Training Epoch 0] Batch 2040, Loss 0.3564600646495819\n",
      "[Training Epoch 0] Batch 2041, Loss 0.33027952909469604\n",
      "[Training Epoch 0] Batch 2042, Loss 0.34109264612197876\n",
      "[Training Epoch 0] Batch 2043, Loss 0.3104453682899475\n",
      "[Training Epoch 0] Batch 2044, Loss 0.3434288203716278\n",
      "[Training Epoch 0] Batch 2045, Loss 0.33006909489631653\n",
      "[Training Epoch 0] Batch 2046, Loss 0.3372421860694885\n",
      "[Training Epoch 0] Batch 2047, Loss 0.3039862811565399\n",
      "[Training Epoch 0] Batch 2048, Loss 0.33052122592926025\n",
      "[Training Epoch 0] Batch 2049, Loss 0.3037568926811218\n",
      "[Training Epoch 0] Batch 2050, Loss 0.333209753036499\n",
      "[Training Epoch 0] Batch 2051, Loss 0.3094104528427124\n",
      "[Training Epoch 0] Batch 2052, Loss 0.33577969670295715\n",
      "[Training Epoch 0] Batch 2053, Loss 0.29433512687683105\n",
      "[Training Epoch 0] Batch 2054, Loss 0.30539053678512573\n",
      "[Training Epoch 0] Batch 2055, Loss 0.33235102891921997\n",
      "[Training Epoch 0] Batch 2056, Loss 0.29945945739746094\n",
      "[Training Epoch 0] Batch 2057, Loss 0.299659788608551\n",
      "[Training Epoch 0] Batch 2058, Loss 0.2937976121902466\n",
      "[Training Epoch 0] Batch 2059, Loss 0.32358789443969727\n",
      "[Training Epoch 0] Batch 2060, Loss 0.34353578090667725\n",
      "[Training Epoch 0] Batch 2061, Loss 0.3442811369895935\n",
      "[Training Epoch 0] Batch 2062, Loss 0.33537378907203674\n",
      "[Training Epoch 0] Batch 2063, Loss 0.32923388481140137\n",
      "[Training Epoch 0] Batch 2064, Loss 0.32024919986724854\n",
      "[Training Epoch 0] Batch 2065, Loss 0.3278796374797821\n",
      "[Training Epoch 0] Batch 2066, Loss 0.3027421534061432\n",
      "[Training Epoch 0] Batch 2067, Loss 0.3294254541397095\n",
      "[Training Epoch 0] Batch 2068, Loss 0.3014611303806305\n",
      "[Training Epoch 0] Batch 2069, Loss 0.32701483368873596\n",
      "[Training Epoch 0] Batch 2070, Loss 0.3070177137851715\n",
      "[Training Epoch 0] Batch 2071, Loss 0.33668985962867737\n",
      "[Training Epoch 0] Batch 2072, Loss 0.3035913407802582\n",
      "[Training Epoch 0] Batch 2073, Loss 0.30825886130332947\n",
      "[Training Epoch 0] Batch 2074, Loss 0.3037201762199402\n",
      "[Training Epoch 0] Batch 2075, Loss 0.32885420322418213\n",
      "[Training Epoch 0] Batch 2076, Loss 0.32661765813827515\n",
      "[Training Epoch 0] Batch 2077, Loss 0.3504366874694824\n",
      "[Training Epoch 0] Batch 2078, Loss 0.34954267740249634\n",
      "[Training Epoch 0] Batch 2079, Loss 0.3065618574619293\n",
      "[Training Epoch 0] Batch 2080, Loss 0.3467637300491333\n",
      "[Training Epoch 0] Batch 2081, Loss 0.3223000168800354\n",
      "[Training Epoch 0] Batch 2082, Loss 0.3439539074897766\n",
      "[Training Epoch 0] Batch 2083, Loss 0.31833112239837646\n",
      "[Training Epoch 0] Batch 2084, Loss 0.3353021740913391\n",
      "[Training Epoch 0] Batch 2085, Loss 0.33879685401916504\n",
      "[Training Epoch 0] Batch 2086, Loss 0.29920390248298645\n",
      "[Training Epoch 0] Batch 2087, Loss 0.32932156324386597\n",
      "[Training Epoch 0] Batch 2088, Loss 0.32476940751075745\n",
      "[Training Epoch 0] Batch 2089, Loss 0.2831730544567108\n",
      "[Training Epoch 0] Batch 2090, Loss 0.3311951756477356\n",
      "[Training Epoch 0] Batch 2091, Loss 0.3177110552787781\n",
      "[Training Epoch 0] Batch 2092, Loss 0.3106064796447754\n",
      "[Training Epoch 0] Batch 2093, Loss 0.29568493366241455\n",
      "[Training Epoch 0] Batch 2094, Loss 0.3528968095779419\n",
      "[Training Epoch 0] Batch 2095, Loss 0.3299822509288788\n",
      "[Training Epoch 0] Batch 2096, Loss 0.3226030766963959\n",
      "[Training Epoch 0] Batch 2097, Loss 0.2961888015270233\n",
      "[Training Epoch 0] Batch 2098, Loss 0.30022743344306946\n",
      "[Training Epoch 0] Batch 2099, Loss 0.3301545977592468\n",
      "[Training Epoch 0] Batch 2100, Loss 0.2996123135089874\n",
      "[Training Epoch 0] Batch 2101, Loss 0.3189818561077118\n",
      "[Training Epoch 0] Batch 2102, Loss 0.3137807846069336\n",
      "[Training Epoch 0] Batch 2103, Loss 0.2977365255355835\n",
      "[Training Epoch 0] Batch 2104, Loss 0.34848567843437195\n",
      "[Training Epoch 0] Batch 2105, Loss 0.30955564975738525\n",
      "[Training Epoch 0] Batch 2106, Loss 0.31310969591140747\n",
      "[Training Epoch 0] Batch 2107, Loss 0.3330596685409546\n",
      "[Training Epoch 0] Batch 2108, Loss 0.30780452489852905\n",
      "[Training Epoch 0] Batch 2109, Loss 0.3123522996902466\n",
      "[Training Epoch 0] Batch 2110, Loss 0.3002968728542328\n",
      "[Training Epoch 0] Batch 2111, Loss 0.3233319818973541\n",
      "[Training Epoch 0] Batch 2112, Loss 0.33460143208503723\n",
      "[Training Epoch 0] Batch 2113, Loss 0.3139638602733612\n",
      "[Training Epoch 0] Batch 2114, Loss 0.33243295550346375\n",
      "[Training Epoch 0] Batch 2115, Loss 0.32520920038223267\n",
      "[Training Epoch 0] Batch 2116, Loss 0.3154045343399048\n",
      "[Training Epoch 0] Batch 2117, Loss 0.299711138010025\n",
      "[Training Epoch 0] Batch 2118, Loss 0.3162466287612915\n",
      "[Training Epoch 0] Batch 2119, Loss 0.298450767993927\n",
      "[Training Epoch 0] Batch 2120, Loss 0.30779191851615906\n",
      "[Training Epoch 0] Batch 2121, Loss 0.3117237985134125\n",
      "[Training Epoch 0] Batch 2122, Loss 0.36110493540763855\n",
      "[Training Epoch 0] Batch 2123, Loss 0.30602508783340454\n",
      "[Training Epoch 0] Batch 2124, Loss 0.3325701653957367\n",
      "[Training Epoch 0] Batch 2125, Loss 0.3268541395664215\n",
      "[Training Epoch 0] Batch 2126, Loss 0.3287956416606903\n",
      "[Training Epoch 0] Batch 2127, Loss 0.3232868015766144\n",
      "[Training Epoch 0] Batch 2128, Loss 0.32375574111938477\n",
      "[Training Epoch 0] Batch 2129, Loss 0.3267309069633484\n",
      "[Training Epoch 0] Batch 2130, Loss 0.33354493975639343\n",
      "[Training Epoch 0] Batch 2131, Loss 0.3229129910469055\n",
      "[Training Epoch 0] Batch 2132, Loss 0.33387741446495056\n",
      "[Training Epoch 0] Batch 2133, Loss 0.28791454434394836\n",
      "[Training Epoch 0] Batch 2134, Loss 0.3129580020904541\n",
      "[Training Epoch 0] Batch 2135, Loss 0.3333278000354767\n",
      "[Training Epoch 0] Batch 2136, Loss 0.3023952841758728\n",
      "[Training Epoch 0] Batch 2137, Loss 0.30072689056396484\n",
      "[Training Epoch 0] Batch 2138, Loss 0.3279871940612793\n",
      "[Training Epoch 0] Batch 2139, Loss 0.3342190980911255\n",
      "[Training Epoch 0] Batch 2140, Loss 0.2885879874229431\n",
      "[Training Epoch 0] Batch 2141, Loss 0.34442442655563354\n",
      "[Training Epoch 0] Batch 2142, Loss 0.3309856355190277\n",
      "[Training Epoch 0] Batch 2143, Loss 0.31434327363967896\n",
      "[Training Epoch 0] Batch 2144, Loss 0.3001042604446411\n",
      "[Training Epoch 0] Batch 2145, Loss 0.30652567744255066\n",
      "[Training Epoch 0] Batch 2146, Loss 0.30419713258743286\n",
      "[Training Epoch 0] Batch 2147, Loss 0.322665274143219\n",
      "[Training Epoch 0] Batch 2148, Loss 0.3214031159877777\n",
      "[Training Epoch 0] Batch 2149, Loss 0.33474230766296387\n",
      "[Training Epoch 0] Batch 2150, Loss 0.3325780928134918\n",
      "[Training Epoch 0] Batch 2151, Loss 0.29472261667251587\n",
      "[Training Epoch 0] Batch 2152, Loss 0.3143540620803833\n",
      "[Training Epoch 0] Batch 2153, Loss 0.342681348323822\n",
      "[Training Epoch 0] Batch 2154, Loss 0.3399120569229126\n",
      "[Training Epoch 0] Batch 2155, Loss 0.31976860761642456\n",
      "[Training Epoch 0] Batch 2156, Loss 0.3263782858848572\n",
      "[Training Epoch 0] Batch 2157, Loss 0.32379868626594543\n",
      "[Training Epoch 0] Batch 2158, Loss 0.28714579343795776\n",
      "[Training Epoch 0] Batch 2159, Loss 0.3112446963787079\n",
      "[Training Epoch 0] Batch 2160, Loss 0.3465035557746887\n",
      "[Training Epoch 0] Batch 2161, Loss 0.31278425455093384\n",
      "[Training Epoch 0] Batch 2162, Loss 0.34081339836120605\n",
      "[Training Epoch 0] Batch 2163, Loss 0.31654831767082214\n",
      "[Training Epoch 0] Batch 2164, Loss 0.28291893005371094\n",
      "[Training Epoch 0] Batch 2165, Loss 0.28988268971443176\n",
      "[Training Epoch 0] Batch 2166, Loss 0.26965048909187317\n",
      "[Training Epoch 0] Batch 2167, Loss 0.34278154373168945\n",
      "[Training Epoch 0] Batch 2168, Loss 0.33660218119621277\n",
      "[Training Epoch 0] Batch 2169, Loss 0.2917827367782593\n",
      "[Training Epoch 0] Batch 2170, Loss 0.34469735622406006\n",
      "[Training Epoch 0] Batch 2171, Loss 0.3017778992652893\n",
      "[Training Epoch 0] Batch 2172, Loss 0.30619800090789795\n",
      "[Training Epoch 0] Batch 2173, Loss 0.3172546327114105\n",
      "[Training Epoch 0] Batch 2174, Loss 0.3406619429588318\n",
      "[Training Epoch 0] Batch 2175, Loss 0.2857677936553955\n",
      "[Training Epoch 0] Batch 2176, Loss 0.3294188976287842\n",
      "[Training Epoch 0] Batch 2177, Loss 0.3096732497215271\n",
      "[Training Epoch 0] Batch 2178, Loss 0.31853604316711426\n",
      "[Training Epoch 0] Batch 2179, Loss 0.2910410463809967\n",
      "[Training Epoch 0] Batch 2180, Loss 0.31730151176452637\n",
      "[Training Epoch 0] Batch 2181, Loss 0.34200146794319153\n",
      "[Training Epoch 0] Batch 2182, Loss 0.34672755002975464\n",
      "[Training Epoch 0] Batch 2183, Loss 0.33874791860580444\n",
      "[Training Epoch 0] Batch 2184, Loss 0.3014374077320099\n",
      "[Training Epoch 0] Batch 2185, Loss 0.32382720708847046\n",
      "[Training Epoch 0] Batch 2186, Loss 0.33426961302757263\n",
      "[Training Epoch 0] Batch 2187, Loss 0.2619737982749939\n",
      "[Training Epoch 0] Batch 2188, Loss 0.274900883436203\n",
      "[Training Epoch 0] Batch 2189, Loss 0.3147103786468506\n",
      "[Training Epoch 0] Batch 2190, Loss 0.30188655853271484\n",
      "[Training Epoch 0] Batch 2191, Loss 0.33268222212791443\n",
      "[Training Epoch 0] Batch 2192, Loss 0.31712231040000916\n",
      "[Training Epoch 0] Batch 2193, Loss 0.29792067408561707\n",
      "[Training Epoch 0] Batch 2194, Loss 0.33584973216056824\n",
      "[Training Epoch 0] Batch 2195, Loss 0.28656867146492004\n",
      "[Training Epoch 0] Batch 2196, Loss 0.37417855858802795\n",
      "[Training Epoch 0] Batch 2197, Loss 0.32445618510246277\n",
      "[Training Epoch 0] Batch 2198, Loss 0.3018045425415039\n",
      "[Training Epoch 0] Batch 2199, Loss 0.3320004642009735\n",
      "[Training Epoch 0] Batch 2200, Loss 0.32831254601478577\n",
      "[Training Epoch 0] Batch 2201, Loss 0.30047816038131714\n",
      "[Training Epoch 0] Batch 2202, Loss 0.33779260516166687\n",
      "[Training Epoch 0] Batch 2203, Loss 0.3104053735733032\n",
      "[Training Epoch 0] Batch 2204, Loss 0.3011988699436188\n",
      "[Training Epoch 0] Batch 2205, Loss 0.3353961706161499\n",
      "[Training Epoch 0] Batch 2206, Loss 0.27201715111732483\n",
      "[Training Epoch 0] Batch 2207, Loss 0.35736170411109924\n",
      "[Training Epoch 0] Batch 2208, Loss 0.3347850739955902\n",
      "[Training Epoch 0] Batch 2209, Loss 0.31489917635917664\n",
      "[Training Epoch 0] Batch 2210, Loss 0.3306816816329956\n",
      "[Training Epoch 0] Batch 2211, Loss 0.3388044238090515\n",
      "[Training Epoch 0] Batch 2212, Loss 0.33922475576400757\n",
      "[Training Epoch 0] Batch 2213, Loss 0.36245298385620117\n",
      "[Training Epoch 0] Batch 2214, Loss 0.3342328369617462\n",
      "[Training Epoch 0] Batch 2215, Loss 0.3097693622112274\n",
      "[Training Epoch 0] Batch 2216, Loss 0.320264607667923\n",
      "[Training Epoch 0] Batch 2217, Loss 0.3109172284603119\n",
      "[Training Epoch 0] Batch 2218, Loss 0.321326345205307\n",
      "[Training Epoch 0] Batch 2219, Loss 0.30562740564346313\n",
      "[Training Epoch 0] Batch 2220, Loss 0.2872806489467621\n",
      "[Training Epoch 0] Batch 2221, Loss 0.33304348587989807\n",
      "[Training Epoch 0] Batch 2222, Loss 0.29009488224983215\n",
      "[Training Epoch 0] Batch 2223, Loss 0.3102055788040161\n",
      "[Training Epoch 0] Batch 2224, Loss 0.3232637941837311\n",
      "[Training Epoch 0] Batch 2225, Loss 0.32281023263931274\n",
      "[Training Epoch 0] Batch 2226, Loss 0.2861425578594208\n",
      "[Training Epoch 0] Batch 2227, Loss 0.3639720380306244\n",
      "[Training Epoch 0] Batch 2228, Loss 0.32308465242385864\n",
      "[Training Epoch 0] Batch 2229, Loss 0.3127375543117523\n",
      "[Training Epoch 0] Batch 2230, Loss 0.3144495189189911\n",
      "[Training Epoch 0] Batch 2231, Loss 0.3321620225906372\n",
      "[Training Epoch 0] Batch 2232, Loss 0.2812320590019226\n",
      "[Training Epoch 0] Batch 2233, Loss 0.3180980682373047\n",
      "[Training Epoch 0] Batch 2234, Loss 0.29188892245292664\n",
      "[Training Epoch 0] Batch 2235, Loss 0.31797921657562256\n",
      "[Training Epoch 0] Batch 2236, Loss 0.3139597773551941\n",
      "[Training Epoch 0] Batch 2237, Loss 0.32448816299438477\n",
      "[Training Epoch 0] Batch 2238, Loss 0.2944367229938507\n",
      "[Training Epoch 0] Batch 2239, Loss 0.3387448191642761\n",
      "[Training Epoch 0] Batch 2240, Loss 0.3023594617843628\n",
      "[Training Epoch 0] Batch 2241, Loss 0.3258386254310608\n",
      "[Training Epoch 0] Batch 2242, Loss 0.3333143889904022\n",
      "[Training Epoch 0] Batch 2243, Loss 0.3033912479877472\n",
      "[Training Epoch 0] Batch 2244, Loss 0.3205212354660034\n",
      "[Training Epoch 0] Batch 2245, Loss 0.3535652756690979\n",
      "[Training Epoch 0] Batch 2246, Loss 0.3230580687522888\n",
      "[Training Epoch 0] Batch 2247, Loss 0.3317897319793701\n",
      "[Training Epoch 0] Batch 2248, Loss 0.32760798931121826\n",
      "[Training Epoch 0] Batch 2249, Loss 0.2924180328845978\n",
      "[Training Epoch 0] Batch 2250, Loss 0.3295988440513611\n",
      "[Training Epoch 0] Batch 2251, Loss 0.3159780502319336\n",
      "[Training Epoch 0] Batch 2252, Loss 0.31439292430877686\n",
      "[Training Epoch 0] Batch 2253, Loss 0.3121771216392517\n",
      "[Training Epoch 0] Batch 2254, Loss 0.3032194972038269\n",
      "[Training Epoch 0] Batch 2255, Loss 0.3335569202899933\n",
      "[Training Epoch 0] Batch 2256, Loss 0.3333203196525574\n",
      "[Training Epoch 0] Batch 2257, Loss 0.3208961486816406\n",
      "[Training Epoch 0] Batch 2258, Loss 0.33214640617370605\n",
      "[Training Epoch 0] Batch 2259, Loss 0.3427761197090149\n",
      "[Training Epoch 0] Batch 2260, Loss 0.31043827533721924\n",
      "[Training Epoch 0] Batch 2261, Loss 0.325604110956192\n",
      "[Training Epoch 0] Batch 2262, Loss 0.3209867775440216\n",
      "[Training Epoch 0] Batch 2263, Loss 0.31661689281463623\n",
      "[Training Epoch 0] Batch 2264, Loss 0.3442450761795044\n",
      "[Training Epoch 0] Batch 2265, Loss 0.3294335603713989\n",
      "[Training Epoch 0] Batch 2266, Loss 0.33617377281188965\n",
      "[Training Epoch 0] Batch 2267, Loss 0.3010762631893158\n",
      "[Training Epoch 0] Batch 2268, Loss 0.30707231163978577\n",
      "[Training Epoch 0] Batch 2269, Loss 0.3312813341617584\n",
      "[Training Epoch 0] Batch 2270, Loss 0.34173136949539185\n",
      "[Training Epoch 0] Batch 2271, Loss 0.2925742566585541\n",
      "[Training Epoch 0] Batch 2272, Loss 0.3259561061859131\n",
      "[Training Epoch 0] Batch 2273, Loss 0.31781449913978577\n",
      "[Training Epoch 0] Batch 2274, Loss 0.3108733296394348\n",
      "[Training Epoch 0] Batch 2275, Loss 0.30428457260131836\n",
      "[Training Epoch 0] Batch 2276, Loss 0.3180682957172394\n",
      "[Training Epoch 0] Batch 2277, Loss 0.30939748883247375\n",
      "[Training Epoch 0] Batch 2278, Loss 0.2892214059829712\n",
      "[Training Epoch 0] Batch 2279, Loss 0.3160058856010437\n",
      "[Training Epoch 0] Batch 2280, Loss 0.3139137029647827\n",
      "[Training Epoch 0] Batch 2281, Loss 0.31493306159973145\n",
      "[Training Epoch 0] Batch 2282, Loss 0.33703941106796265\n",
      "[Training Epoch 0] Batch 2283, Loss 0.30304259061813354\n",
      "[Training Epoch 0] Batch 2284, Loss 0.3275875449180603\n",
      "[Training Epoch 0] Batch 2285, Loss 0.3233983516693115\n",
      "[Training Epoch 0] Batch 2286, Loss 0.31651902198791504\n",
      "[Training Epoch 0] Batch 2287, Loss 0.3083048164844513\n",
      "[Training Epoch 0] Batch 2288, Loss 0.28752967715263367\n",
      "[Training Epoch 0] Batch 2289, Loss 0.3342699408531189\n",
      "[Training Epoch 0] Batch 2290, Loss 0.3074958920478821\n",
      "[Training Epoch 0] Batch 2291, Loss 0.31866908073425293\n",
      "[Training Epoch 0] Batch 2292, Loss 0.2997710704803467\n",
      "[Training Epoch 0] Batch 2293, Loss 0.3319432735443115\n",
      "[Training Epoch 0] Batch 2294, Loss 0.3300129771232605\n",
      "[Training Epoch 0] Batch 2295, Loss 0.31283727288246155\n",
      "[Training Epoch 0] Batch 2296, Loss 0.37655192613601685\n",
      "[Training Epoch 0] Batch 2297, Loss 0.3453294038772583\n",
      "[Training Epoch 0] Batch 2298, Loss 0.3163272738456726\n",
      "[Training Epoch 0] Batch 2299, Loss 0.308979868888855\n",
      "[Training Epoch 0] Batch 2300, Loss 0.3156648576259613\n",
      "[Training Epoch 0] Batch 2301, Loss 0.3075436055660248\n",
      "[Training Epoch 0] Batch 2302, Loss 0.28526970744132996\n",
      "[Training Epoch 0] Batch 2303, Loss 0.31366419792175293\n",
      "[Training Epoch 0] Batch 2304, Loss 0.3410477936267853\n",
      "[Training Epoch 0] Batch 2305, Loss 0.3431527018547058\n",
      "[Training Epoch 0] Batch 2306, Loss 0.3176296055316925\n",
      "[Training Epoch 0] Batch 2307, Loss 0.3536528944969177\n",
      "[Training Epoch 0] Batch 2308, Loss 0.333261102437973\n",
      "[Training Epoch 0] Batch 2309, Loss 0.3009642958641052\n",
      "[Training Epoch 0] Batch 2310, Loss 0.32184460759162903\n",
      "[Training Epoch 0] Batch 2311, Loss 0.3211976885795593\n",
      "[Training Epoch 0] Batch 2312, Loss 0.34407979249954224\n",
      "[Training Epoch 0] Batch 2313, Loss 0.3468658924102783\n",
      "[Training Epoch 0] Batch 2314, Loss 0.30244094133377075\n",
      "[Training Epoch 0] Batch 2315, Loss 0.27446943521499634\n",
      "[Training Epoch 0] Batch 2316, Loss 0.32617706060409546\n",
      "[Training Epoch 0] Batch 2317, Loss 0.34106940031051636\n",
      "[Training Epoch 0] Batch 2318, Loss 0.3040299117565155\n",
      "[Training Epoch 0] Batch 2319, Loss 0.32224810123443604\n",
      "[Training Epoch 0] Batch 2320, Loss 0.2816525995731354\n",
      "[Training Epoch 0] Batch 2321, Loss 0.32922208309173584\n",
      "[Training Epoch 0] Batch 2322, Loss 0.3059596121311188\n",
      "[Training Epoch 0] Batch 2323, Loss 0.32966190576553345\n",
      "[Training Epoch 0] Batch 2324, Loss 0.3216208219528198\n",
      "[Training Epoch 0] Batch 2325, Loss 0.3533463180065155\n",
      "[Training Epoch 0] Batch 2326, Loss 0.2749510407447815\n",
      "[Training Epoch 0] Batch 2327, Loss 0.2735840380191803\n",
      "[Training Epoch 0] Batch 2328, Loss 0.323091596364975\n",
      "[Training Epoch 0] Batch 2329, Loss 0.32784533500671387\n",
      "[Training Epoch 0] Batch 2330, Loss 0.32193228602409363\n",
      "[Training Epoch 0] Batch 2331, Loss 0.3074767589569092\n",
      "[Training Epoch 0] Batch 2332, Loss 0.3313026428222656\n",
      "[Training Epoch 0] Batch 2333, Loss 0.33001452684402466\n",
      "[Training Epoch 0] Batch 2334, Loss 0.3311242163181305\n",
      "[Training Epoch 0] Batch 2335, Loss 0.34631800651550293\n",
      "[Training Epoch 0] Batch 2336, Loss 0.3246009349822998\n",
      "[Training Epoch 0] Batch 2337, Loss 0.32433822751045227\n",
      "[Training Epoch 0] Batch 2338, Loss 0.30826660990715027\n",
      "[Training Epoch 0] Batch 2339, Loss 0.31789112091064453\n",
      "[Training Epoch 0] Batch 2340, Loss 0.3237167298793793\n",
      "[Training Epoch 0] Batch 2341, Loss 0.3219780921936035\n",
      "[Training Epoch 0] Batch 2342, Loss 0.33563369512557983\n",
      "[Training Epoch 0] Batch 2343, Loss 0.2947065532207489\n",
      "[Training Epoch 0] Batch 2344, Loss 0.32530534267425537\n",
      "[Training Epoch 0] Batch 2345, Loss 0.32638445496559143\n",
      "[Training Epoch 0] Batch 2346, Loss 0.35929054021835327\n",
      "[Training Epoch 0] Batch 2347, Loss 0.3007715046405792\n",
      "[Training Epoch 0] Batch 2348, Loss 0.29989057779312134\n",
      "[Training Epoch 0] Batch 2349, Loss 0.3272841274738312\n",
      "[Training Epoch 0] Batch 2350, Loss 0.30021071434020996\n",
      "[Training Epoch 0] Batch 2351, Loss 0.29202577471733093\n",
      "[Training Epoch 0] Batch 2352, Loss 0.29800671339035034\n",
      "[Training Epoch 0] Batch 2353, Loss 0.30567342042922974\n",
      "[Training Epoch 0] Batch 2354, Loss 0.31715044379234314\n",
      "[Training Epoch 0] Batch 2355, Loss 0.3033725321292877\n",
      "[Training Epoch 0] Batch 2356, Loss 0.306774765253067\n",
      "[Training Epoch 0] Batch 2357, Loss 0.3151823878288269\n",
      "[Training Epoch 0] Batch 2358, Loss 0.301929771900177\n",
      "[Training Epoch 0] Batch 2359, Loss 0.30081337690353394\n",
      "[Training Epoch 0] Batch 2360, Loss 0.3176489770412445\n",
      "[Training Epoch 0] Batch 2361, Loss 0.3248145580291748\n",
      "[Training Epoch 0] Batch 2362, Loss 0.3220866620540619\n",
      "[Training Epoch 0] Batch 2363, Loss 0.3363175392150879\n",
      "[Training Epoch 0] Batch 2364, Loss 0.3148767352104187\n",
      "[Training Epoch 0] Batch 2365, Loss 0.33135902881622314\n",
      "[Training Epoch 0] Batch 2366, Loss 0.29421764612197876\n",
      "[Training Epoch 0] Batch 2367, Loss 0.2954629063606262\n",
      "[Training Epoch 0] Batch 2368, Loss 0.3565013110637665\n",
      "[Training Epoch 0] Batch 2369, Loss 0.28667405247688293\n",
      "[Training Epoch 0] Batch 2370, Loss 0.30401936173439026\n",
      "[Training Epoch 0] Batch 2371, Loss 0.31404510140419006\n",
      "[Training Epoch 0] Batch 2372, Loss 0.3441346287727356\n",
      "[Training Epoch 0] Batch 2373, Loss 0.3114051818847656\n",
      "[Training Epoch 0] Batch 2374, Loss 0.30754441022872925\n",
      "[Training Epoch 0] Batch 2375, Loss 0.3309817612171173\n",
      "[Training Epoch 0] Batch 2376, Loss 0.29458528757095337\n",
      "[Training Epoch 0] Batch 2377, Loss 0.3127812445163727\n",
      "[Training Epoch 0] Batch 2378, Loss 0.28987276554107666\n",
      "[Training Epoch 0] Batch 2379, Loss 0.324630469083786\n",
      "[Training Epoch 0] Batch 2380, Loss 0.3160683214664459\n",
      "[Training Epoch 0] Batch 2381, Loss 0.2840404212474823\n",
      "[Training Epoch 0] Batch 2382, Loss 0.322691410779953\n",
      "[Training Epoch 0] Batch 2383, Loss 0.29953914880752563\n",
      "[Training Epoch 0] Batch 2384, Loss 0.30549368262290955\n",
      "[Training Epoch 0] Batch 2385, Loss 0.342682421207428\n",
      "[Training Epoch 0] Batch 2386, Loss 0.31913086771965027\n",
      "[Training Epoch 0] Batch 2387, Loss 0.28775304555892944\n",
      "[Training Epoch 0] Batch 2388, Loss 0.286246120929718\n",
      "[Training Epoch 0] Batch 2389, Loss 0.3325890600681305\n",
      "[Training Epoch 0] Batch 2390, Loss 0.29508230090141296\n",
      "[Training Epoch 0] Batch 2391, Loss 0.30205273628234863\n",
      "[Training Epoch 0] Batch 2392, Loss 0.3054003119468689\n",
      "[Training Epoch 0] Batch 2393, Loss 0.29918617010116577\n",
      "[Training Epoch 0] Batch 2394, Loss 0.30750948190689087\n",
      "[Training Epoch 0] Batch 2395, Loss 0.34479820728302\n",
      "[Training Epoch 0] Batch 2396, Loss 0.3032534122467041\n",
      "[Training Epoch 0] Batch 2397, Loss 0.2915782630443573\n",
      "[Training Epoch 0] Batch 2398, Loss 0.32184797525405884\n",
      "[Training Epoch 0] Batch 2399, Loss 0.34268876910209656\n",
      "[Training Epoch 0] Batch 2400, Loss 0.3734219968318939\n",
      "[Training Epoch 0] Batch 2401, Loss 0.33487337827682495\n",
      "[Training Epoch 0] Batch 2402, Loss 0.3280029892921448\n",
      "[Training Epoch 0] Batch 2403, Loss 0.32159918546676636\n",
      "[Training Epoch 0] Batch 2404, Loss 0.33674320578575134\n",
      "[Training Epoch 0] Batch 2405, Loss 0.30745160579681396\n",
      "[Training Epoch 0] Batch 2406, Loss 0.30652710795402527\n",
      "[Training Epoch 0] Batch 2407, Loss 0.3218557834625244\n",
      "[Training Epoch 0] Batch 2408, Loss 0.33830365538597107\n",
      "[Training Epoch 0] Batch 2409, Loss 0.28555160760879517\n",
      "[Training Epoch 0] Batch 2410, Loss 0.37126997113227844\n",
      "[Training Epoch 0] Batch 2411, Loss 0.30396342277526855\n",
      "[Training Epoch 0] Batch 2412, Loss 0.31044870615005493\n",
      "[Training Epoch 0] Batch 2413, Loss 0.3263455629348755\n",
      "[Training Epoch 0] Batch 2414, Loss 0.32853615283966064\n",
      "[Training Epoch 0] Batch 2415, Loss 0.2989685833454132\n",
      "[Training Epoch 0] Batch 2416, Loss 0.3178606331348419\n",
      "[Training Epoch 0] Batch 2417, Loss 0.32402557134628296\n",
      "[Training Epoch 0] Batch 2418, Loss 0.28216293454170227\n",
      "[Training Epoch 0] Batch 2419, Loss 0.30065032839775085\n",
      "[Training Epoch 0] Batch 2420, Loss 0.3390743136405945\n",
      "[Training Epoch 0] Batch 2421, Loss 0.30396485328674316\n",
      "[Training Epoch 0] Batch 2422, Loss 0.3227623999118805\n",
      "[Training Epoch 0] Batch 2423, Loss 0.3199424743652344\n",
      "[Training Epoch 0] Batch 2424, Loss 0.3546521067619324\n",
      "[Training Epoch 0] Batch 2425, Loss 0.318179190158844\n",
      "[Training Epoch 0] Batch 2426, Loss 0.2992270290851593\n",
      "[Training Epoch 0] Batch 2427, Loss 0.29954901337623596\n",
      "[Training Epoch 0] Batch 2428, Loss 0.3245096802711487\n",
      "[Training Epoch 0] Batch 2429, Loss 0.34702008962631226\n",
      "[Training Epoch 0] Batch 2430, Loss 0.317390114068985\n",
      "[Training Epoch 0] Batch 2431, Loss 0.3086477518081665\n",
      "[Training Epoch 0] Batch 2432, Loss 0.311307817697525\n",
      "[Training Epoch 0] Batch 2433, Loss 0.33555930852890015\n",
      "[Training Epoch 0] Batch 2434, Loss 0.3170458674430847\n",
      "[Training Epoch 0] Batch 2435, Loss 0.3102329671382904\n",
      "[Training Epoch 0] Batch 2436, Loss 0.330295205116272\n",
      "[Training Epoch 0] Batch 2437, Loss 0.3225194215774536\n",
      "[Training Epoch 0] Batch 2438, Loss 0.31763291358947754\n",
      "[Training Epoch 0] Batch 2439, Loss 0.2884460985660553\n",
      "[Training Epoch 0] Batch 2440, Loss 0.29927539825439453\n",
      "[Training Epoch 0] Batch 2441, Loss 0.32637593150138855\n",
      "[Training Epoch 0] Batch 2442, Loss 0.31590884923934937\n",
      "[Training Epoch 0] Batch 2443, Loss 0.3333882689476013\n",
      "[Training Epoch 0] Batch 2444, Loss 0.3039277195930481\n",
      "[Training Epoch 0] Batch 2445, Loss 0.3388076424598694\n",
      "[Training Epoch 0] Batch 2446, Loss 0.30401232838630676\n",
      "[Training Epoch 0] Batch 2447, Loss 0.32115551829338074\n",
      "[Training Epoch 0] Batch 2448, Loss 0.3164828419685364\n",
      "[Training Epoch 0] Batch 2449, Loss 0.282867431640625\n",
      "[Training Epoch 0] Batch 2450, Loss 0.3296583890914917\n",
      "[Training Epoch 0] Batch 2451, Loss 0.3183867037296295\n",
      "[Training Epoch 0] Batch 2452, Loss 0.3348360061645508\n",
      "[Training Epoch 0] Batch 2453, Loss 0.3335125148296356\n",
      "[Training Epoch 0] Batch 2454, Loss 0.3197987377643585\n",
      "[Training Epoch 0] Batch 2455, Loss 0.30743759870529175\n",
      "[Training Epoch 0] Batch 2456, Loss 0.3423621654510498\n",
      "[Training Epoch 0] Batch 2457, Loss 0.3379141092300415\n",
      "[Training Epoch 0] Batch 2458, Loss 0.30947938561439514\n",
      "[Training Epoch 0] Batch 2459, Loss 0.3282010555267334\n",
      "[Training Epoch 0] Batch 2460, Loss 0.331865519285202\n",
      "[Training Epoch 0] Batch 2461, Loss 0.31828486919403076\n",
      "[Training Epoch 0] Batch 2462, Loss 0.2800433039665222\n",
      "[Training Epoch 0] Batch 2463, Loss 0.31749412417411804\n",
      "[Training Epoch 0] Batch 2464, Loss 0.30583733320236206\n",
      "[Training Epoch 0] Batch 2465, Loss 0.34745216369628906\n",
      "[Training Epoch 0] Batch 2466, Loss 0.32443884015083313\n",
      "[Training Epoch 0] Batch 2467, Loss 0.2958787977695465\n",
      "[Training Epoch 0] Batch 2468, Loss 0.3319438695907593\n",
      "[Training Epoch 0] Batch 2469, Loss 0.29537636041641235\n",
      "[Training Epoch 0] Batch 2470, Loss 0.3138048052787781\n",
      "[Training Epoch 0] Batch 2471, Loss 0.3448275327682495\n",
      "[Training Epoch 0] Batch 2472, Loss 0.3298964202404022\n",
      "[Training Epoch 0] Batch 2473, Loss 0.3214126229286194\n",
      "[Training Epoch 0] Batch 2474, Loss 0.3438367545604706\n",
      "[Training Epoch 0] Batch 2475, Loss 0.3073573112487793\n",
      "[Training Epoch 0] Batch 2476, Loss 0.32009315490722656\n",
      "[Training Epoch 0] Batch 2477, Loss 0.2967306673526764\n",
      "[Training Epoch 0] Batch 2478, Loss 0.34536102414131165\n",
      "[Training Epoch 0] Batch 2479, Loss 0.28158652782440186\n",
      "[Training Epoch 0] Batch 2480, Loss 0.32562312483787537\n",
      "[Training Epoch 0] Batch 2481, Loss 0.31436556577682495\n",
      "[Training Epoch 0] Batch 2482, Loss 0.3571018576622009\n",
      "[Training Epoch 0] Batch 2483, Loss 0.32353195548057556\n",
      "[Training Epoch 0] Batch 2484, Loss 0.2851834297180176\n",
      "[Training Epoch 0] Batch 2485, Loss 0.3427198827266693\n",
      "[Training Epoch 0] Batch 2486, Loss 0.28802695870399475\n",
      "[Training Epoch 0] Batch 2487, Loss 0.2906278073787689\n",
      "[Training Epoch 0] Batch 2488, Loss 0.3450806736946106\n",
      "[Training Epoch 0] Batch 2489, Loss 0.32067710161209106\n",
      "[Training Epoch 0] Batch 2490, Loss 0.32203060388565063\n",
      "[Training Epoch 0] Batch 2491, Loss 0.3442375063896179\n",
      "[Training Epoch 0] Batch 2492, Loss 0.29982784390449524\n",
      "[Training Epoch 0] Batch 2493, Loss 0.3217664062976837\n",
      "[Training Epoch 0] Batch 2494, Loss 0.311065137386322\n",
      "[Training Epoch 0] Batch 2495, Loss 0.32562339305877686\n",
      "[Training Epoch 0] Batch 2496, Loss 0.3269217312335968\n",
      "[Training Epoch 0] Batch 2497, Loss 0.3072650730609894\n",
      "[Training Epoch 0] Batch 2498, Loss 0.3096437454223633\n",
      "[Training Epoch 0] Batch 2499, Loss 0.3151055574417114\n",
      "[Training Epoch 0] Batch 2500, Loss 0.34176939725875854\n",
      "[Training Epoch 0] Batch 2501, Loss 0.32239583134651184\n",
      "[Training Epoch 0] Batch 2502, Loss 0.3437255322933197\n",
      "[Training Epoch 0] Batch 2503, Loss 0.3043116331100464\n",
      "[Training Epoch 0] Batch 2504, Loss 0.30401721596717834\n",
      "[Training Epoch 0] Batch 2505, Loss 0.28652384877204895\n",
      "[Training Epoch 0] Batch 2506, Loss 0.3189676105976105\n",
      "[Training Epoch 0] Batch 2507, Loss 0.34928175806999207\n",
      "[Training Epoch 0] Batch 2508, Loss 0.34948864579200745\n",
      "[Training Epoch 0] Batch 2509, Loss 0.3425891697406769\n",
      "[Training Epoch 0] Batch 2510, Loss 0.2775619626045227\n",
      "[Training Epoch 0] Batch 2511, Loss 0.29140210151672363\n",
      "[Training Epoch 0] Batch 2512, Loss 0.3053169548511505\n",
      "[Training Epoch 0] Batch 2513, Loss 0.3198436498641968\n",
      "[Training Epoch 0] Batch 2514, Loss 0.30610546469688416\n",
      "[Training Epoch 0] Batch 2515, Loss 0.330973356962204\n",
      "[Training Epoch 0] Batch 2516, Loss 0.32541200518608093\n",
      "[Training Epoch 0] Batch 2517, Loss 0.34472712874412537\n",
      "[Training Epoch 0] Batch 2518, Loss 0.32034915685653687\n",
      "[Training Epoch 0] Batch 2519, Loss 0.3029928207397461\n",
      "[Training Epoch 0] Batch 2520, Loss 0.34606385231018066\n",
      "[Training Epoch 0] Batch 2521, Loss 0.3390471637248993\n",
      "[Training Epoch 0] Batch 2522, Loss 0.3082095980644226\n",
      "[Training Epoch 0] Batch 2523, Loss 0.3293280005455017\n",
      "[Training Epoch 0] Batch 2524, Loss 0.3216874599456787\n",
      "[Training Epoch 0] Batch 2525, Loss 0.29850322008132935\n",
      "[Training Epoch 0] Batch 2526, Loss 0.2984103858470917\n",
      "[Training Epoch 0] Batch 2527, Loss 0.2839634120464325\n",
      "[Training Epoch 0] Batch 2528, Loss 0.334500253200531\n",
      "[Training Epoch 0] Batch 2529, Loss 0.29623648524284363\n",
      "[Training Epoch 0] Batch 2530, Loss 0.29168951511383057\n",
      "[Training Epoch 0] Batch 2531, Loss 0.35339996218681335\n",
      "[Training Epoch 0] Batch 2532, Loss 0.30950111150741577\n",
      "[Training Epoch 0] Batch 2533, Loss 0.32501283288002014\n",
      "[Training Epoch 0] Batch 2534, Loss 0.32054105401039124\n",
      "[Training Epoch 0] Batch 2535, Loss 0.32618245482444763\n",
      "[Training Epoch 0] Batch 2536, Loss 0.32653164863586426\n",
      "[Training Epoch 0] Batch 2537, Loss 0.284758597612381\n",
      "[Training Epoch 0] Batch 2538, Loss 0.31143930554389954\n",
      "[Training Epoch 0] Batch 2539, Loss 0.29769545793533325\n",
      "[Training Epoch 0] Batch 2540, Loss 0.29060453176498413\n",
      "[Training Epoch 0] Batch 2541, Loss 0.2880230247974396\n",
      "[Training Epoch 0] Batch 2542, Loss 0.31141358613967896\n",
      "[Training Epoch 0] Batch 2543, Loss 0.31855636835098267\n",
      "[Training Epoch 0] Batch 2544, Loss 0.3007977306842804\n",
      "[Training Epoch 0] Batch 2545, Loss 0.33033305406570435\n",
      "[Training Epoch 0] Batch 2546, Loss 0.34951719641685486\n",
      "[Training Epoch 0] Batch 2547, Loss 0.33993804454803467\n",
      "[Training Epoch 0] Batch 2548, Loss 0.3262760043144226\n",
      "[Training Epoch 0] Batch 2549, Loss 0.3154248893260956\n",
      "[Training Epoch 0] Batch 2550, Loss 0.30734023451805115\n",
      "[Training Epoch 0] Batch 2551, Loss 0.34942907094955444\n",
      "[Training Epoch 0] Batch 2552, Loss 0.34163713455200195\n",
      "[Training Epoch 0] Batch 2553, Loss 0.33775562047958374\n",
      "[Training Epoch 0] Batch 2554, Loss 0.3324023485183716\n",
      "[Training Epoch 0] Batch 2555, Loss 0.2753547430038452\n",
      "[Training Epoch 0] Batch 2556, Loss 0.31518280506134033\n",
      "[Training Epoch 0] Batch 2557, Loss 0.3267679512500763\n",
      "[Training Epoch 0] Batch 2558, Loss 0.31117796897888184\n",
      "[Training Epoch 0] Batch 2559, Loss 0.29661646485328674\n",
      "[Training Epoch 0] Batch 2560, Loss 0.33975714445114136\n",
      "[Training Epoch 0] Batch 2561, Loss 0.32671499252319336\n",
      "[Training Epoch 0] Batch 2562, Loss 0.3290041983127594\n",
      "[Training Epoch 0] Batch 2563, Loss 0.2986115515232086\n",
      "[Training Epoch 0] Batch 2564, Loss 0.3458594083786011\n",
      "[Training Epoch 0] Batch 2565, Loss 0.34882235527038574\n",
      "[Training Epoch 0] Batch 2566, Loss 0.3370334506034851\n",
      "[Training Epoch 0] Batch 2567, Loss 0.30931225419044495\n",
      "[Training Epoch 0] Batch 2568, Loss 0.33527499437332153\n",
      "[Training Epoch 0] Batch 2569, Loss 0.32126474380493164\n",
      "[Training Epoch 0] Batch 2570, Loss 0.3284810483455658\n",
      "[Training Epoch 0] Batch 2571, Loss 0.3321096897125244\n",
      "[Training Epoch 0] Batch 2572, Loss 0.3017692565917969\n",
      "[Training Epoch 0] Batch 2573, Loss 0.3216853439807892\n",
      "[Training Epoch 0] Batch 2574, Loss 0.32005733251571655\n",
      "[Training Epoch 0] Batch 2575, Loss 0.3329309821128845\n",
      "[Training Epoch 0] Batch 2576, Loss 0.3435552716255188\n",
      "[Training Epoch 0] Batch 2577, Loss 0.3041638433933258\n",
      "[Training Epoch 0] Batch 2578, Loss 0.3356820046901703\n",
      "[Training Epoch 0] Batch 2579, Loss 0.3262346386909485\n",
      "[Training Epoch 0] Batch 2580, Loss 0.3294295072555542\n",
      "[Training Epoch 0] Batch 2581, Loss 0.3401494324207306\n",
      "[Training Epoch 0] Batch 2582, Loss 0.33274754881858826\n",
      "[Training Epoch 0] Batch 2583, Loss 0.31678953766822815\n",
      "[Training Epoch 0] Batch 2584, Loss 0.316148042678833\n",
      "[Training Epoch 0] Batch 2585, Loss 0.31638264656066895\n",
      "[Training Epoch 0] Batch 2586, Loss 0.31469935178756714\n",
      "[Training Epoch 0] Batch 2587, Loss 0.32778438925743103\n",
      "[Training Epoch 0] Batch 2588, Loss 0.3245081901550293\n",
      "[Training Epoch 0] Batch 2589, Loss 0.31441622972488403\n",
      "[Training Epoch 0] Batch 2590, Loss 0.3229595720767975\n",
      "[Training Epoch 0] Batch 2591, Loss 0.3353221118450165\n",
      "[Training Epoch 0] Batch 2592, Loss 0.31836003065109253\n",
      "[Training Epoch 0] Batch 2593, Loss 0.3320663273334503\n",
      "[Training Epoch 0] Batch 2594, Loss 0.3251359164714813\n",
      "[Training Epoch 0] Batch 2595, Loss 0.2964745759963989\n",
      "[Training Epoch 0] Batch 2596, Loss 0.33634868264198303\n",
      "[Training Epoch 0] Batch 2597, Loss 0.32932549715042114\n",
      "[Training Epoch 0] Batch 2598, Loss 0.3010280430316925\n",
      "[Training Epoch 0] Batch 2599, Loss 0.3157109022140503\n",
      "[Training Epoch 0] Batch 2600, Loss 0.33815574645996094\n",
      "[Training Epoch 0] Batch 2601, Loss 0.33008623123168945\n",
      "[Training Epoch 0] Batch 2602, Loss 0.3259713649749756\n",
      "[Training Epoch 0] Batch 2603, Loss 0.3285585939884186\n",
      "[Training Epoch 0] Batch 2604, Loss 0.30603253841400146\n",
      "[Training Epoch 0] Batch 2605, Loss 0.35123980045318604\n",
      "[Training Epoch 0] Batch 2606, Loss 0.331975519657135\n",
      "[Training Epoch 0] Batch 2607, Loss 0.3181292712688446\n",
      "[Training Epoch 0] Batch 2608, Loss 0.3208141624927521\n",
      "[Training Epoch 0] Batch 2609, Loss 0.3064723014831543\n",
      "[Training Epoch 0] Batch 2610, Loss 0.30415356159210205\n",
      "[Training Epoch 0] Batch 2611, Loss 0.298347532749176\n",
      "[Training Epoch 0] Batch 2612, Loss 0.325877845287323\n",
      "[Training Epoch 0] Batch 2613, Loss 0.3057249188423157\n",
      "[Training Epoch 0] Batch 2614, Loss 0.3107765316963196\n",
      "[Training Epoch 0] Batch 2615, Loss 0.3105858266353607\n",
      "[Training Epoch 0] Batch 2616, Loss 0.31418466567993164\n",
      "[Training Epoch 0] Batch 2617, Loss 0.30119603872299194\n",
      "[Training Epoch 0] Batch 2618, Loss 0.31162577867507935\n",
      "[Training Epoch 0] Batch 2619, Loss 0.2894533574581146\n",
      "[Training Epoch 0] Batch 2620, Loss 0.3396202027797699\n",
      "[Training Epoch 0] Batch 2621, Loss 0.2965331971645355\n",
      "[Training Epoch 0] Batch 2622, Loss 0.34953004121780396\n",
      "[Training Epoch 0] Batch 2623, Loss 0.300938218832016\n",
      "[Training Epoch 0] Batch 2624, Loss 0.3337879478931427\n",
      "[Training Epoch 0] Batch 2625, Loss 0.28059786558151245\n",
      "[Training Epoch 0] Batch 2626, Loss 0.3424327075481415\n",
      "[Training Epoch 0] Batch 2627, Loss 0.3231033980846405\n",
      "[Training Epoch 0] Batch 2628, Loss 0.31420445442199707\n",
      "[Training Epoch 0] Batch 2629, Loss 0.31307777762413025\n",
      "[Training Epoch 0] Batch 2630, Loss 0.3135456144809723\n",
      "[Training Epoch 0] Batch 2631, Loss 0.32492104172706604\n",
      "[Training Epoch 0] Batch 2632, Loss 0.34584158658981323\n",
      "[Training Epoch 0] Batch 2633, Loss 0.3226564824581146\n",
      "[Training Epoch 0] Batch 2634, Loss 0.33281341195106506\n",
      "[Training Epoch 0] Batch 2635, Loss 0.2769889235496521\n",
      "[Training Epoch 0] Batch 2636, Loss 0.33156895637512207\n",
      "[Training Epoch 0] Batch 2637, Loss 0.32479506731033325\n",
      "[Training Epoch 0] Batch 2638, Loss 0.32700952887535095\n",
      "[Training Epoch 0] Batch 2639, Loss 0.3251917362213135\n",
      "[Training Epoch 0] Batch 2640, Loss 0.3090353310108185\n",
      "[Training Epoch 0] Batch 2641, Loss 0.2723844647407532\n",
      "[Training Epoch 0] Batch 2642, Loss 0.30258074402809143\n",
      "[Training Epoch 0] Batch 2643, Loss 0.33331871032714844\n",
      "[Training Epoch 0] Batch 2644, Loss 0.2987076938152313\n",
      "[Training Epoch 0] Batch 2645, Loss 0.30617690086364746\n",
      "[Training Epoch 0] Batch 2646, Loss 0.36468344926834106\n",
      "[Training Epoch 0] Batch 2647, Loss 0.3400903046131134\n",
      "[Training Epoch 0] Batch 2648, Loss 0.32416918873786926\n",
      "[Training Epoch 0] Batch 2649, Loss 0.311273455619812\n",
      "[Training Epoch 0] Batch 2650, Loss 0.3207497298717499\n",
      "[Training Epoch 0] Batch 2651, Loss 0.34849613904953003\n",
      "[Training Epoch 0] Batch 2652, Loss 0.31733438372612\n",
      "[Training Epoch 0] Batch 2653, Loss 0.3212171792984009\n",
      "[Training Epoch 0] Batch 2654, Loss 0.30446985363960266\n",
      "[Training Epoch 0] Batch 2655, Loss 0.29198309779167175\n",
      "[Training Epoch 0] Batch 2656, Loss 0.3049597144126892\n",
      "[Training Epoch 0] Batch 2657, Loss 0.32123446464538574\n",
      "[Training Epoch 0] Batch 2658, Loss 0.27727019786834717\n",
      "[Training Epoch 0] Batch 2659, Loss 0.3083920180797577\n",
      "[Training Epoch 0] Batch 2660, Loss 0.3071991801261902\n",
      "[Training Epoch 0] Batch 2661, Loss 0.3124183714389801\n",
      "[Training Epoch 0] Batch 2662, Loss 0.2852829694747925\n",
      "[Training Epoch 0] Batch 2663, Loss 0.29257237911224365\n",
      "[Training Epoch 0] Batch 2664, Loss 0.2886400818824768\n",
      "[Training Epoch 0] Batch 2665, Loss 0.2916226387023926\n",
      "[Training Epoch 0] Batch 2666, Loss 0.32181844115257263\n",
      "[Training Epoch 0] Batch 2667, Loss 0.3090856969356537\n",
      "[Training Epoch 0] Batch 2668, Loss 0.30311712622642517\n",
      "[Training Epoch 0] Batch 2669, Loss 0.32197660207748413\n",
      "[Training Epoch 0] Batch 2670, Loss 0.3059970438480377\n",
      "[Training Epoch 0] Batch 2671, Loss 0.2927578389644623\n",
      "[Training Epoch 0] Batch 2672, Loss 0.2914391756057739\n",
      "[Training Epoch 0] Batch 2673, Loss 0.29733896255493164\n",
      "[Training Epoch 0] Batch 2674, Loss 0.3157137930393219\n",
      "[Training Epoch 0] Batch 2675, Loss 0.3047524690628052\n",
      "[Training Epoch 0] Batch 2676, Loss 0.3473200798034668\n",
      "[Training Epoch 0] Batch 2677, Loss 0.28757238388061523\n",
      "[Training Epoch 0] Batch 2678, Loss 0.3141317367553711\n",
      "[Training Epoch 0] Batch 2679, Loss 0.29144367575645447\n",
      "[Training Epoch 0] Batch 2680, Loss 0.31505343317985535\n",
      "[Training Epoch 0] Batch 2681, Loss 0.31574195623397827\n",
      "[Training Epoch 0] Batch 2682, Loss 0.3392452597618103\n",
      "[Training Epoch 0] Batch 2683, Loss 0.295087069272995\n",
      "[Training Epoch 0] Batch 2684, Loss 0.33706796169281006\n",
      "[Training Epoch 0] Batch 2685, Loss 0.3444349467754364\n",
      "[Training Epoch 0] Batch 2686, Loss 0.32256659865379333\n",
      "[Training Epoch 0] Batch 2687, Loss 0.29093247652053833\n",
      "[Training Epoch 0] Batch 2688, Loss 0.3212820887565613\n",
      "[Training Epoch 0] Batch 2689, Loss 0.3300760090351105\n",
      "[Training Epoch 0] Batch 2690, Loss 0.33966490626335144\n",
      "[Training Epoch 0] Batch 2691, Loss 0.280352383852005\n",
      "[Training Epoch 0] Batch 2692, Loss 0.3337814211845398\n",
      "[Training Epoch 0] Batch 2693, Loss 0.3106008470058441\n",
      "[Training Epoch 0] Batch 2694, Loss 0.31676971912384033\n",
      "[Training Epoch 0] Batch 2695, Loss 0.3285225033760071\n",
      "[Training Epoch 0] Batch 2696, Loss 0.29360851645469666\n",
      "[Training Epoch 0] Batch 2697, Loss 0.32122349739074707\n",
      "[Training Epoch 0] Batch 2698, Loss 0.28320443630218506\n",
      "[Training Epoch 0] Batch 2699, Loss 0.30233627557754517\n",
      "[Training Epoch 0] Batch 2700, Loss 0.2767811417579651\n",
      "[Training Epoch 0] Batch 2701, Loss 0.3307345509529114\n",
      "[Training Epoch 0] Batch 2702, Loss 0.3467313051223755\n",
      "[Training Epoch 0] Batch 2703, Loss 0.3211141526699066\n",
      "[Training Epoch 0] Batch 2704, Loss 0.30742552876472473\n",
      "[Training Epoch 0] Batch 2705, Loss 0.33479389548301697\n",
      "[Training Epoch 0] Batch 2706, Loss 0.3003026843070984\n",
      "[Training Epoch 0] Batch 2707, Loss 0.30125147104263306\n",
      "[Training Epoch 0] Batch 2708, Loss 0.3436387777328491\n",
      "[Training Epoch 0] Batch 2709, Loss 0.3279687762260437\n",
      "[Training Epoch 0] Batch 2710, Loss 0.330420583486557\n",
      "[Training Epoch 0] Batch 2711, Loss 0.3159247636795044\n",
      "[Training Epoch 0] Batch 2712, Loss 0.31852975487709045\n",
      "[Training Epoch 0] Batch 2713, Loss 0.32304978370666504\n",
      "[Training Epoch 0] Batch 2714, Loss 0.2918664813041687\n",
      "[Training Epoch 0] Batch 2715, Loss 0.3345453441143036\n",
      "[Training Epoch 0] Batch 2716, Loss 0.3174689710140228\n",
      "[Training Epoch 0] Batch 2717, Loss 0.29701870679855347\n",
      "[Training Epoch 0] Batch 2718, Loss 0.27752721309661865\n",
      "[Training Epoch 0] Batch 2719, Loss 0.3112172484397888\n",
      "[Training Epoch 0] Batch 2720, Loss 0.28629910945892334\n",
      "[Training Epoch 0] Batch 2721, Loss 0.3062417805194855\n",
      "[Training Epoch 0] Batch 2722, Loss 0.29868781566619873\n",
      "[Training Epoch 0] Batch 2723, Loss 0.28138643503189087\n",
      "[Training Epoch 0] Batch 2724, Loss 0.3337956964969635\n",
      "[Training Epoch 0] Batch 2725, Loss 0.32918640971183777\n",
      "[Training Epoch 0] Batch 2726, Loss 0.35045385360717773\n",
      "[Training Epoch 0] Batch 2727, Loss 0.3270556330680847\n",
      "[Training Epoch 0] Batch 2728, Loss 0.3056160807609558\n",
      "[Training Epoch 0] Batch 2729, Loss 0.3106856644153595\n",
      "[Training Epoch 0] Batch 2730, Loss 0.31031879782676697\n",
      "[Training Epoch 0] Batch 2731, Loss 0.30372729897499084\n",
      "[Training Epoch 0] Batch 2732, Loss 0.35153770446777344\n",
      "[Training Epoch 0] Batch 2733, Loss 0.3011690378189087\n",
      "[Training Epoch 0] Batch 2734, Loss 0.2997531294822693\n",
      "[Training Epoch 0] Batch 2735, Loss 0.3067190647125244\n",
      "[Training Epoch 0] Batch 2736, Loss 0.3337547481060028\n",
      "[Training Epoch 0] Batch 2737, Loss 0.36693263053894043\n",
      "[Training Epoch 0] Batch 2738, Loss 0.28969985246658325\n",
      "[Training Epoch 0] Batch 2739, Loss 0.3215535283088684\n",
      "[Training Epoch 0] Batch 2740, Loss 0.2953796982765198\n",
      "[Training Epoch 0] Batch 2741, Loss 0.3260890245437622\n",
      "[Training Epoch 0] Batch 2742, Loss 0.29224446415901184\n",
      "[Training Epoch 0] Batch 2743, Loss 0.3195801079273224\n",
      "[Training Epoch 0] Batch 2744, Loss 0.33910971879959106\n",
      "[Training Epoch 0] Batch 2745, Loss 0.29229891300201416\n",
      "[Training Epoch 0] Batch 2746, Loss 0.3283524513244629\n",
      "[Training Epoch 0] Batch 2747, Loss 0.29739370942115784\n",
      "[Training Epoch 0] Batch 2748, Loss 0.3237665891647339\n",
      "[Training Epoch 0] Batch 2749, Loss 0.3278793692588806\n",
      "[Training Epoch 0] Batch 2750, Loss 0.3298775553703308\n",
      "[Training Epoch 0] Batch 2751, Loss 0.333236962556839\n",
      "[Training Epoch 0] Batch 2752, Loss 0.3001113533973694\n",
      "[Training Epoch 0] Batch 2753, Loss 0.27768638730049133\n",
      "[Training Epoch 0] Batch 2754, Loss 0.33396756649017334\n",
      "[Training Epoch 0] Batch 2755, Loss 0.313105970621109\n",
      "[Training Epoch 0] Batch 2756, Loss 0.32604825496673584\n",
      "[Training Epoch 0] Batch 2757, Loss 0.33801600337028503\n",
      "[Training Epoch 0] Batch 2758, Loss 0.32147878408432007\n",
      "[Training Epoch 0] Batch 2759, Loss 0.30911770462989807\n",
      "[Training Epoch 0] Batch 2760, Loss 0.3195410966873169\n",
      "[Training Epoch 0] Batch 2761, Loss 0.3193344175815582\n",
      "[Training Epoch 0] Batch 2762, Loss 0.32334890961647034\n",
      "[Training Epoch 0] Batch 2763, Loss 0.30798986554145813\n",
      "[Training Epoch 0] Batch 2764, Loss 0.30764636397361755\n",
      "[Training Epoch 0] Batch 2765, Loss 0.3282880187034607\n",
      "[Training Epoch 0] Batch 2766, Loss 0.31314098834991455\n",
      "[Training Epoch 0] Batch 2767, Loss 0.2858344316482544\n",
      "[Training Epoch 0] Batch 2768, Loss 0.3277415931224823\n",
      "[Training Epoch 0] Batch 2769, Loss 0.34378719329833984\n",
      "[Training Epoch 0] Batch 2770, Loss 0.3106776475906372\n",
      "[Training Epoch 0] Batch 2771, Loss 0.32885318994522095\n",
      "[Training Epoch 0] Batch 2772, Loss 0.3292429745197296\n",
      "[Training Epoch 0] Batch 2773, Loss 0.3084481358528137\n",
      "[Training Epoch 0] Batch 2774, Loss 0.33083274960517883\n",
      "[Training Epoch 0] Batch 2775, Loss 0.3234429955482483\n",
      "[Training Epoch 0] Batch 2776, Loss 0.33229386806488037\n",
      "[Training Epoch 0] Batch 2777, Loss 0.31047162413597107\n",
      "[Training Epoch 0] Batch 2778, Loss 0.3279300332069397\n",
      "[Training Epoch 0] Batch 2779, Loss 0.3141418695449829\n",
      "[Training Epoch 0] Batch 2780, Loss 0.32726603746414185\n",
      "[Training Epoch 0] Batch 2781, Loss 0.32037144899368286\n",
      "[Training Epoch 0] Batch 2782, Loss 0.3075091540813446\n",
      "[Training Epoch 0] Batch 2783, Loss 0.33678069710731506\n",
      "[Training Epoch 0] Batch 2784, Loss 0.31170207262039185\n",
      "[Training Epoch 0] Batch 2785, Loss 0.31233304738998413\n",
      "[Training Epoch 0] Batch 2786, Loss 0.3154180645942688\n",
      "[Training Epoch 0] Batch 2787, Loss 0.3144477903842926\n",
      "[Training Epoch 0] Batch 2788, Loss 0.3238809108734131\n",
      "[Training Epoch 0] Batch 2789, Loss 0.31423866748809814\n",
      "[Training Epoch 0] Batch 2790, Loss 0.30937379598617554\n",
      "[Training Epoch 0] Batch 2791, Loss 0.3192444145679474\n",
      "[Training Epoch 0] Batch 2792, Loss 0.28001758456230164\n",
      "[Training Epoch 0] Batch 2793, Loss 0.3220076262950897\n",
      "[Training Epoch 0] Batch 2794, Loss 0.3053476810455322\n",
      "[Training Epoch 0] Batch 2795, Loss 0.33716508746147156\n",
      "[Training Epoch 0] Batch 2796, Loss 0.3219410181045532\n",
      "[Training Epoch 0] Batch 2797, Loss 0.3237863779067993\n",
      "[Training Epoch 0] Batch 2798, Loss 0.28363293409347534\n",
      "[Training Epoch 0] Batch 2799, Loss 0.34124961495399475\n",
      "[Training Epoch 0] Batch 2800, Loss 0.2912488877773285\n",
      "[Training Epoch 0] Batch 2801, Loss 0.32262271642684937\n",
      "[Training Epoch 0] Batch 2802, Loss 0.34048816561698914\n",
      "[Training Epoch 0] Batch 2803, Loss 0.308099627494812\n",
      "[Training Epoch 0] Batch 2804, Loss 0.33266955614089966\n",
      "[Training Epoch 0] Batch 2805, Loss 0.30233094096183777\n",
      "[Training Epoch 0] Batch 2806, Loss 0.32140421867370605\n",
      "[Training Epoch 0] Batch 2807, Loss 0.2960274815559387\n",
      "[Training Epoch 0] Batch 2808, Loss 0.32312023639678955\n",
      "[Training Epoch 0] Batch 2809, Loss 0.3034932017326355\n",
      "[Training Epoch 0] Batch 2810, Loss 0.30645397305488586\n",
      "[Training Epoch 0] Batch 2811, Loss 0.28126490116119385\n",
      "[Training Epoch 0] Batch 2812, Loss 0.2967853546142578\n",
      "[Training Epoch 0] Batch 2813, Loss 0.3456103205680847\n",
      "[Training Epoch 0] Batch 2814, Loss 0.3431953489780426\n",
      "[Training Epoch 0] Batch 2815, Loss 0.29845118522644043\n",
      "[Training Epoch 0] Batch 2816, Loss 0.3081647753715515\n",
      "[Training Epoch 0] Batch 2817, Loss 0.33523431420326233\n",
      "[Training Epoch 0] Batch 2818, Loss 0.29735344648361206\n",
      "[Training Epoch 0] Batch 2819, Loss 0.30933526158332825\n",
      "[Training Epoch 0] Batch 2820, Loss 0.325445294380188\n",
      "[Training Epoch 0] Batch 2821, Loss 0.3369740843772888\n",
      "[Training Epoch 0] Batch 2822, Loss 0.31951606273651123\n",
      "[Training Epoch 0] Batch 2823, Loss 0.3365769386291504\n",
      "[Training Epoch 0] Batch 2824, Loss 0.3202833831310272\n",
      "[Training Epoch 0] Batch 2825, Loss 0.3034381568431854\n",
      "[Training Epoch 0] Batch 2826, Loss 0.3196474015712738\n",
      "[Training Epoch 0] Batch 2827, Loss 0.324705570936203\n",
      "[Training Epoch 0] Batch 2828, Loss 0.3394862711429596\n",
      "[Training Epoch 0] Batch 2829, Loss 0.30346083641052246\n",
      "[Training Epoch 0] Batch 2830, Loss 0.31197288632392883\n",
      "[Training Epoch 0] Batch 2831, Loss 0.31596797704696655\n",
      "[Training Epoch 0] Batch 2832, Loss 0.3176855444908142\n",
      "[Training Epoch 0] Batch 2833, Loss 0.26823487877845764\n",
      "[Training Epoch 0] Batch 2834, Loss 0.28500896692276\n",
      "[Training Epoch 0] Batch 2835, Loss 0.3057902753353119\n",
      "[Training Epoch 0] Batch 2836, Loss 0.29546061158180237\n",
      "[Training Epoch 0] Batch 2837, Loss 0.3221375346183777\n",
      "[Training Epoch 0] Batch 2838, Loss 0.35314375162124634\n",
      "[Training Epoch 0] Batch 2839, Loss 0.3451266288757324\n",
      "[Training Epoch 0] Batch 2840, Loss 0.30915603041648865\n",
      "[Training Epoch 0] Batch 2841, Loss 0.31049442291259766\n",
      "[Training Epoch 0] Batch 2842, Loss 0.33013132214546204\n",
      "[Training Epoch 0] Batch 2843, Loss 0.29924240708351135\n",
      "[Training Epoch 0] Batch 2844, Loss 0.32695290446281433\n",
      "[Training Epoch 0] Batch 2845, Loss 0.32654255628585815\n",
      "[Training Epoch 0] Batch 2846, Loss 0.32589277625083923\n",
      "[Training Epoch 0] Batch 2847, Loss 0.2873968482017517\n",
      "[Training Epoch 0] Batch 2848, Loss 0.31255877017974854\n",
      "[Training Epoch 0] Batch 2849, Loss 0.3149828612804413\n",
      "[Training Epoch 0] Batch 2850, Loss 0.33664992451667786\n",
      "[Training Epoch 0] Batch 2851, Loss 0.3182195723056793\n",
      "[Training Epoch 0] Batch 2852, Loss 0.36064788699150085\n",
      "[Training Epoch 0] Batch 2853, Loss 0.2804136872291565\n",
      "[Training Epoch 0] Batch 2854, Loss 0.3004295825958252\n",
      "[Training Epoch 0] Batch 2855, Loss 0.3007226586341858\n",
      "[Training Epoch 0] Batch 2856, Loss 0.32264524698257446\n",
      "[Training Epoch 0] Batch 2857, Loss 0.3230242133140564\n",
      "[Training Epoch 0] Batch 2858, Loss 0.2776280343532562\n",
      "[Training Epoch 0] Batch 2859, Loss 0.3186873197555542\n",
      "[Training Epoch 0] Batch 2860, Loss 0.3225557804107666\n",
      "[Training Epoch 0] Batch 2861, Loss 0.3048858642578125\n",
      "[Training Epoch 0] Batch 2862, Loss 0.3425992727279663\n",
      "[Training Epoch 0] Batch 2863, Loss 0.33050447702407837\n",
      "[Training Epoch 0] Batch 2864, Loss 0.31816843152046204\n",
      "[Training Epoch 0] Batch 2865, Loss 0.33582979440689087\n",
      "[Training Epoch 0] Batch 2866, Loss 0.3255699574947357\n",
      "[Training Epoch 0] Batch 2867, Loss 0.3141902685165405\n",
      "[Training Epoch 0] Batch 2868, Loss 0.31990528106689453\n",
      "[Training Epoch 0] Batch 2869, Loss 0.31030166149139404\n",
      "[Training Epoch 0] Batch 2870, Loss 0.3583919405937195\n",
      "[Training Epoch 0] Batch 2871, Loss 0.26509374380111694\n",
      "[Training Epoch 0] Batch 2872, Loss 0.2956888675689697\n",
      "[Training Epoch 0] Batch 2873, Loss 0.3191741406917572\n",
      "[Training Epoch 0] Batch 2874, Loss 0.30474910140037537\n",
      "[Training Epoch 0] Batch 2875, Loss 0.29909610748291016\n",
      "[Training Epoch 0] Batch 2876, Loss 0.3074161410331726\n",
      "[Training Epoch 0] Batch 2877, Loss 0.30911532044410706\n",
      "[Training Epoch 0] Batch 2878, Loss 0.28962254524230957\n",
      "[Training Epoch 0] Batch 2879, Loss 0.30720269680023193\n",
      "[Training Epoch 0] Batch 2880, Loss 0.33775684237480164\n",
      "[Training Epoch 0] Batch 2881, Loss 0.31744661927223206\n",
      "[Training Epoch 0] Batch 2882, Loss 0.30458956956863403\n",
      "[Training Epoch 0] Batch 2883, Loss 0.2937467396259308\n",
      "[Training Epoch 0] Batch 2884, Loss 0.3029993772506714\n",
      "[Training Epoch 0] Batch 2885, Loss 0.3135094940662384\n",
      "[Training Epoch 0] Batch 2886, Loss 0.2963559329509735\n",
      "[Training Epoch 0] Batch 2887, Loss 0.3178589940071106\n",
      "[Training Epoch 0] Batch 2888, Loss 0.31230565905570984\n",
      "[Training Epoch 0] Batch 2889, Loss 0.29789257049560547\n",
      "[Training Epoch 0] Batch 2890, Loss 0.30395281314849854\n",
      "[Training Epoch 0] Batch 2891, Loss 0.3067850172519684\n",
      "[Training Epoch 0] Batch 2892, Loss 0.31134891510009766\n",
      "[Training Epoch 0] Batch 2893, Loss 0.3218163847923279\n",
      "[Training Epoch 0] Batch 2894, Loss 0.30761009454727173\n",
      "[Training Epoch 0] Batch 2895, Loss 0.31296175718307495\n",
      "[Training Epoch 0] Batch 2896, Loss 0.31708991527557373\n",
      "[Training Epoch 0] Batch 2897, Loss 0.33885422348976135\n",
      "[Training Epoch 0] Batch 2898, Loss 0.3399457037448883\n",
      "[Training Epoch 0] Batch 2899, Loss 0.31750863790512085\n",
      "[Training Epoch 0] Batch 2900, Loss 0.30560022592544556\n",
      "[Training Epoch 0] Batch 2901, Loss 0.3316408693790436\n",
      "[Training Epoch 0] Batch 2902, Loss 0.3207549452781677\n",
      "[Training Epoch 0] Batch 2903, Loss 0.3074214458465576\n",
      "[Training Epoch 0] Batch 2904, Loss 0.2970348000526428\n",
      "[Training Epoch 0] Batch 2905, Loss 0.31298166513442993\n",
      "[Training Epoch 0] Batch 2906, Loss 0.316192090511322\n",
      "[Training Epoch 0] Batch 2907, Loss 0.29269900918006897\n",
      "[Training Epoch 0] Batch 2908, Loss 0.3187065124511719\n",
      "[Training Epoch 0] Batch 2909, Loss 0.30532145500183105\n",
      "[Training Epoch 0] Batch 2910, Loss 0.32162758708000183\n",
      "[Training Epoch 0] Batch 2911, Loss 0.3465365767478943\n",
      "[Training Epoch 0] Batch 2912, Loss 0.32175496220588684\n",
      "[Training Epoch 0] Batch 2913, Loss 0.3069739043712616\n",
      "[Training Epoch 0] Batch 2914, Loss 0.2783608138561249\n",
      "[Training Epoch 0] Batch 2915, Loss 0.31253957748413086\n",
      "[Training Epoch 0] Batch 2916, Loss 0.2851254343986511\n",
      "[Training Epoch 0] Batch 2917, Loss 0.33293282985687256\n",
      "[Training Epoch 0] Batch 2918, Loss 0.3044891059398651\n",
      "[Training Epoch 0] Batch 2919, Loss 0.3431938588619232\n",
      "[Training Epoch 0] Batch 2920, Loss 0.3044811487197876\n",
      "[Training Epoch 0] Batch 2921, Loss 0.29757553339004517\n",
      "[Training Epoch 0] Batch 2922, Loss 0.2990362346172333\n",
      "[Training Epoch 0] Batch 2923, Loss 0.32267993688583374\n",
      "[Training Epoch 0] Batch 2924, Loss 0.29492753744125366\n",
      "[Training Epoch 0] Batch 2925, Loss 0.29493793845176697\n",
      "[Training Epoch 0] Batch 2926, Loss 0.29511553049087524\n",
      "[Training Epoch 0] Batch 2927, Loss 0.2907153367996216\n",
      "[Training Epoch 0] Batch 2928, Loss 0.31738895177841187\n",
      "[Training Epoch 0] Batch 2929, Loss 0.30330967903137207\n",
      "[Training Epoch 0] Batch 2930, Loss 0.3077769875526428\n",
      "[Training Epoch 0] Batch 2931, Loss 0.2906056344509125\n",
      "[Training Epoch 0] Batch 2932, Loss 0.3391275405883789\n",
      "[Training Epoch 0] Batch 2933, Loss 0.3238877058029175\n",
      "[Training Epoch 0] Batch 2934, Loss 0.3366810381412506\n",
      "[Training Epoch 0] Batch 2935, Loss 0.34200936555862427\n",
      "[Training Epoch 0] Batch 2936, Loss 0.2956005930900574\n",
      "[Training Epoch 0] Batch 2937, Loss 0.28072983026504517\n",
      "[Training Epoch 0] Batch 2938, Loss 0.3001821041107178\n",
      "[Training Epoch 0] Batch 2939, Loss 0.3393676280975342\n",
      "[Training Epoch 0] Batch 2940, Loss 0.317640483379364\n",
      "[Training Epoch 0] Batch 2941, Loss 0.2957722842693329\n",
      "[Training Epoch 0] Batch 2942, Loss 0.3251112699508667\n",
      "[Training Epoch 0] Batch 2943, Loss 0.3276897370815277\n",
      "[Training Epoch 0] Batch 2944, Loss 0.3091951310634613\n",
      "[Training Epoch 0] Batch 2945, Loss 0.323049932718277\n",
      "[Training Epoch 0] Batch 2946, Loss 0.3047672212123871\n",
      "[Training Epoch 0] Batch 2947, Loss 0.3111362159252167\n",
      "[Training Epoch 0] Batch 2948, Loss 0.31711873412132263\n",
      "[Training Epoch 0] Batch 2949, Loss 0.29609236121177673\n",
      "[Training Epoch 0] Batch 2950, Loss 0.3209241032600403\n",
      "[Training Epoch 0] Batch 2951, Loss 0.29833918809890747\n",
      "[Training Epoch 0] Batch 2952, Loss 0.35228413343429565\n",
      "[Training Epoch 0] Batch 2953, Loss 0.3291166424751282\n",
      "[Training Epoch 0] Batch 2954, Loss 0.3501652181148529\n",
      "[Training Epoch 0] Batch 2955, Loss 0.2977462708950043\n",
      "[Training Epoch 0] Batch 2956, Loss 0.32730966806411743\n",
      "[Training Epoch 0] Batch 2957, Loss 0.3304934799671173\n",
      "[Training Epoch 0] Batch 2958, Loss 0.341273695230484\n",
      "[Training Epoch 0] Batch 2959, Loss 0.29531484842300415\n",
      "[Training Epoch 0] Batch 2960, Loss 0.29214462637901306\n",
      "[Training Epoch 0] Batch 2961, Loss 0.29208311438560486\n",
      "[Training Epoch 0] Batch 2962, Loss 0.32617953419685364\n",
      "[Training Epoch 0] Batch 2963, Loss 0.3048873543739319\n",
      "[Training Epoch 0] Batch 2964, Loss 0.2890370190143585\n",
      "[Training Epoch 0] Batch 2965, Loss 0.33387085795402527\n",
      "[Training Epoch 0] Batch 2966, Loss 0.2955487370491028\n",
      "[Training Epoch 0] Batch 2967, Loss 0.3095133602619171\n",
      "[Training Epoch 0] Batch 2968, Loss 0.3387736976146698\n",
      "[Training Epoch 0] Batch 2969, Loss 0.31944939494132996\n",
      "[Training Epoch 0] Batch 2970, Loss 0.33856526017189026\n",
      "[Training Epoch 0] Batch 2971, Loss 0.3081636130809784\n",
      "[Training Epoch 0] Batch 2972, Loss 0.3306213915348053\n",
      "[Training Epoch 0] Batch 2973, Loss 0.30945706367492676\n",
      "[Training Epoch 0] Batch 2974, Loss 0.34162673354148865\n",
      "[Training Epoch 0] Batch 2975, Loss 0.3328233063220978\n",
      "[Training Epoch 0] Batch 2976, Loss 0.3429446220397949\n",
      "[Training Epoch 0] Batch 2977, Loss 0.3165716528892517\n",
      "[Training Epoch 0] Batch 2978, Loss 0.32659268379211426\n",
      "[Training Epoch 0] Batch 2979, Loss 0.2835785150527954\n",
      "[Training Epoch 0] Batch 2980, Loss 0.2859848439693451\n",
      "[Training Epoch 0] Batch 2981, Loss 0.3133992552757263\n",
      "[Training Epoch 0] Batch 2982, Loss 0.33652451634407043\n",
      "[Training Epoch 0] Batch 2983, Loss 0.3121602535247803\n",
      "[Training Epoch 0] Batch 2984, Loss 0.3175159692764282\n",
      "[Training Epoch 0] Batch 2985, Loss 0.3220727741718292\n",
      "[Training Epoch 0] Batch 2986, Loss 0.2988685965538025\n",
      "[Training Epoch 0] Batch 2987, Loss 0.3409872353076935\n",
      "[Training Epoch 0] Batch 2988, Loss 0.3328244686126709\n",
      "[Training Epoch 0] Batch 2989, Loss 0.31169766187667847\n",
      "[Training Epoch 0] Batch 2990, Loss 0.3167075216770172\n",
      "[Training Epoch 0] Batch 2991, Loss 0.3180832266807556\n",
      "[Training Epoch 0] Batch 2992, Loss 0.3177429437637329\n",
      "[Training Epoch 0] Batch 2993, Loss 0.29256972670555115\n",
      "[Training Epoch 0] Batch 2994, Loss 0.36617955565452576\n",
      "[Training Epoch 0] Batch 2995, Loss 0.30101191997528076\n",
      "[Training Epoch 0] Batch 2996, Loss 0.3076353073120117\n",
      "[Training Epoch 0] Batch 2997, Loss 0.2991321086883545\n",
      "[Training Epoch 0] Batch 2998, Loss 0.32003289461135864\n",
      "[Training Epoch 0] Batch 2999, Loss 0.3103882968425751\n",
      "[Training Epoch 0] Batch 3000, Loss 0.27541986107826233\n",
      "[Training Epoch 0] Batch 3001, Loss 0.3668212890625\n",
      "[Training Epoch 0] Batch 3002, Loss 0.3168327212333679\n",
      "[Training Epoch 0] Batch 3003, Loss 0.3005721867084503\n",
      "[Training Epoch 0] Batch 3004, Loss 0.3241384029388428\n",
      "[Training Epoch 0] Batch 3005, Loss 0.3262396454811096\n",
      "[Training Epoch 0] Batch 3006, Loss 0.3087879419326782\n",
      "[Training Epoch 0] Batch 3007, Loss 0.33946657180786133\n",
      "[Training Epoch 0] Batch 3008, Loss 0.30070042610168457\n",
      "[Training Epoch 0] Batch 3009, Loss 0.32244935631752014\n",
      "[Training Epoch 0] Batch 3010, Loss 0.33794063329696655\n",
      "[Training Epoch 0] Batch 3011, Loss 0.32235777378082275\n",
      "[Training Epoch 0] Batch 3012, Loss 0.2947750687599182\n",
      "[Training Epoch 0] Batch 3013, Loss 0.30241042375564575\n",
      "[Training Epoch 0] Batch 3014, Loss 0.29008758068084717\n",
      "[Training Epoch 0] Batch 3015, Loss 0.2780319154262543\n",
      "[Training Epoch 0] Batch 3016, Loss 0.3186662197113037\n",
      "[Training Epoch 0] Batch 3017, Loss 0.3371373414993286\n",
      "[Training Epoch 0] Batch 3018, Loss 0.307009756565094\n",
      "[Training Epoch 0] Batch 3019, Loss 0.3385589122772217\n",
      "[Training Epoch 0] Batch 3020, Loss 0.31969818472862244\n",
      "[Training Epoch 0] Batch 3021, Loss 0.3090730607509613\n",
      "[Training Epoch 0] Batch 3022, Loss 0.33007028698921204\n",
      "[Training Epoch 0] Batch 3023, Loss 0.30266672372817993\n",
      "[Training Epoch 0] Batch 3024, Loss 0.31956639885902405\n",
      "[Training Epoch 0] Batch 3025, Loss 0.31570619344711304\n",
      "[Training Epoch 0] Batch 3026, Loss 0.3071368932723999\n",
      "[Training Epoch 0] Batch 3027, Loss 0.30748647451400757\n",
      "[Training Epoch 0] Batch 3028, Loss 0.3228936195373535\n",
      "[Training Epoch 0] Batch 3029, Loss 0.2992989420890808\n",
      "[Training Epoch 0] Batch 3030, Loss 0.29381006956100464\n",
      "[Training Epoch 0] Batch 3031, Loss 0.34570860862731934\n",
      "[Training Epoch 0] Batch 3032, Loss 0.318742960691452\n",
      "[Training Epoch 0] Batch 3033, Loss 0.31895196437835693\n",
      "[Training Epoch 0] Batch 3034, Loss 0.3354826867580414\n",
      "[Training Epoch 0] Batch 3035, Loss 0.30975133180618286\n",
      "[Training Epoch 0] Batch 3036, Loss 0.29413333535194397\n",
      "[Training Epoch 0] Batch 3037, Loss 0.3273043930530548\n",
      "[Training Epoch 0] Batch 3038, Loss 0.30285099148750305\n",
      "[Training Epoch 0] Batch 3039, Loss 0.3216545581817627\n",
      "[Training Epoch 0] Batch 3040, Loss 0.31912603974342346\n",
      "[Training Epoch 0] Batch 3041, Loss 0.3273758590221405\n",
      "[Training Epoch 0] Batch 3042, Loss 0.3027285933494568\n",
      "[Training Epoch 0] Batch 3043, Loss 0.3136986196041107\n",
      "[Training Epoch 0] Batch 3044, Loss 0.3098573684692383\n",
      "[Training Epoch 0] Batch 3045, Loss 0.2915572226047516\n",
      "[Training Epoch 0] Batch 3046, Loss 0.2748129069805145\n",
      "[Training Epoch 0] Batch 3047, Loss 0.2970429062843323\n",
      "[Training Epoch 0] Batch 3048, Loss 0.3001237213611603\n",
      "[Training Epoch 0] Batch 3049, Loss 0.2961765229701996\n",
      "[Training Epoch 0] Batch 3050, Loss 0.3307221233844757\n",
      "[Training Epoch 0] Batch 3051, Loss 0.31293168663978577\n",
      "[Training Epoch 0] Batch 3052, Loss 0.329708993434906\n",
      "[Training Epoch 0] Batch 3053, Loss 0.3163057565689087\n",
      "[Training Epoch 0] Batch 3054, Loss 0.331757515668869\n",
      "[Training Epoch 0] Batch 3055, Loss 0.28830212354660034\n",
      "[Training Epoch 0] Batch 3056, Loss 0.3427309989929199\n",
      "[Training Epoch 0] Batch 3057, Loss 0.29652589559555054\n",
      "[Training Epoch 0] Batch 3058, Loss 0.3082651197910309\n",
      "[Training Epoch 0] Batch 3059, Loss 0.30094799399375916\n",
      "[Training Epoch 0] Batch 3060, Loss 0.28589123487472534\n",
      "[Training Epoch 0] Batch 3061, Loss 0.3017634451389313\n",
      "[Training Epoch 0] Batch 3062, Loss 0.3392578363418579\n",
      "[Training Epoch 0] Batch 3063, Loss 0.3183833062648773\n",
      "[Training Epoch 0] Batch 3064, Loss 0.29814857244491577\n",
      "[Training Epoch 0] Batch 3065, Loss 0.3163887560367584\n",
      "[Training Epoch 0] Batch 3066, Loss 0.2960050702095032\n",
      "[Training Epoch 0] Batch 3067, Loss 0.3775143325328827\n",
      "[Training Epoch 0] Batch 3068, Loss 0.3133770525455475\n",
      "[Training Epoch 0] Batch 3069, Loss 0.31655940413475037\n",
      "[Training Epoch 0] Batch 3070, Loss 0.3155132234096527\n",
      "[Training Epoch 0] Batch 3071, Loss 0.2912771999835968\n",
      "[Training Epoch 0] Batch 3072, Loss 0.28406044840812683\n",
      "[Training Epoch 0] Batch 3073, Loss 0.29154515266418457\n",
      "[Training Epoch 0] Batch 3074, Loss 0.33331823348999023\n",
      "[Training Epoch 0] Batch 3075, Loss 0.34049907326698303\n",
      "[Training Epoch 0] Batch 3076, Loss 0.2809932231903076\n",
      "[Training Epoch 0] Batch 3077, Loss 0.3096637725830078\n",
      "[Training Epoch 0] Batch 3078, Loss 0.3238123655319214\n",
      "[Training Epoch 0] Batch 3079, Loss 0.3315272033214569\n",
      "[Training Epoch 0] Batch 3080, Loss 0.29284313321113586\n",
      "[Training Epoch 0] Batch 3081, Loss 0.30688685178756714\n",
      "[Training Epoch 0] Batch 3082, Loss 0.3128041625022888\n",
      "[Training Epoch 0] Batch 3083, Loss 0.28480589389801025\n",
      "[Training Epoch 0] Batch 3084, Loss 0.31255871057510376\n",
      "[Training Epoch 0] Batch 3085, Loss 0.3109789490699768\n",
      "[Training Epoch 0] Batch 3086, Loss 0.3021206259727478\n",
      "[Training Epoch 0] Batch 3087, Loss 0.31318747997283936\n",
      "[Training Epoch 0] Batch 3088, Loss 0.3339326083660126\n",
      "[Training Epoch 0] Batch 3089, Loss 0.3053237199783325\n",
      "[Training Epoch 0] Batch 3090, Loss 0.32328760623931885\n",
      "[Training Epoch 0] Batch 3091, Loss 0.30027151107788086\n",
      "[Training Epoch 0] Batch 3092, Loss 0.33118563890457153\n",
      "[Training Epoch 0] Batch 3093, Loss 0.29246044158935547\n",
      "[Training Epoch 0] Batch 3094, Loss 0.3197743892669678\n",
      "[Training Epoch 0] Batch 3095, Loss 0.2995580732822418\n",
      "[Training Epoch 0] Batch 3096, Loss 0.31405943632125854\n",
      "[Training Epoch 0] Batch 3097, Loss 0.3182080388069153\n",
      "[Training Epoch 0] Batch 3098, Loss 0.3118157386779785\n",
      "[Training Epoch 0] Batch 3099, Loss 0.2994590699672699\n",
      "[Training Epoch 0] Batch 3100, Loss 0.3209179937839508\n",
      "[Training Epoch 0] Batch 3101, Loss 0.3312295079231262\n",
      "[Training Epoch 0] Batch 3102, Loss 0.3166199028491974\n",
      "[Training Epoch 0] Batch 3103, Loss 0.2983430325984955\n",
      "[Training Epoch 0] Batch 3104, Loss 0.34206658601760864\n",
      "[Training Epoch 0] Batch 3105, Loss 0.3349250555038452\n",
      "[Training Epoch 0] Batch 3106, Loss 0.3200657069683075\n",
      "[Training Epoch 0] Batch 3107, Loss 0.33028966188430786\n",
      "[Training Epoch 0] Batch 3108, Loss 0.27998849749565125\n",
      "[Training Epoch 0] Batch 3109, Loss 0.33211958408355713\n",
      "[Training Epoch 0] Batch 3110, Loss 0.3432677090167999\n",
      "[Training Epoch 0] Batch 3111, Loss 0.3342135548591614\n",
      "[Training Epoch 0] Batch 3112, Loss 0.296116441488266\n",
      "[Training Epoch 0] Batch 3113, Loss 0.29513365030288696\n",
      "[Training Epoch 0] Batch 3114, Loss 0.29446470737457275\n",
      "[Training Epoch 0] Batch 3115, Loss 0.3157702088356018\n",
      "[Training Epoch 0] Batch 3116, Loss 0.30187979340553284\n",
      "[Training Epoch 0] Batch 3117, Loss 0.31170663237571716\n",
      "[Training Epoch 0] Batch 3118, Loss 0.33371102809906006\n",
      "[Training Epoch 0] Batch 3119, Loss 0.2888588011264801\n",
      "[Training Epoch 0] Batch 3120, Loss 0.3039531111717224\n",
      "[Training Epoch 0] Batch 3121, Loss 0.34106379747390747\n",
      "[Training Epoch 0] Batch 3122, Loss 0.3168664574623108\n",
      "[Training Epoch 0] Batch 3123, Loss 0.2994265556335449\n",
      "[Training Epoch 0] Batch 3124, Loss 0.3195208013057709\n",
      "[Training Epoch 0] Batch 3125, Loss 0.30906349420547485\n",
      "[Training Epoch 0] Batch 3126, Loss 0.3214453160762787\n",
      "[Training Epoch 0] Batch 3127, Loss 0.3152104318141937\n",
      "[Training Epoch 0] Batch 3128, Loss 0.32853683829307556\n",
      "[Training Epoch 0] Batch 3129, Loss 0.30550417304039\n",
      "[Training Epoch 0] Batch 3130, Loss 0.3118715286254883\n",
      "[Training Epoch 0] Batch 3131, Loss 0.30397486686706543\n",
      "[Training Epoch 0] Batch 3132, Loss 0.3134182393550873\n",
      "[Training Epoch 0] Batch 3133, Loss 0.2853167653083801\n",
      "[Training Epoch 0] Batch 3134, Loss 0.2886609435081482\n",
      "[Training Epoch 0] Batch 3135, Loss 0.31927287578582764\n",
      "[Training Epoch 0] Batch 3136, Loss 0.33354347944259644\n",
      "[Training Epoch 0] Batch 3137, Loss 0.299801230430603\n",
      "[Training Epoch 0] Batch 3138, Loss 0.3277927339076996\n",
      "[Training Epoch 0] Batch 3139, Loss 0.3237966299057007\n",
      "[Training Epoch 0] Batch 3140, Loss 0.3353368937969208\n",
      "[Training Epoch 0] Batch 3141, Loss 0.3076021373271942\n",
      "[Training Epoch 0] Batch 3142, Loss 0.310658723115921\n",
      "[Training Epoch 0] Batch 3143, Loss 0.3395480215549469\n",
      "[Training Epoch 0] Batch 3144, Loss 0.32341518998146057\n",
      "[Training Epoch 0] Batch 3145, Loss 0.32366234064102173\n",
      "[Training Epoch 0] Batch 3146, Loss 0.334350049495697\n",
      "[Training Epoch 0] Batch 3147, Loss 0.3156472444534302\n",
      "[Training Epoch 0] Batch 3148, Loss 0.29362261295318604\n",
      "[Training Epoch 0] Batch 3149, Loss 0.3136153519153595\n",
      "[Training Epoch 0] Batch 3150, Loss 0.3292050063610077\n",
      "[Training Epoch 0] Batch 3151, Loss 0.33088815212249756\n",
      "[Training Epoch 0] Batch 3152, Loss 0.30782732367515564\n",
      "[Training Epoch 0] Batch 3153, Loss 0.3191092610359192\n",
      "[Training Epoch 0] Batch 3154, Loss 0.32147881388664246\n",
      "[Training Epoch 0] Batch 3155, Loss 0.3074713945388794\n",
      "[Training Epoch 0] Batch 3156, Loss 0.304756224155426\n",
      "[Training Epoch 0] Batch 3157, Loss 0.3305225074291229\n",
      "[Training Epoch 0] Batch 3158, Loss 0.3659493327140808\n",
      "[Training Epoch 0] Batch 3159, Loss 0.31750252842903137\n",
      "[Training Epoch 0] Batch 3160, Loss 0.3007963001728058\n",
      "[Training Epoch 0] Batch 3161, Loss 0.32871124148368835\n",
      "[Training Epoch 0] Batch 3162, Loss 0.3295241594314575\n",
      "[Training Epoch 0] Batch 3163, Loss 0.30885735154151917\n",
      "[Training Epoch 0] Batch 3164, Loss 0.2675403654575348\n",
      "[Training Epoch 0] Batch 3165, Loss 0.3148152530193329\n",
      "[Training Epoch 0] Batch 3166, Loss 0.330876886844635\n",
      "[Training Epoch 0] Batch 3167, Loss 0.30087652802467346\n",
      "[Training Epoch 0] Batch 3168, Loss 0.306789368391037\n",
      "[Training Epoch 0] Batch 3169, Loss 0.3121999204158783\n",
      "[Training Epoch 0] Batch 3170, Loss 0.2991677224636078\n",
      "[Training Epoch 0] Batch 3171, Loss 0.32408320903778076\n",
      "[Training Epoch 0] Batch 3172, Loss 0.32228752970695496\n",
      "[Training Epoch 0] Batch 3173, Loss 0.3133583962917328\n",
      "[Training Epoch 0] Batch 3174, Loss 0.3519880175590515\n",
      "[Training Epoch 0] Batch 3175, Loss 0.31757429242134094\n",
      "[Training Epoch 0] Batch 3176, Loss 0.2988426983356476\n",
      "[Training Epoch 0] Batch 3177, Loss 0.3207314610481262\n",
      "[Training Epoch 0] Batch 3178, Loss 0.32086628675460815\n",
      "[Training Epoch 0] Batch 3179, Loss 0.2948533296585083\n",
      "[Training Epoch 0] Batch 3180, Loss 0.3217327892780304\n",
      "[Training Epoch 0] Batch 3181, Loss 0.2772881090641022\n",
      "[Training Epoch 0] Batch 3182, Loss 0.34085047245025635\n",
      "[Training Epoch 0] Batch 3183, Loss 0.30693456530570984\n",
      "[Training Epoch 0] Batch 3184, Loss 0.3103203773498535\n",
      "[Training Epoch 0] Batch 3185, Loss 0.3470154404640198\n",
      "[Training Epoch 0] Batch 3186, Loss 0.3344942033290863\n",
      "[Training Epoch 0] Batch 3187, Loss 0.28728562593460083\n",
      "[Training Epoch 0] Batch 3188, Loss 0.31455984711647034\n",
      "[Training Epoch 0] Batch 3189, Loss 0.3045329749584198\n",
      "[Training Epoch 0] Batch 3190, Loss 0.3125046491622925\n",
      "[Training Epoch 0] Batch 3191, Loss 0.33612194657325745\n",
      "[Training Epoch 0] Batch 3192, Loss 0.31329968571662903\n",
      "[Training Epoch 0] Batch 3193, Loss 0.291830837726593\n",
      "[Training Epoch 0] Batch 3194, Loss 0.31130656599998474\n",
      "[Training Epoch 0] Batch 3195, Loss 0.33791112899780273\n",
      "[Training Epoch 0] Batch 3196, Loss 0.32479196786880493\n",
      "[Training Epoch 0] Batch 3197, Loss 0.2960343658924103\n",
      "[Training Epoch 0] Batch 3198, Loss 0.31851720809936523\n",
      "[Training Epoch 0] Batch 3199, Loss 0.3436053693294525\n",
      "[Training Epoch 0] Batch 3200, Loss 0.3012736439704895\n",
      "[Training Epoch 0] Batch 3201, Loss 0.31522899866104126\n",
      "[Training Epoch 0] Batch 3202, Loss 0.32331329584121704\n",
      "[Training Epoch 0] Batch 3203, Loss 0.292347252368927\n",
      "[Training Epoch 0] Batch 3204, Loss 0.3299957513809204\n",
      "[Training Epoch 0] Batch 3205, Loss 0.31731438636779785\n",
      "[Training Epoch 0] Batch 3206, Loss 0.2863970994949341\n",
      "[Training Epoch 0] Batch 3207, Loss 0.3098749816417694\n",
      "[Training Epoch 0] Batch 3208, Loss 0.3096083104610443\n",
      "[Training Epoch 0] Batch 3209, Loss 0.3227778971195221\n",
      "[Training Epoch 0] Batch 3210, Loss 0.3169977366924286\n",
      "[Training Epoch 0] Batch 3211, Loss 0.3188866972923279\n",
      "[Training Epoch 0] Batch 3212, Loss 0.2849128544330597\n",
      "[Training Epoch 0] Batch 3213, Loss 0.3275260627269745\n",
      "[Training Epoch 0] Batch 3214, Loss 0.2972640097141266\n",
      "[Training Epoch 0] Batch 3215, Loss 0.3031226396560669\n",
      "[Training Epoch 0] Batch 3216, Loss 0.29911327362060547\n",
      "[Training Epoch 0] Batch 3217, Loss 0.2751809358596802\n",
      "[Training Epoch 0] Batch 3218, Loss 0.33758217096328735\n",
      "[Training Epoch 0] Batch 3219, Loss 0.31447795033454895\n",
      "[Training Epoch 0] Batch 3220, Loss 0.3344308137893677\n",
      "[Training Epoch 0] Batch 3221, Loss 0.30647820234298706\n",
      "[Training Epoch 0] Batch 3222, Loss 0.2976292073726654\n",
      "[Training Epoch 0] Batch 3223, Loss 0.31752490997314453\n",
      "[Training Epoch 0] Batch 3224, Loss 0.33885136246681213\n",
      "[Training Epoch 0] Batch 3225, Loss 0.2989761531352997\n",
      "[Training Epoch 0] Batch 3226, Loss 0.33520668745040894\n",
      "[Training Epoch 0] Batch 3227, Loss 0.3277008533477783\n",
      "[Training Epoch 0] Batch 3228, Loss 0.306101530790329\n",
      "[Training Epoch 0] Batch 3229, Loss 0.34147870540618896\n",
      "[Training Epoch 0] Batch 3230, Loss 0.33397018909454346\n",
      "[Training Epoch 0] Batch 3231, Loss 0.33776021003723145\n",
      "[Training Epoch 0] Batch 3232, Loss 0.3510493040084839\n",
      "[Training Epoch 0] Batch 3233, Loss 0.32992634177207947\n",
      "[Training Epoch 0] Batch 3234, Loss 0.31423020362854004\n",
      "[Training Epoch 0] Batch 3235, Loss 0.33317670226097107\n",
      "[Training Epoch 0] Batch 3236, Loss 0.3197709918022156\n",
      "[Training Epoch 0] Batch 3237, Loss 0.28088808059692383\n",
      "[Training Epoch 0] Batch 3238, Loss 0.3300301730632782\n",
      "[Training Epoch 0] Batch 3239, Loss 0.3171609044075012\n",
      "[Training Epoch 0] Batch 3240, Loss 0.29772698879241943\n",
      "[Training Epoch 0] Batch 3241, Loss 0.2969520092010498\n",
      "[Training Epoch 0] Batch 3242, Loss 0.25940802693367004\n",
      "[Training Epoch 0] Batch 3243, Loss 0.3075712025165558\n",
      "[Training Epoch 0] Batch 3244, Loss 0.3186795115470886\n",
      "[Training Epoch 0] Batch 3245, Loss 0.2994444966316223\n",
      "[Training Epoch 0] Batch 3246, Loss 0.3122085630893707\n",
      "[Training Epoch 0] Batch 3247, Loss 0.2984596788883209\n",
      "[Training Epoch 0] Batch 3248, Loss 0.2812120020389557\n",
      "[Training Epoch 0] Batch 3249, Loss 0.2985355257987976\n",
      "[Training Epoch 0] Batch 3250, Loss 0.3123854696750641\n",
      "[Training Epoch 0] Batch 3251, Loss 0.34190165996551514\n",
      "[Training Epoch 0] Batch 3252, Loss 0.3010970950126648\n",
      "[Training Epoch 0] Batch 3253, Loss 0.294007807970047\n",
      "[Training Epoch 0] Batch 3254, Loss 0.31568410992622375\n",
      "[Training Epoch 0] Batch 3255, Loss 0.31973645091056824\n",
      "[Training Epoch 0] Batch 3256, Loss 0.3205588161945343\n",
      "[Training Epoch 0] Batch 3257, Loss 0.30580681562423706\n",
      "[Training Epoch 0] Batch 3258, Loss 0.32238829135894775\n",
      "[Training Epoch 0] Batch 3259, Loss 0.32888779044151306\n",
      "[Training Epoch 0] Batch 3260, Loss 0.31106263399124146\n",
      "[Training Epoch 0] Batch 3261, Loss 0.3404310643672943\n",
      "[Training Epoch 0] Batch 3262, Loss 0.31021764874458313\n",
      "[Training Epoch 0] Batch 3263, Loss 0.3110100030899048\n",
      "[Training Epoch 0] Batch 3264, Loss 0.30892089009284973\n",
      "[Training Epoch 0] Batch 3265, Loss 0.2832973003387451\n",
      "[Training Epoch 0] Batch 3266, Loss 0.3337056040763855\n",
      "[Training Epoch 0] Batch 3267, Loss 0.3240290880203247\n",
      "[Training Epoch 0] Batch 3268, Loss 0.3234438896179199\n",
      "[Training Epoch 0] Batch 3269, Loss 0.3094151020050049\n",
      "[Training Epoch 0] Batch 3270, Loss 0.34612342715263367\n",
      "[Training Epoch 0] Batch 3271, Loss 0.2958395779132843\n",
      "[Training Epoch 0] Batch 3272, Loss 0.31605255603790283\n",
      "[Training Epoch 0] Batch 3273, Loss 0.30043211579322815\n",
      "[Training Epoch 0] Batch 3274, Loss 0.3369351625442505\n",
      "[Training Epoch 0] Batch 3275, Loss 0.307462602853775\n",
      "[Training Epoch 0] Batch 3276, Loss 0.3265915513038635\n",
      "[Training Epoch 0] Batch 3277, Loss 0.3133600652217865\n",
      "[Training Epoch 0] Batch 3278, Loss 0.3107761740684509\n",
      "[Training Epoch 0] Batch 3279, Loss 0.3211241364479065\n",
      "[Training Epoch 0] Batch 3280, Loss 0.3053721487522125\n",
      "[Training Epoch 0] Batch 3281, Loss 0.30473190546035767\n",
      "[Training Epoch 0] Batch 3282, Loss 0.30506110191345215\n",
      "[Training Epoch 0] Batch 3283, Loss 0.2999185025691986\n",
      "[Training Epoch 0] Batch 3284, Loss 0.3473256230354309\n",
      "[Training Epoch 0] Batch 3285, Loss 0.29969561100006104\n",
      "[Training Epoch 0] Batch 3286, Loss 0.31694549322128296\n",
      "[Training Epoch 0] Batch 3287, Loss 0.2936478555202484\n",
      "[Training Epoch 0] Batch 3288, Loss 0.289503812789917\n",
      "[Training Epoch 0] Batch 3289, Loss 0.3153763711452484\n",
      "[Training Epoch 0] Batch 3290, Loss 0.2981075644493103\n",
      "[Training Epoch 0] Batch 3291, Loss 0.3226580023765564\n",
      "[Training Epoch 0] Batch 3292, Loss 0.3441504240036011\n",
      "[Training Epoch 0] Batch 3293, Loss 0.331318199634552\n",
      "[Training Epoch 0] Batch 3294, Loss 0.30217042565345764\n",
      "[Training Epoch 0] Batch 3295, Loss 0.3195194900035858\n",
      "[Training Epoch 0] Batch 3296, Loss 0.3072706460952759\n",
      "[Training Epoch 0] Batch 3297, Loss 0.31914448738098145\n",
      "[Training Epoch 0] Batch 3298, Loss 0.31952008605003357\n",
      "[Training Epoch 0] Batch 3299, Loss 0.28620871901512146\n",
      "[Training Epoch 0] Batch 3300, Loss 0.2850531339645386\n",
      "[Training Epoch 0] Batch 3301, Loss 0.344270795583725\n",
      "[Training Epoch 0] Batch 3302, Loss 0.27828702330589294\n",
      "[Training Epoch 0] Batch 3303, Loss 0.3033295273780823\n",
      "[Training Epoch 0] Batch 3304, Loss 0.3114855885505676\n",
      "[Training Epoch 0] Batch 3305, Loss 0.31585419178009033\n",
      "[Training Epoch 0] Batch 3306, Loss 0.3009808659553528\n",
      "[Training Epoch 0] Batch 3307, Loss 0.30295872688293457\n",
      "[Training Epoch 0] Batch 3308, Loss 0.3194020092487335\n",
      "[Training Epoch 0] Batch 3309, Loss 0.3101150393486023\n",
      "[Training Epoch 0] Batch 3310, Loss 0.32677504420280457\n",
      "[Training Epoch 0] Batch 3311, Loss 0.2822387218475342\n",
      "[Training Epoch 0] Batch 3312, Loss 0.31661084294319153\n",
      "[Training Epoch 0] Batch 3313, Loss 0.3131694793701172\n",
      "[Training Epoch 0] Batch 3314, Loss 0.26730430126190186\n",
      "[Training Epoch 0] Batch 3315, Loss 0.32202863693237305\n",
      "[Training Epoch 0] Batch 3316, Loss 0.3056797385215759\n",
      "[Training Epoch 0] Batch 3317, Loss 0.3181981146335602\n",
      "[Training Epoch 0] Batch 3318, Loss 0.3168698847293854\n",
      "[Training Epoch 0] Batch 3319, Loss 0.3235684037208557\n",
      "[Training Epoch 0] Batch 3320, Loss 0.3135643005371094\n",
      "[Training Epoch 0] Batch 3321, Loss 0.34176090359687805\n",
      "[Training Epoch 0] Batch 3322, Loss 0.3148919343948364\n",
      "[Training Epoch 0] Batch 3323, Loss 0.3165614902973175\n",
      "[Training Epoch 0] Batch 3324, Loss 0.3256029188632965\n",
      "[Training Epoch 0] Batch 3325, Loss 0.3044738471508026\n",
      "[Training Epoch 0] Batch 3326, Loss 0.3177582323551178\n",
      "[Training Epoch 0] Batch 3327, Loss 0.31108933687210083\n",
      "[Training Epoch 0] Batch 3328, Loss 0.32020044326782227\n",
      "[Training Epoch 0] Batch 3329, Loss 0.2970848083496094\n",
      "[Training Epoch 0] Batch 3330, Loss 0.32724279165267944\n",
      "[Training Epoch 0] Batch 3331, Loss 0.3087119162082672\n",
      "[Training Epoch 0] Batch 3332, Loss 0.3573951721191406\n",
      "[Training Epoch 0] Batch 3333, Loss 0.2995572090148926\n",
      "[Training Epoch 0] Batch 3334, Loss 0.31465280055999756\n",
      "[Training Epoch 0] Batch 3335, Loss 0.30601853132247925\n",
      "[Training Epoch 0] Batch 3336, Loss 0.3048090934753418\n",
      "[Training Epoch 0] Batch 3337, Loss 0.30604976415634155\n",
      "[Training Epoch 0] Batch 3338, Loss 0.3266272246837616\n",
      "[Training Epoch 0] Batch 3339, Loss 0.31321293115615845\n",
      "[Training Epoch 0] Batch 3340, Loss 0.30448025465011597\n",
      "[Training Epoch 0] Batch 3341, Loss 0.31359028816223145\n",
      "[Training Epoch 0] Batch 3342, Loss 0.3342670798301697\n",
      "[Training Epoch 0] Batch 3343, Loss 0.3216124475002289\n",
      "[Training Epoch 0] Batch 3344, Loss 0.31202706694602966\n",
      "[Training Epoch 0] Batch 3345, Loss 0.3551150858402252\n",
      "[Training Epoch 0] Batch 3346, Loss 0.31746169924736023\n",
      "[Training Epoch 0] Batch 3347, Loss 0.3301115334033966\n",
      "[Training Epoch 0] Batch 3348, Loss 0.30583909153938293\n",
      "[Training Epoch 0] Batch 3349, Loss 0.3147938549518585\n",
      "[Training Epoch 0] Batch 3350, Loss 0.2695613503456116\n",
      "[Training Epoch 0] Batch 3351, Loss 0.30197614431381226\n",
      "[Training Epoch 0] Batch 3352, Loss 0.32435342669487\n",
      "[Training Epoch 0] Batch 3353, Loss 0.32176506519317627\n",
      "[Training Epoch 0] Batch 3354, Loss 0.32741042971611023\n",
      "[Training Epoch 0] Batch 3355, Loss 0.31367799639701843\n",
      "[Training Epoch 0] Batch 3356, Loss 0.2871006727218628\n",
      "[Training Epoch 0] Batch 3357, Loss 0.30118370056152344\n",
      "[Training Epoch 0] Batch 3358, Loss 0.3377290964126587\n",
      "[Training Epoch 0] Batch 3359, Loss 0.2944267988204956\n",
      "[Training Epoch 0] Batch 3360, Loss 0.31884875893592834\n",
      "[Training Epoch 0] Batch 3361, Loss 0.3267369866371155\n",
      "[Training Epoch 0] Batch 3362, Loss 0.30850496888160706\n",
      "[Training Epoch 0] Batch 3363, Loss 0.2891663610935211\n",
      "[Training Epoch 0] Batch 3364, Loss 0.2997927963733673\n",
      "[Training Epoch 0] Batch 3365, Loss 0.31541597843170166\n",
      "[Training Epoch 0] Batch 3366, Loss 0.3230504095554352\n",
      "[Training Epoch 0] Batch 3367, Loss 0.2999180853366852\n",
      "[Training Epoch 0] Batch 3368, Loss 0.32790619134902954\n",
      "[Training Epoch 0] Batch 3369, Loss 0.3135150671005249\n",
      "[Training Epoch 0] Batch 3370, Loss 0.330168753862381\n",
      "[Training Epoch 0] Batch 3371, Loss 0.3060600459575653\n",
      "[Training Epoch 0] Batch 3372, Loss 0.3439163565635681\n",
      "[Training Epoch 0] Batch 3373, Loss 0.30933812260627747\n",
      "[Training Epoch 0] Batch 3374, Loss 0.31520432233810425\n",
      "[Training Epoch 0] Batch 3375, Loss 0.3388838768005371\n",
      "[Training Epoch 0] Batch 3376, Loss 0.3126310110092163\n",
      "[Training Epoch 0] Batch 3377, Loss 0.308076411485672\n",
      "[Training Epoch 0] Batch 3378, Loss 0.3084489107131958\n",
      "[Training Epoch 0] Batch 3379, Loss 0.2979513704776764\n",
      "[Training Epoch 0] Batch 3380, Loss 0.3092673718929291\n",
      "[Training Epoch 0] Batch 3381, Loss 0.3037776052951813\n",
      "[Training Epoch 0] Batch 3382, Loss 0.3218773901462555\n",
      "[Training Epoch 0] Batch 3383, Loss 0.30103611946105957\n",
      "[Training Epoch 0] Batch 3384, Loss 0.33272409439086914\n",
      "[Training Epoch 0] Batch 3385, Loss 0.3291192352771759\n",
      "[Training Epoch 0] Batch 3386, Loss 0.3054714798927307\n",
      "[Training Epoch 0] Batch 3387, Loss 0.2821926474571228\n",
      "[Training Epoch 0] Batch 3388, Loss 0.3136983811855316\n",
      "[Training Epoch 0] Batch 3389, Loss 0.3156603276729584\n",
      "[Training Epoch 0] Batch 3390, Loss 0.28119516372680664\n",
      "[Training Epoch 0] Batch 3391, Loss 0.30248385667800903\n",
      "[Training Epoch 0] Batch 3392, Loss 0.28481268882751465\n",
      "[Training Epoch 0] Batch 3393, Loss 0.30096766352653503\n",
      "[Training Epoch 0] Batch 3394, Loss 0.3131721317768097\n",
      "[Training Epoch 0] Batch 3395, Loss 0.32933688163757324\n",
      "[Training Epoch 0] Batch 3396, Loss 0.31202033162117004\n",
      "[Training Epoch 0] Batch 3397, Loss 0.3271462619304657\n",
      "[Training Epoch 0] Batch 3398, Loss 0.3375070095062256\n",
      "[Training Epoch 0] Batch 3399, Loss 0.32205823063850403\n",
      "[Training Epoch 0] Batch 3400, Loss 0.3251355290412903\n",
      "[Training Epoch 0] Batch 3401, Loss 0.3106750547885895\n",
      "[Training Epoch 0] Batch 3402, Loss 0.298432320356369\n",
      "[Training Epoch 0] Batch 3403, Loss 0.3029243052005768\n",
      "[Training Epoch 0] Batch 3404, Loss 0.2963075339794159\n",
      "[Training Epoch 0] Batch 3405, Loss 0.2992204427719116\n",
      "[Training Epoch 0] Batch 3406, Loss 0.3198172152042389\n",
      "[Training Epoch 0] Batch 3407, Loss 0.2813798487186432\n",
      "[Training Epoch 0] Batch 3408, Loss 0.3403640687465668\n",
      "[Training Epoch 0] Batch 3409, Loss 0.30251428484916687\n",
      "[Training Epoch 0] Batch 3410, Loss 0.3348942995071411\n",
      "[Training Epoch 0] Batch 3411, Loss 0.31309887766838074\n",
      "[Training Epoch 0] Batch 3412, Loss 0.32597604393959045\n",
      "[Training Epoch 0] Batch 3413, Loss 0.37157678604125977\n",
      "[Training Epoch 0] Batch 3414, Loss 0.3027186691761017\n",
      "[Training Epoch 0] Batch 3415, Loss 0.31898829340934753\n",
      "[Training Epoch 0] Batch 3416, Loss 0.31155410408973694\n",
      "[Training Epoch 0] Batch 3417, Loss 0.30797749757766724\n",
      "[Training Epoch 0] Batch 3418, Loss 0.30305588245391846\n",
      "[Training Epoch 0] Batch 3419, Loss 0.309177041053772\n",
      "[Training Epoch 0] Batch 3420, Loss 0.33243390917778015\n",
      "[Training Epoch 0] Batch 3421, Loss 0.31789177656173706\n",
      "[Training Epoch 0] Batch 3422, Loss 0.32729071378707886\n",
      "[Training Epoch 0] Batch 3423, Loss 0.3463849425315857\n",
      "[Training Epoch 0] Batch 3424, Loss 0.31668317317962646\n",
      "[Training Epoch 0] Batch 3425, Loss 0.3020016849040985\n",
      "[Training Epoch 0] Batch 3426, Loss 0.30537718534469604\n",
      "[Training Epoch 0] Batch 3427, Loss 0.3047674596309662\n",
      "[Training Epoch 0] Batch 3428, Loss 0.3294830918312073\n",
      "[Training Epoch 0] Batch 3429, Loss 0.2809138894081116\n",
      "[Training Epoch 0] Batch 3430, Loss 0.3107423484325409\n",
      "[Training Epoch 0] Batch 3431, Loss 0.3393651247024536\n",
      "[Training Epoch 0] Batch 3432, Loss 0.3328678011894226\n",
      "[Training Epoch 0] Batch 3433, Loss 0.2864774763584137\n",
      "[Training Epoch 0] Batch 3434, Loss 0.3041808009147644\n",
      "[Training Epoch 0] Batch 3435, Loss 0.3505352735519409\n",
      "[Training Epoch 0] Batch 3436, Loss 0.2890782952308655\n",
      "[Training Epoch 0] Batch 3437, Loss 0.32535192370414734\n",
      "[Training Epoch 0] Batch 3438, Loss 0.27952197194099426\n",
      "[Training Epoch 0] Batch 3439, Loss 0.29696401953697205\n",
      "[Training Epoch 0] Batch 3440, Loss 0.3298318386077881\n",
      "[Training Epoch 0] Batch 3441, Loss 0.3180142939090729\n",
      "[Training Epoch 0] Batch 3442, Loss 0.2865297794342041\n",
      "[Training Epoch 0] Batch 3443, Loss 0.3080679476261139\n",
      "[Training Epoch 0] Batch 3444, Loss 0.3343144953250885\n",
      "[Training Epoch 0] Batch 3445, Loss 0.2970011830329895\n",
      "[Training Epoch 0] Batch 3446, Loss 0.3182617127895355\n",
      "[Training Epoch 0] Batch 3447, Loss 0.3064691424369812\n",
      "[Training Epoch 0] Batch 3448, Loss 0.3249504566192627\n",
      "[Training Epoch 0] Batch 3449, Loss 0.32557058334350586\n",
      "[Training Epoch 0] Batch 3450, Loss 0.31146639585494995\n",
      "[Training Epoch 0] Batch 3451, Loss 0.33176735043525696\n",
      "[Training Epoch 0] Batch 3452, Loss 0.3116854727268219\n",
      "[Training Epoch 0] Batch 3453, Loss 0.30007678270339966\n",
      "[Training Epoch 0] Batch 3454, Loss 0.28113558888435364\n",
      "[Training Epoch 0] Batch 3455, Loss 0.3010599911212921\n",
      "[Training Epoch 0] Batch 3456, Loss 0.3020288646221161\n",
      "[Training Epoch 0] Batch 3457, Loss 0.2960444390773773\n",
      "[Training Epoch 0] Batch 3458, Loss 0.3077426850795746\n",
      "[Training Epoch 0] Batch 3459, Loss 0.32518237829208374\n",
      "[Training Epoch 0] Batch 3460, Loss 0.2931690216064453\n",
      "[Training Epoch 0] Batch 3461, Loss 0.3096214532852173\n",
      "[Training Epoch 0] Batch 3462, Loss 0.34778469800949097\n",
      "[Training Epoch 0] Batch 3463, Loss 0.28656110167503357\n",
      "[Training Epoch 0] Batch 3464, Loss 0.2972588837146759\n",
      "[Training Epoch 0] Batch 3465, Loss 0.2998851537704468\n",
      "[Training Epoch 0] Batch 3466, Loss 0.3144852817058563\n",
      "[Training Epoch 0] Batch 3467, Loss 0.3054448962211609\n",
      "[Training Epoch 0] Batch 3468, Loss 0.29637613892555237\n",
      "[Training Epoch 0] Batch 3469, Loss 0.29601800441741943\n",
      "[Training Epoch 0] Batch 3470, Loss 0.31218814849853516\n",
      "[Training Epoch 0] Batch 3471, Loss 0.3156425654888153\n",
      "[Training Epoch 0] Batch 3472, Loss 0.3015194535255432\n",
      "[Training Epoch 0] Batch 3473, Loss 0.32097816467285156\n",
      "[Training Epoch 0] Batch 3474, Loss 0.32694870233535767\n",
      "[Training Epoch 0] Batch 3475, Loss 0.3179223835468292\n",
      "[Training Epoch 0] Batch 3476, Loss 0.323398232460022\n",
      "[Training Epoch 0] Batch 3477, Loss 0.30066758394241333\n",
      "[Training Epoch 0] Batch 3478, Loss 0.33219456672668457\n",
      "[Training Epoch 0] Batch 3479, Loss 0.2896912693977356\n",
      "[Training Epoch 0] Batch 3480, Loss 0.322559654712677\n",
      "[Training Epoch 0] Batch 3481, Loss 0.3037168085575104\n",
      "[Training Epoch 0] Batch 3482, Loss 0.2620517611503601\n",
      "[Training Epoch 0] Batch 3483, Loss 0.2918592095375061\n",
      "[Training Epoch 0] Batch 3484, Loss 0.29949265718460083\n",
      "[Training Epoch 0] Batch 3485, Loss 0.31162750720977783\n",
      "[Training Epoch 0] Batch 3486, Loss 0.31141048669815063\n",
      "[Training Epoch 0] Batch 3487, Loss 0.31946659088134766\n",
      "[Training Epoch 0] Batch 3488, Loss 0.29019778966903687\n",
      "[Training Epoch 0] Batch 3489, Loss 0.3427128791809082\n",
      "[Training Epoch 0] Batch 3490, Loss 0.3058989644050598\n",
      "[Training Epoch 0] Batch 3491, Loss 0.292263925075531\n",
      "[Training Epoch 0] Batch 3492, Loss 0.30944889783859253\n",
      "[Training Epoch 0] Batch 3493, Loss 0.3243108093738556\n",
      "[Training Epoch 0] Batch 3494, Loss 0.344090074300766\n",
      "[Training Epoch 0] Batch 3495, Loss 0.29917043447494507\n",
      "[Training Epoch 0] Batch 3496, Loss 0.31761038303375244\n",
      "[Training Epoch 0] Batch 3497, Loss 0.3095669448375702\n",
      "[Training Epoch 0] Batch 3498, Loss 0.36686521768569946\n",
      "[Training Epoch 0] Batch 3499, Loss 0.30599790811538696\n",
      "[Training Epoch 0] Batch 3500, Loss 0.3115401566028595\n",
      "[Training Epoch 0] Batch 3501, Loss 0.31115278601646423\n",
      "[Training Epoch 0] Batch 3502, Loss 0.3002678155899048\n",
      "[Training Epoch 0] Batch 3503, Loss 0.34431564807891846\n",
      "[Training Epoch 0] Batch 3504, Loss 0.3263327181339264\n",
      "[Training Epoch 0] Batch 3505, Loss 0.3294745683670044\n",
      "[Training Epoch 0] Batch 3506, Loss 0.30855727195739746\n",
      "[Training Epoch 0] Batch 3507, Loss 0.3355458378791809\n",
      "[Training Epoch 0] Batch 3508, Loss 0.2954709827899933\n",
      "[Training Epoch 0] Batch 3509, Loss 0.32821425795555115\n",
      "[Training Epoch 0] Batch 3510, Loss 0.31908226013183594\n",
      "[Training Epoch 0] Batch 3511, Loss 0.3262958824634552\n",
      "[Training Epoch 0] Batch 3512, Loss 0.31750842928886414\n",
      "[Training Epoch 0] Batch 3513, Loss 0.3380807042121887\n",
      "[Training Epoch 0] Batch 3514, Loss 0.3007352352142334\n",
      "[Training Epoch 0] Batch 3515, Loss 0.2899642586708069\n",
      "[Training Epoch 0] Batch 3516, Loss 0.3375180959701538\n",
      "[Training Epoch 0] Batch 3517, Loss 0.2920397222042084\n",
      "[Training Epoch 0] Batch 3518, Loss 0.3096533417701721\n",
      "[Training Epoch 0] Batch 3519, Loss 0.3378846049308777\n",
      "[Training Epoch 0] Batch 3520, Loss 0.30963215231895447\n",
      "[Training Epoch 0] Batch 3521, Loss 0.33450254797935486\n",
      "[Training Epoch 0] Batch 3522, Loss 0.3103519082069397\n",
      "[Training Epoch 0] Batch 3523, Loss 0.3170011639595032\n",
      "[Training Epoch 0] Batch 3524, Loss 0.31515002250671387\n",
      "[Training Epoch 0] Batch 3525, Loss 0.2949143350124359\n",
      "[Training Epoch 0] Batch 3526, Loss 0.297661691904068\n",
      "[Training Epoch 0] Batch 3527, Loss 0.3392004072666168\n",
      "[Training Epoch 0] Batch 3528, Loss 0.30725935101509094\n",
      "[Training Epoch 0] Batch 3529, Loss 0.31827685236930847\n",
      "[Training Epoch 0] Batch 3530, Loss 0.31305044889450073\n",
      "[Training Epoch 0] Batch 3531, Loss 0.3285037577152252\n",
      "[Training Epoch 0] Batch 3532, Loss 0.33329707384109497\n",
      "[Training Epoch 0] Batch 3533, Loss 0.3345945477485657\n",
      "[Training Epoch 0] Batch 3534, Loss 0.2912832200527191\n",
      "[Training Epoch 0] Batch 3535, Loss 0.31166788935661316\n",
      "[Training Epoch 0] Batch 3536, Loss 0.33020421862602234\n",
      "[Training Epoch 0] Batch 3537, Loss 0.3284650444984436\n",
      "[Training Epoch 0] Batch 3538, Loss 0.3292224109172821\n",
      "[Training Epoch 0] Batch 3539, Loss 0.30802470445632935\n",
      "[Training Epoch 0] Batch 3540, Loss 0.3086698055267334\n",
      "[Training Epoch 0] Batch 3541, Loss 0.30651113390922546\n",
      "[Training Epoch 0] Batch 3542, Loss 0.28653451800346375\n",
      "[Training Epoch 0] Batch 3543, Loss 0.3003651797771454\n",
      "[Training Epoch 0] Batch 3544, Loss 0.30323296785354614\n",
      "[Training Epoch 0] Batch 3545, Loss 0.3241419196128845\n",
      "[Training Epoch 0] Batch 3546, Loss 0.2977854013442993\n",
      "[Training Epoch 0] Batch 3547, Loss 0.33043885231018066\n",
      "[Training Epoch 0] Batch 3548, Loss 0.30468928813934326\n",
      "[Training Epoch 0] Batch 3549, Loss 0.32514211535453796\n",
      "[Training Epoch 0] Batch 3550, Loss 0.3112221360206604\n",
      "[Training Epoch 0] Batch 3551, Loss 0.28032830357551575\n",
      "[Training Epoch 0] Batch 3552, Loss 0.3234209716320038\n",
      "[Training Epoch 0] Batch 3553, Loss 0.2980887293815613\n",
      "[Training Epoch 0] Batch 3554, Loss 0.34184885025024414\n",
      "[Training Epoch 0] Batch 3555, Loss 0.32834115624427795\n",
      "[Training Epoch 0] Batch 3556, Loss 0.3148098289966583\n",
      "[Training Epoch 0] Batch 3557, Loss 0.3397049903869629\n",
      "[Training Epoch 0] Batch 3558, Loss 0.29861748218536377\n",
      "[Training Epoch 0] Batch 3559, Loss 0.32203516364097595\n",
      "[Training Epoch 0] Batch 3560, Loss 0.31926146149635315\n",
      "[Training Epoch 0] Batch 3561, Loss 0.2930896282196045\n",
      "[Training Epoch 0] Batch 3562, Loss 0.3099631071090698\n",
      "[Training Epoch 0] Batch 3563, Loss 0.3205849826335907\n",
      "[Training Epoch 0] Batch 3564, Loss 0.2971954345703125\n",
      "[Training Epoch 0] Batch 3565, Loss 0.3099745213985443\n",
      "[Training Epoch 0] Batch 3566, Loss 0.3422737419605255\n",
      "[Training Epoch 0] Batch 3567, Loss 0.3040953576564789\n",
      "[Training Epoch 0] Batch 3568, Loss 0.31522464752197266\n",
      "[Training Epoch 0] Batch 3569, Loss 0.3191283047199249\n",
      "[Training Epoch 0] Batch 3570, Loss 0.29839393496513367\n",
      "[Training Epoch 0] Batch 3571, Loss 0.2851407527923584\n",
      "[Training Epoch 0] Batch 3572, Loss 0.32348746061325073\n",
      "[Training Epoch 0] Batch 3573, Loss 0.3403724133968353\n",
      "[Training Epoch 0] Batch 3574, Loss 0.29693442583084106\n",
      "[Training Epoch 0] Batch 3575, Loss 0.3202342987060547\n",
      "[Training Epoch 0] Batch 3576, Loss 0.30536770820617676\n",
      "[Training Epoch 0] Batch 3577, Loss 0.32105544209480286\n",
      "[Training Epoch 0] Batch 3578, Loss 0.30335840582847595\n",
      "[Training Epoch 0] Batch 3579, Loss 0.31185975670814514\n",
      "[Training Epoch 0] Batch 3580, Loss 0.29263192415237427\n",
      "[Training Epoch 0] Batch 3581, Loss 0.32121044397354126\n",
      "[Training Epoch 0] Batch 3582, Loss 0.32878345251083374\n",
      "[Training Epoch 0] Batch 3583, Loss 0.3290742039680481\n",
      "[Training Epoch 0] Batch 3584, Loss 0.3474290668964386\n",
      "[Training Epoch 0] Batch 3585, Loss 0.3264032006263733\n",
      "[Training Epoch 0] Batch 3586, Loss 0.31898224353790283\n",
      "[Training Epoch 0] Batch 3587, Loss 0.30412060022354126\n",
      "[Training Epoch 0] Batch 3588, Loss 0.30520007014274597\n",
      "[Training Epoch 0] Batch 3589, Loss 0.2981768846511841\n",
      "[Training Epoch 0] Batch 3590, Loss 0.2928270995616913\n",
      "[Training Epoch 0] Batch 3591, Loss 0.27844616770744324\n",
      "[Training Epoch 0] Batch 3592, Loss 0.3304196000099182\n",
      "[Training Epoch 0] Batch 3593, Loss 0.287506103515625\n",
      "[Training Epoch 0] Batch 3594, Loss 0.34471672773361206\n",
      "[Training Epoch 0] Batch 3595, Loss 0.3044455051422119\n",
      "[Training Epoch 0] Batch 3596, Loss 0.3099048435688019\n",
      "[Training Epoch 0] Batch 3597, Loss 0.2908281981945038\n",
      "[Training Epoch 0] Batch 3598, Loss 0.3489148020744324\n",
      "[Training Epoch 0] Batch 3599, Loss 0.3211449384689331\n",
      "[Training Epoch 0] Batch 3600, Loss 0.31926649808883667\n",
      "[Training Epoch 0] Batch 3601, Loss 0.3186008334159851\n",
      "[Training Epoch 0] Batch 3602, Loss 0.25912824273109436\n",
      "[Training Epoch 0] Batch 3603, Loss 0.296718031167984\n",
      "[Training Epoch 0] Batch 3604, Loss 0.3207498788833618\n",
      "[Training Epoch 0] Batch 3605, Loss 0.2997339963912964\n",
      "[Training Epoch 0] Batch 3606, Loss 0.32848575711250305\n",
      "[Training Epoch 0] Batch 3607, Loss 0.3094753324985504\n",
      "[Training Epoch 0] Batch 3608, Loss 0.30847951769828796\n",
      "[Training Epoch 0] Batch 3609, Loss 0.33368372917175293\n",
      "[Training Epoch 0] Batch 3610, Loss 0.30297011137008667\n",
      "[Training Epoch 0] Batch 3611, Loss 0.28593310713768005\n",
      "[Training Epoch 0] Batch 3612, Loss 0.29028627276420593\n",
      "[Training Epoch 0] Batch 3613, Loss 0.3303770124912262\n",
      "[Training Epoch 0] Batch 3614, Loss 0.31540247797966003\n",
      "[Training Epoch 0] Batch 3615, Loss 0.27694499492645264\n",
      "[Training Epoch 0] Batch 3616, Loss 0.29652494192123413\n",
      "[Training Epoch 0] Batch 3617, Loss 0.3309081792831421\n",
      "[Training Epoch 0] Batch 3618, Loss 0.29171326756477356\n",
      "[Training Epoch 0] Batch 3619, Loss 0.30979233980178833\n",
      "[Training Epoch 0] Batch 3620, Loss 0.3240945339202881\n",
      "[Training Epoch 0] Batch 3621, Loss 0.3169207274913788\n",
      "[Training Epoch 0] Batch 3622, Loss 0.29453107714653015\n",
      "[Training Epoch 0] Batch 3623, Loss 0.3296697437763214\n",
      "[Training Epoch 0] Batch 3624, Loss 0.30483365058898926\n",
      "[Training Epoch 0] Batch 3625, Loss 0.2860451638698578\n",
      "[Training Epoch 0] Batch 3626, Loss 0.27154257893562317\n",
      "[Training Epoch 0] Batch 3627, Loss 0.29629260301589966\n",
      "[Training Epoch 0] Batch 3628, Loss 0.3153378665447235\n",
      "[Training Epoch 0] Batch 3629, Loss 0.2971452474594116\n",
      "[Training Epoch 0] Batch 3630, Loss 0.318591445684433\n",
      "[Training Epoch 0] Batch 3631, Loss 0.35968488454818726\n",
      "[Training Epoch 0] Batch 3632, Loss 0.35039618611335754\n",
      "[Training Epoch 0] Batch 3633, Loss 0.3022706210613251\n",
      "[Training Epoch 0] Batch 3634, Loss 0.30010786652565\n",
      "[Training Epoch 0] Batch 3635, Loss 0.3411681652069092\n",
      "[Training Epoch 0] Batch 3636, Loss 0.30619633197784424\n",
      "[Training Epoch 0] Batch 3637, Loss 0.317531943321228\n",
      "[Training Epoch 0] Batch 3638, Loss 0.3354520797729492\n",
      "[Training Epoch 0] Batch 3639, Loss 0.3213236927986145\n",
      "[Training Epoch 0] Batch 3640, Loss 0.3254234194755554\n",
      "[Training Epoch 0] Batch 3641, Loss 0.3351758122444153\n",
      "[Training Epoch 0] Batch 3642, Loss 0.2901071608066559\n",
      "[Training Epoch 0] Batch 3643, Loss 0.3417641520500183\n",
      "[Training Epoch 0] Batch 3644, Loss 0.3030190169811249\n",
      "[Training Epoch 0] Batch 3645, Loss 0.2957291305065155\n",
      "[Training Epoch 0] Batch 3646, Loss 0.292571485042572\n",
      "[Training Epoch 0] Batch 3647, Loss 0.2872977554798126\n",
      "[Training Epoch 0] Batch 3648, Loss 0.3297967314720154\n",
      "[Training Epoch 0] Batch 3649, Loss 0.30282455682754517\n",
      "[Training Epoch 0] Batch 3650, Loss 0.2974422872066498\n",
      "[Training Epoch 0] Batch 3651, Loss 0.3258310556411743\n",
      "[Training Epoch 0] Batch 3652, Loss 0.2952365279197693\n",
      "[Training Epoch 0] Batch 3653, Loss 0.35718202590942383\n",
      "[Training Epoch 0] Batch 3654, Loss 0.32345086336135864\n",
      "[Training Epoch 0] Batch 3655, Loss 0.31637436151504517\n",
      "[Training Epoch 0] Batch 3656, Loss 0.3068930506706238\n",
      "[Training Epoch 0] Batch 3657, Loss 0.3217356503009796\n",
      "[Training Epoch 0] Batch 3658, Loss 0.2841207683086395\n",
      "[Training Epoch 0] Batch 3659, Loss 0.3465903401374817\n",
      "[Training Epoch 0] Batch 3660, Loss 0.33826908469200134\n",
      "[Training Epoch 0] Batch 3661, Loss 0.34498435258865356\n",
      "[Training Epoch 0] Batch 3662, Loss 0.31020092964172363\n",
      "[Training Epoch 0] Batch 3663, Loss 0.2948562204837799\n",
      "[Training Epoch 0] Batch 3664, Loss 0.34003937244415283\n",
      "[Training Epoch 0] Batch 3665, Loss 0.2972090244293213\n",
      "[Training Epoch 0] Batch 3666, Loss 0.30940860509872437\n",
      "[Training Epoch 0] Batch 3667, Loss 0.3160320222377777\n",
      "[Training Epoch 0] Batch 3668, Loss 0.3269283175468445\n",
      "[Training Epoch 0] Batch 3669, Loss 0.3392050862312317\n",
      "[Training Epoch 0] Batch 3670, Loss 0.28516995906829834\n",
      "[Training Epoch 0] Batch 3671, Loss 0.29831254482269287\n",
      "[Training Epoch 0] Batch 3672, Loss 0.3242623507976532\n",
      "[Training Epoch 0] Batch 3673, Loss 0.31879061460494995\n",
      "[Training Epoch 0] Batch 3674, Loss 0.3192932903766632\n",
      "[Training Epoch 0] Batch 3675, Loss 0.28724992275238037\n",
      "[Training Epoch 0] Batch 3676, Loss 0.3179715871810913\n",
      "[Training Epoch 0] Batch 3677, Loss 0.30972999334335327\n",
      "[Training Epoch 0] Batch 3678, Loss 0.28149503469467163\n",
      "[Training Epoch 0] Batch 3679, Loss 0.28547215461730957\n",
      "[Training Epoch 0] Batch 3680, Loss 0.3465306758880615\n",
      "[Training Epoch 0] Batch 3681, Loss 0.3304476737976074\n",
      "[Training Epoch 0] Batch 3682, Loss 0.32093939185142517\n",
      "[Training Epoch 0] Batch 3683, Loss 0.30740201473236084\n",
      "[Training Epoch 0] Batch 3684, Loss 0.31634071469306946\n",
      "[Training Epoch 0] Batch 3685, Loss 0.2654266059398651\n",
      "[Training Epoch 0] Batch 3686, Loss 0.28981196880340576\n",
      "[Training Epoch 0] Batch 3687, Loss 0.28498125076293945\n",
      "[Training Epoch 0] Batch 3688, Loss 0.3172444999217987\n",
      "[Training Epoch 0] Batch 3689, Loss 0.31918489933013916\n",
      "[Training Epoch 0] Batch 3690, Loss 0.2695264220237732\n",
      "[Training Epoch 0] Batch 3691, Loss 0.3047925531864166\n",
      "[Training Epoch 0] Batch 3692, Loss 0.3201536238193512\n",
      "[Training Epoch 0] Batch 3693, Loss 0.38348421454429626\n",
      "[Training Epoch 0] Batch 3694, Loss 0.30866897106170654\n",
      "[Training Epoch 0] Batch 3695, Loss 0.31178194284439087\n",
      "[Training Epoch 0] Batch 3696, Loss 0.2778536379337311\n",
      "[Training Epoch 0] Batch 3697, Loss 0.3452067971229553\n",
      "[Training Epoch 0] Batch 3698, Loss 0.28522002696990967\n",
      "[Training Epoch 0] Batch 3699, Loss 0.2971750497817993\n",
      "[Training Epoch 0] Batch 3700, Loss 0.305022656917572\n",
      "[Training Epoch 0] Batch 3701, Loss 0.30312034487724304\n",
      "[Training Epoch 0] Batch 3702, Loss 0.32504376769065857\n",
      "[Training Epoch 0] Batch 3703, Loss 0.27804943919181824\n",
      "[Training Epoch 0] Batch 3704, Loss 0.3243734538555145\n",
      "[Training Epoch 0] Batch 3705, Loss 0.3040270507335663\n",
      "[Training Epoch 0] Batch 3706, Loss 0.32612988352775574\n",
      "[Training Epoch 0] Batch 3707, Loss 0.29844313859939575\n",
      "[Training Epoch 0] Batch 3708, Loss 0.3143015503883362\n",
      "[Training Epoch 0] Batch 3709, Loss 0.30981966853141785\n",
      "[Training Epoch 0] Batch 3710, Loss 0.33149710297584534\n",
      "[Training Epoch 0] Batch 3711, Loss 0.2756100296974182\n",
      "[Training Epoch 0] Batch 3712, Loss 0.32560670375823975\n",
      "[Training Epoch 0] Batch 3713, Loss 0.3114936351776123\n",
      "[Training Epoch 0] Batch 3714, Loss 0.2872544825077057\n",
      "[Training Epoch 0] Batch 3715, Loss 0.30863362550735474\n",
      "[Training Epoch 0] Batch 3716, Loss 0.30233636498451233\n",
      "[Training Epoch 0] Batch 3717, Loss 0.3141878545284271\n",
      "[Training Epoch 0] Batch 3718, Loss 0.2835874557495117\n",
      "[Training Epoch 0] Batch 3719, Loss 0.3373268246650696\n",
      "[Training Epoch 0] Batch 3720, Loss 0.30027100443840027\n",
      "[Training Epoch 0] Batch 3721, Loss 0.29809415340423584\n",
      "[Training Epoch 0] Batch 3722, Loss 0.30738598108291626\n",
      "[Training Epoch 0] Batch 3723, Loss 0.2924853265285492\n",
      "[Training Epoch 0] Batch 3724, Loss 0.3162635564804077\n",
      "[Training Epoch 0] Batch 3725, Loss 0.32675111293792725\n",
      "[Training Epoch 0] Batch 3726, Loss 0.31033650040626526\n",
      "[Training Epoch 0] Batch 3727, Loss 0.2918425500392914\n",
      "[Training Epoch 0] Batch 3728, Loss 0.319155216217041\n",
      "[Training Epoch 0] Batch 3729, Loss 0.3043060898780823\n",
      "[Training Epoch 0] Batch 3730, Loss 0.2951711416244507\n",
      "[Training Epoch 0] Batch 3731, Loss 0.28801706433296204\n",
      "[Training Epoch 0] Batch 3732, Loss 0.30004093050956726\n",
      "[Training Epoch 0] Batch 3733, Loss 0.3329263925552368\n",
      "[Training Epoch 0] Batch 3734, Loss 0.3403053879737854\n",
      "[Training Epoch 0] Batch 3735, Loss 0.31715381145477295\n",
      "[Training Epoch 0] Batch 3736, Loss 0.2680151164531708\n",
      "[Training Epoch 0] Batch 3737, Loss 0.32315829396247864\n",
      "[Training Epoch 0] Batch 3738, Loss 0.3020021617412567\n",
      "[Training Epoch 0] Batch 3739, Loss 0.3024321496486664\n",
      "[Training Epoch 0] Batch 3740, Loss 0.31539177894592285\n",
      "[Training Epoch 0] Batch 3741, Loss 0.3106750547885895\n",
      "[Training Epoch 0] Batch 3742, Loss 0.3184274435043335\n",
      "[Training Epoch 0] Batch 3743, Loss 0.3240261375904083\n",
      "[Training Epoch 0] Batch 3744, Loss 0.32078513503074646\n",
      "[Training Epoch 0] Batch 3745, Loss 0.33562254905700684\n",
      "[Training Epoch 0] Batch 3746, Loss 0.291999489068985\n",
      "[Training Epoch 0] Batch 3747, Loss 0.2986219525337219\n",
      "[Training Epoch 0] Batch 3748, Loss 0.2666548192501068\n",
      "[Training Epoch 0] Batch 3749, Loss 0.32393205165863037\n",
      "[Training Epoch 0] Batch 3750, Loss 0.3113015294075012\n",
      "[Training Epoch 0] Batch 3751, Loss 0.31101128458976746\n",
      "[Training Epoch 0] Batch 3752, Loss 0.26549792289733887\n",
      "[Training Epoch 0] Batch 3753, Loss 0.2997781038284302\n",
      "[Training Epoch 0] Batch 3754, Loss 0.2943880259990692\n",
      "[Training Epoch 0] Batch 3755, Loss 0.3271825313568115\n",
      "[Training Epoch 0] Batch 3756, Loss 0.3206721246242523\n",
      "[Training Epoch 0] Batch 3757, Loss 0.31540367007255554\n",
      "[Training Epoch 0] Batch 3758, Loss 0.30584341287612915\n",
      "[Training Epoch 0] Batch 3759, Loss 0.3162725567817688\n",
      "[Training Epoch 0] Batch 3760, Loss 0.3123044967651367\n",
      "[Training Epoch 0] Batch 3761, Loss 0.2911466658115387\n",
      "[Training Epoch 0] Batch 3762, Loss 0.287572979927063\n",
      "[Training Epoch 0] Batch 3763, Loss 0.31008923053741455\n",
      "[Training Epoch 0] Batch 3764, Loss 0.3454384207725525\n",
      "[Training Epoch 0] Batch 3765, Loss 0.3306988477706909\n",
      "[Training Epoch 0] Batch 3766, Loss 0.27956438064575195\n",
      "[Training Epoch 0] Batch 3767, Loss 0.3188609182834625\n",
      "[Training Epoch 0] Batch 3768, Loss 0.3167417347431183\n",
      "[Training Epoch 0] Batch 3769, Loss 0.31171223521232605\n",
      "[Training Epoch 0] Batch 3770, Loss 0.2811107337474823\n",
      "[Training Epoch 0] Batch 3771, Loss 0.2977466285228729\n",
      "[Training Epoch 0] Batch 3772, Loss 0.2984473407268524\n",
      "[Training Epoch 0] Batch 3773, Loss 0.3108850121498108\n",
      "[Training Epoch 0] Batch 3774, Loss 0.33344975113868713\n",
      "[Training Epoch 0] Batch 3775, Loss 0.2855764627456665\n",
      "[Training Epoch 0] Batch 3776, Loss 0.2874854505062103\n",
      "[Training Epoch 0] Batch 3777, Loss 0.3275197148323059\n",
      "[Training Epoch 0] Batch 3778, Loss 0.32056188583374023\n",
      "[Training Epoch 0] Batch 3779, Loss 0.31414157152175903\n",
      "[Training Epoch 0] Batch 3780, Loss 0.30798861384391785\n",
      "[Training Epoch 0] Batch 3781, Loss 0.3045576810836792\n",
      "[Training Epoch 0] Batch 3782, Loss 0.2937775254249573\n",
      "[Training Epoch 0] Batch 3783, Loss 0.3033410906791687\n",
      "[Training Epoch 0] Batch 3784, Loss 0.32267799973487854\n",
      "[Training Epoch 0] Batch 3785, Loss 0.29115936160087585\n",
      "[Training Epoch 0] Batch 3786, Loss 0.2904384732246399\n",
      "[Training Epoch 0] Batch 3787, Loss 0.30489638447761536\n",
      "[Training Epoch 0] Batch 3788, Loss 0.31425949931144714\n",
      "[Training Epoch 0] Batch 3789, Loss 0.29417717456817627\n",
      "[Training Epoch 0] Batch 3790, Loss 0.30582863092422485\n",
      "[Training Epoch 0] Batch 3791, Loss 0.3162706196308136\n",
      "[Training Epoch 0] Batch 3792, Loss 0.2943810820579529\n",
      "[Training Epoch 0] Batch 3793, Loss 0.29096874594688416\n",
      "[Training Epoch 0] Batch 3794, Loss 0.3295542001724243\n",
      "[Training Epoch 0] Batch 3795, Loss 0.28501415252685547\n",
      "[Training Epoch 0] Batch 3796, Loss 0.2773193418979645\n",
      "[Training Epoch 0] Batch 3797, Loss 0.29694342613220215\n",
      "[Training Epoch 0] Batch 3798, Loss 0.2968063950538635\n",
      "[Training Epoch 0] Batch 3799, Loss 0.31051313877105713\n",
      "[Training Epoch 0] Batch 3800, Loss 0.30889129638671875\n",
      "[Training Epoch 0] Batch 3801, Loss 0.2936911880970001\n",
      "[Training Epoch 0] Batch 3802, Loss 0.3107587397098541\n",
      "[Training Epoch 0] Batch 3803, Loss 0.32051658630371094\n",
      "[Training Epoch 0] Batch 3804, Loss 0.31004542112350464\n",
      "[Training Epoch 0] Batch 3805, Loss 0.2928747236728668\n",
      "[Training Epoch 0] Batch 3806, Loss 0.3050832450389862\n",
      "[Training Epoch 0] Batch 3807, Loss 0.26952725648880005\n",
      "[Training Epoch 0] Batch 3808, Loss 0.2933253347873688\n",
      "[Training Epoch 0] Batch 3809, Loss 0.32036933302879333\n",
      "[Training Epoch 0] Batch 3810, Loss 0.30803927779197693\n",
      "[Training Epoch 0] Batch 3811, Loss 0.33413049578666687\n",
      "[Training Epoch 0] Batch 3812, Loss 0.3182038962841034\n",
      "[Training Epoch 0] Batch 3813, Loss 0.33042243123054504\n",
      "[Training Epoch 0] Batch 3814, Loss 0.30217066407203674\n",
      "[Training Epoch 0] Batch 3815, Loss 0.3002448081970215\n",
      "[Training Epoch 0] Batch 3816, Loss 0.311540812253952\n",
      "[Training Epoch 0] Batch 3817, Loss 0.3037649989128113\n",
      "[Training Epoch 0] Batch 3818, Loss 0.3057991862297058\n",
      "[Training Epoch 0] Batch 3819, Loss 0.32019007205963135\n",
      "[Training Epoch 0] Batch 3820, Loss 0.2834099233150482\n",
      "[Training Epoch 0] Batch 3821, Loss 0.3015987277030945\n",
      "[Training Epoch 0] Batch 3822, Loss 0.3225603997707367\n",
      "[Training Epoch 0] Batch 3823, Loss 0.30871087312698364\n",
      "[Training Epoch 0] Batch 3824, Loss 0.30853763222694397\n",
      "[Training Epoch 0] Batch 3825, Loss 0.2969454824924469\n",
      "[Training Epoch 0] Batch 3826, Loss 0.3288438022136688\n",
      "[Training Epoch 0] Batch 3827, Loss 0.3080671429634094\n",
      "[Training Epoch 0] Batch 3828, Loss 0.28586870431900024\n",
      "[Training Epoch 0] Batch 3829, Loss 0.3001379072666168\n",
      "[Training Epoch 0] Batch 3830, Loss 0.2999950647354126\n",
      "[Training Epoch 0] Batch 3831, Loss 0.3136001527309418\n",
      "[Training Epoch 0] Batch 3832, Loss 0.3017258644104004\n",
      "[Training Epoch 0] Batch 3833, Loss 0.33109554648399353\n",
      "[Training Epoch 0] Batch 3834, Loss 0.31378409266471863\n",
      "[Training Epoch 0] Batch 3835, Loss 0.32342925667762756\n",
      "[Training Epoch 0] Batch 3836, Loss 0.32494112849235535\n",
      "[Training Epoch 0] Batch 3837, Loss 0.29850059747695923\n",
      "[Training Epoch 0] Batch 3838, Loss 0.2900752127170563\n",
      "[Training Epoch 0] Batch 3839, Loss 0.2914831340312958\n",
      "[Training Epoch 0] Batch 3840, Loss 0.298539400100708\n",
      "[Training Epoch 0] Batch 3841, Loss 0.3190842270851135\n",
      "[Training Epoch 0] Batch 3842, Loss 0.2964724898338318\n",
      "[Training Epoch 0] Batch 3843, Loss 0.31286051869392395\n",
      "[Training Epoch 0] Batch 3844, Loss 0.3202384114265442\n",
      "[Training Epoch 0] Batch 3845, Loss 0.29281020164489746\n",
      "[Training Epoch 0] Batch 3846, Loss 0.2968454360961914\n",
      "[Training Epoch 0] Batch 3847, Loss 0.2811632752418518\n",
      "[Training Epoch 0] Batch 3848, Loss 0.3012339472770691\n",
      "[Training Epoch 0] Batch 3849, Loss 0.3216665983200073\n",
      "[Training Epoch 0] Batch 3850, Loss 0.30074983835220337\n",
      "[Training Epoch 0] Batch 3851, Loss 0.33692800998687744\n",
      "[Training Epoch 0] Batch 3852, Loss 0.28926539421081543\n",
      "[Training Epoch 0] Batch 3853, Loss 0.29670843482017517\n",
      "[Training Epoch 0] Batch 3854, Loss 0.28718146681785583\n",
      "[Training Epoch 0] Batch 3855, Loss 0.2908616364002228\n",
      "[Training Epoch 0] Batch 3856, Loss 0.2952544093132019\n",
      "[Training Epoch 0] Batch 3857, Loss 0.33591872453689575\n",
      "[Training Epoch 0] Batch 3858, Loss 0.3186401128768921\n",
      "[Training Epoch 0] Batch 3859, Loss 0.3148469924926758\n",
      "[Training Epoch 0] Batch 3860, Loss 0.2860889434814453\n",
      "[Training Epoch 0] Batch 3861, Loss 0.3268444240093231\n",
      "[Training Epoch 0] Batch 3862, Loss 0.31686070561408997\n",
      "[Training Epoch 0] Batch 3863, Loss 0.31253185868263245\n",
      "[Training Epoch 0] Batch 3864, Loss 0.3203608989715576\n",
      "[Training Epoch 0] Batch 3865, Loss 0.29082441329956055\n",
      "[Training Epoch 0] Batch 3866, Loss 0.2830246686935425\n",
      "[Training Epoch 0] Batch 3867, Loss 0.3222571313381195\n",
      "[Training Epoch 0] Batch 3868, Loss 0.3307974338531494\n",
      "[Training Epoch 0] Batch 3869, Loss 0.29887503385543823\n",
      "[Training Epoch 0] Batch 3870, Loss 0.3029839098453522\n",
      "[Training Epoch 0] Batch 3871, Loss 0.28607895970344543\n",
      "[Training Epoch 0] Batch 3872, Loss 0.31497886776924133\n",
      "[Training Epoch 0] Batch 3873, Loss 0.31025969982147217\n",
      "[Training Epoch 0] Batch 3874, Loss 0.2955928146839142\n",
      "[Training Epoch 0] Batch 3875, Loss 0.2691270709037781\n",
      "[Training Epoch 0] Batch 3876, Loss 0.2967389225959778\n",
      "[Training Epoch 0] Batch 3877, Loss 0.31051528453826904\n",
      "[Training Epoch 0] Batch 3878, Loss 0.30571383237838745\n",
      "[Training Epoch 0] Batch 3879, Loss 0.3201230764389038\n",
      "[Training Epoch 0] Batch 3880, Loss 0.2954516112804413\n",
      "[Training Epoch 0] Batch 3881, Loss 0.28044772148132324\n",
      "[Training Epoch 0] Batch 3882, Loss 0.35190752148628235\n",
      "[Training Epoch 0] Batch 3883, Loss 0.2851402163505554\n",
      "[Training Epoch 0] Batch 3884, Loss 0.3096000552177429\n",
      "[Training Epoch 0] Batch 3885, Loss 0.3168022930622101\n",
      "[Training Epoch 0] Batch 3886, Loss 0.3229067921638489\n",
      "[Training Epoch 0] Batch 3887, Loss 0.32206133008003235\n",
      "[Training Epoch 0] Batch 3888, Loss 0.3150295913219452\n",
      "[Training Epoch 0] Batch 3889, Loss 0.34083449840545654\n",
      "[Training Epoch 0] Batch 3890, Loss 0.29197898507118225\n",
      "[Training Epoch 0] Batch 3891, Loss 0.32013270258903503\n",
      "[Training Epoch 0] Batch 3892, Loss 0.3067972660064697\n",
      "[Training Epoch 0] Batch 3893, Loss 0.30738842487335205\n",
      "[Training Epoch 0] Batch 3894, Loss 0.32823142409324646\n",
      "[Training Epoch 0] Batch 3895, Loss 0.3155265748500824\n",
      "[Training Epoch 0] Batch 3896, Loss 0.2810395658016205\n",
      "[Training Epoch 0] Batch 3897, Loss 0.30671268701553345\n",
      "[Training Epoch 0] Batch 3898, Loss 0.32265159487724304\n",
      "[Training Epoch 0] Batch 3899, Loss 0.3087456226348877\n",
      "[Training Epoch 0] Batch 3900, Loss 0.3304753601551056\n",
      "[Training Epoch 0] Batch 3901, Loss 0.3265596926212311\n",
      "[Training Epoch 0] Batch 3902, Loss 0.30636677145957947\n",
      "[Training Epoch 0] Batch 3903, Loss 0.3345445394515991\n",
      "[Training Epoch 0] Batch 3904, Loss 0.2900483310222626\n",
      "[Training Epoch 0] Batch 3905, Loss 0.3100331425666809\n",
      "[Training Epoch 0] Batch 3906, Loss 0.28026247024536133\n",
      "[Training Epoch 0] Batch 3907, Loss 0.3189166784286499\n",
      "[Training Epoch 0] Batch 3908, Loss 0.29923540353775024\n",
      "[Training Epoch 0] Batch 3909, Loss 0.3152993321418762\n",
      "[Training Epoch 0] Batch 3910, Loss 0.30709052085876465\n",
      "[Training Epoch 0] Batch 3911, Loss 0.3238580822944641\n",
      "[Training Epoch 0] Batch 3912, Loss 0.3226946294307709\n",
      "[Training Epoch 0] Batch 3913, Loss 0.318989634513855\n",
      "[Training Epoch 0] Batch 3914, Loss 0.31079602241516113\n",
      "[Training Epoch 0] Batch 3915, Loss 0.3117721676826477\n",
      "[Training Epoch 0] Batch 3916, Loss 0.3110635578632355\n",
      "[Training Epoch 0] Batch 3917, Loss 0.3060498833656311\n",
      "[Training Epoch 0] Batch 3918, Loss 0.30403387546539307\n",
      "[Training Epoch 0] Batch 3919, Loss 0.32753604650497437\n",
      "[Training Epoch 0] Batch 3920, Loss 0.29781484603881836\n",
      "[Training Epoch 0] Batch 3921, Loss 0.30302849411964417\n",
      "[Training Epoch 0] Batch 3922, Loss 0.33078837394714355\n",
      "[Training Epoch 0] Batch 3923, Loss 0.3027828633785248\n",
      "[Training Epoch 0] Batch 3924, Loss 0.3527876138687134\n",
      "[Training Epoch 0] Batch 3925, Loss 0.3292086124420166\n",
      "[Training Epoch 0] Batch 3926, Loss 0.32893106341362\n",
      "[Training Epoch 0] Batch 3927, Loss 0.30193835496902466\n",
      "[Training Epoch 0] Batch 3928, Loss 0.31065642833709717\n",
      "[Training Epoch 0] Batch 3929, Loss 0.3317785859107971\n",
      "[Training Epoch 0] Batch 3930, Loss 0.30243274569511414\n",
      "[Training Epoch 0] Batch 3931, Loss 0.29707586765289307\n",
      "[Training Epoch 0] Batch 3932, Loss 0.28212231397628784\n",
      "[Training Epoch 0] Batch 3933, Loss 0.3096943795681\n",
      "[Training Epoch 0] Batch 3934, Loss 0.3432365655899048\n",
      "[Training Epoch 0] Batch 3935, Loss 0.30795568227767944\n",
      "[Training Epoch 0] Batch 3936, Loss 0.2886606752872467\n",
      "[Training Epoch 0] Batch 3937, Loss 0.28590700030326843\n",
      "[Training Epoch 0] Batch 3938, Loss 0.3159184157848358\n",
      "[Training Epoch 0] Batch 3939, Loss 0.3010654151439667\n",
      "[Training Epoch 0] Batch 3940, Loss 0.2993556559085846\n",
      "[Training Epoch 0] Batch 3941, Loss 0.3279469311237335\n",
      "[Training Epoch 0] Batch 3942, Loss 0.2852175235748291\n",
      "[Training Epoch 0] Batch 3943, Loss 0.290402889251709\n",
      "[Training Epoch 0] Batch 3944, Loss 0.308292955160141\n",
      "[Training Epoch 0] Batch 3945, Loss 0.2546807825565338\n",
      "[Training Epoch 0] Batch 3946, Loss 0.3100990355014801\n",
      "[Training Epoch 0] Batch 3947, Loss 0.2996669113636017\n",
      "[Training Epoch 0] Batch 3948, Loss 0.32247695326805115\n",
      "[Training Epoch 0] Batch 3949, Loss 0.2958199083805084\n",
      "[Training Epoch 0] Batch 3950, Loss 0.2974431812763214\n",
      "[Training Epoch 0] Batch 3951, Loss 0.3063936233520508\n",
      "[Training Epoch 0] Batch 3952, Loss 0.2937227487564087\n",
      "[Training Epoch 0] Batch 3953, Loss 0.3255308270454407\n",
      "[Training Epoch 0] Batch 3954, Loss 0.29016581177711487\n",
      "[Training Epoch 0] Batch 3955, Loss 0.3282778263092041\n",
      "[Training Epoch 0] Batch 3956, Loss 0.3316434621810913\n",
      "[Training Epoch 0] Batch 3957, Loss 0.3089020252227783\n",
      "[Training Epoch 0] Batch 3958, Loss 0.34233683347702026\n",
      "[Training Epoch 0] Batch 3959, Loss 0.3336452543735504\n",
      "[Training Epoch 0] Batch 3960, Loss 0.3243907690048218\n",
      "[Training Epoch 0] Batch 3961, Loss 0.3239816129207611\n",
      "[Training Epoch 0] Batch 3962, Loss 0.30322927236557007\n",
      "[Training Epoch 0] Batch 3963, Loss 0.28235745429992676\n",
      "[Training Epoch 0] Batch 3964, Loss 0.3126217722892761\n",
      "[Training Epoch 0] Batch 3965, Loss 0.29944029450416565\n",
      "[Training Epoch 0] Batch 3966, Loss 0.3120129704475403\n",
      "[Training Epoch 0] Batch 3967, Loss 0.3118799924850464\n",
      "[Training Epoch 0] Batch 3968, Loss 0.29420673847198486\n",
      "[Training Epoch 0] Batch 3969, Loss 0.305113285779953\n",
      "[Training Epoch 0] Batch 3970, Loss 0.3213748335838318\n",
      "[Training Epoch 0] Batch 3971, Loss 0.3033483326435089\n",
      "[Training Epoch 0] Batch 3972, Loss 0.2953266203403473\n",
      "[Training Epoch 0] Batch 3973, Loss 0.3204028308391571\n",
      "[Training Epoch 0] Batch 3974, Loss 0.32604551315307617\n",
      "[Training Epoch 0] Batch 3975, Loss 0.2858363389968872\n",
      "[Training Epoch 0] Batch 3976, Loss 0.3322414755821228\n",
      "[Training Epoch 0] Batch 3977, Loss 0.2982281446456909\n",
      "[Training Epoch 0] Batch 3978, Loss 0.3076849579811096\n",
      "[Training Epoch 0] Batch 3979, Loss 0.3115893006324768\n",
      "[Training Epoch 0] Batch 3980, Loss 0.3089626729488373\n",
      "[Training Epoch 0] Batch 3981, Loss 0.32762929797172546\n",
      "[Training Epoch 0] Batch 3982, Loss 0.290302038192749\n",
      "[Training Epoch 0] Batch 3983, Loss 0.31115129590034485\n",
      "[Training Epoch 0] Batch 3984, Loss 0.30941280722618103\n",
      "[Training Epoch 0] Batch 3985, Loss 0.3219658136367798\n",
      "[Training Epoch 0] Batch 3986, Loss 0.2922722101211548\n",
      "[Training Epoch 0] Batch 3987, Loss 0.32041987776756287\n",
      "[Training Epoch 0] Batch 3988, Loss 0.29081588983535767\n",
      "[Training Epoch 0] Batch 3989, Loss 0.3251378834247589\n",
      "[Training Epoch 0] Batch 3990, Loss 0.29688242077827454\n",
      "[Training Epoch 0] Batch 3991, Loss 0.2988642156124115\n",
      "[Training Epoch 0] Batch 3992, Loss 0.318070650100708\n",
      "[Training Epoch 0] Batch 3993, Loss 0.30724042654037476\n",
      "[Training Epoch 0] Batch 3994, Loss 0.3090260326862335\n",
      "[Training Epoch 0] Batch 3995, Loss 0.28760337829589844\n",
      "[Training Epoch 0] Batch 3996, Loss 0.2973669171333313\n",
      "[Training Epoch 0] Batch 3997, Loss 0.31314030289649963\n",
      "[Training Epoch 0] Batch 3998, Loss 0.30595430731773376\n",
      "[Training Epoch 0] Batch 3999, Loss 0.30870360136032104\n",
      "[Training Epoch 0] Batch 4000, Loss 0.3151865303516388\n",
      "[Training Epoch 0] Batch 4001, Loss 0.31022509932518005\n",
      "[Training Epoch 0] Batch 4002, Loss 0.2946411669254303\n",
      "[Training Epoch 0] Batch 4003, Loss 0.2841448187828064\n",
      "[Training Epoch 0] Batch 4004, Loss 0.2888084053993225\n",
      "[Training Epoch 0] Batch 4005, Loss 0.2987384796142578\n",
      "[Training Epoch 0] Batch 4006, Loss 0.28407761454582214\n",
      "[Training Epoch 0] Batch 4007, Loss 0.31485944986343384\n",
      "[Training Epoch 0] Batch 4008, Loss 0.3371134102344513\n",
      "[Training Epoch 0] Batch 4009, Loss 0.32339945435523987\n",
      "[Training Epoch 0] Batch 4010, Loss 0.3068539500236511\n",
      "[Training Epoch 0] Batch 4011, Loss 0.3058737516403198\n",
      "[Training Epoch 0] Batch 4012, Loss 0.32151150703430176\n",
      "[Training Epoch 0] Batch 4013, Loss 0.31921589374542236\n",
      "[Training Epoch 0] Batch 4014, Loss 0.2983948588371277\n",
      "[Training Epoch 0] Batch 4015, Loss 0.3048856854438782\n",
      "[Training Epoch 0] Batch 4016, Loss 0.3177996873855591\n",
      "[Training Epoch 0] Batch 4017, Loss 0.3130212724208832\n",
      "[Training Epoch 0] Batch 4018, Loss 0.28916358947753906\n",
      "[Training Epoch 0] Batch 4019, Loss 0.3321785032749176\n",
      "[Training Epoch 0] Batch 4020, Loss 0.26939302682876587\n",
      "[Training Epoch 0] Batch 4021, Loss 0.30767643451690674\n",
      "[Training Epoch 0] Batch 4022, Loss 0.3277222514152527\n",
      "[Training Epoch 0] Batch 4023, Loss 0.3019408881664276\n",
      "[Training Epoch 0] Batch 4024, Loss 0.3393446207046509\n",
      "[Training Epoch 0] Batch 4025, Loss 0.2913416922092438\n",
      "[Training Epoch 0] Batch 4026, Loss 0.25824564695358276\n",
      "[Training Epoch 0] Batch 4027, Loss 0.3234484791755676\n",
      "[Training Epoch 0] Batch 4028, Loss 0.30228379368782043\n",
      "[Training Epoch 0] Batch 4029, Loss 0.2912405729293823\n",
      "[Training Epoch 0] Batch 4030, Loss 0.3012472987174988\n",
      "[Training Epoch 0] Batch 4031, Loss 0.3445972502231598\n",
      "[Training Epoch 0] Batch 4032, Loss 0.3119397461414337\n",
      "[Training Epoch 0] Batch 4033, Loss 0.2761852443218231\n",
      "[Training Epoch 0] Batch 4034, Loss 0.33326712250709534\n",
      "[Training Epoch 0] Batch 4035, Loss 0.2956061065196991\n",
      "[Training Epoch 0] Batch 4036, Loss 0.3323124647140503\n",
      "[Training Epoch 0] Batch 4037, Loss 0.3020959794521332\n",
      "[Training Epoch 0] Batch 4038, Loss 0.3066730499267578\n",
      "[Training Epoch 0] Batch 4039, Loss 0.2878894805908203\n",
      "[Training Epoch 0] Batch 4040, Loss 0.3181536793708801\n",
      "[Training Epoch 0] Batch 4041, Loss 0.31036651134490967\n",
      "[Training Epoch 0] Batch 4042, Loss 0.27830860018730164\n",
      "[Training Epoch 0] Batch 4043, Loss 0.29528120160102844\n",
      "[Training Epoch 0] Batch 4044, Loss 0.3226162791252136\n",
      "[Training Epoch 0] Batch 4045, Loss 0.28569868206977844\n",
      "[Training Epoch 0] Batch 4046, Loss 0.31480836868286133\n",
      "[Training Epoch 0] Batch 4047, Loss 0.33299145102500916\n",
      "[Training Epoch 0] Batch 4048, Loss 0.3125472366809845\n",
      "[Training Epoch 0] Batch 4049, Loss 0.316204696893692\n",
      "[Training Epoch 0] Batch 4050, Loss 0.30710211396217346\n",
      "[Training Epoch 0] Batch 4051, Loss 0.3332550525665283\n",
      "[Training Epoch 0] Batch 4052, Loss 0.3204503655433655\n",
      "[Training Epoch 0] Batch 4053, Loss 0.32903480529785156\n",
      "[Training Epoch 0] Batch 4054, Loss 0.3268868327140808\n",
      "[Training Epoch 0] Batch 4055, Loss 0.3219447433948517\n",
      "[Training Epoch 0] Batch 4056, Loss 0.2912900447845459\n",
      "[Training Epoch 0] Batch 4057, Loss 0.28812381625175476\n",
      "[Training Epoch 0] Batch 4058, Loss 0.3048982322216034\n",
      "[Training Epoch 0] Batch 4059, Loss 0.3041827976703644\n",
      "[Training Epoch 0] Batch 4060, Loss 0.32620468735694885\n",
      "[Training Epoch 0] Batch 4061, Loss 0.3252996802330017\n",
      "[Training Epoch 0] Batch 4062, Loss 0.28571617603302\n",
      "[Training Epoch 0] Batch 4063, Loss 0.29625436663627625\n",
      "[Training Epoch 0] Batch 4064, Loss 0.33987611532211304\n",
      "[Training Epoch 0] Batch 4065, Loss 0.3143746852874756\n",
      "[Training Epoch 0] Batch 4066, Loss 0.3057538866996765\n",
      "[Training Epoch 0] Batch 4067, Loss 0.30538827180862427\n",
      "[Training Epoch 0] Batch 4068, Loss 0.2974056303501129\n",
      "[Training Epoch 0] Batch 4069, Loss 0.3044317960739136\n",
      "[Training Epoch 0] Batch 4070, Loss 0.300344854593277\n",
      "[Training Epoch 0] Batch 4071, Loss 0.3016522526741028\n",
      "[Training Epoch 0] Batch 4072, Loss 0.31409594416618347\n",
      "[Training Epoch 0] Batch 4073, Loss 0.3083667755126953\n",
      "[Training Epoch 0] Batch 4074, Loss 0.3077896535396576\n",
      "[Training Epoch 0] Batch 4075, Loss 0.3014523983001709\n",
      "[Training Epoch 0] Batch 4076, Loss 0.33154791593551636\n",
      "[Training Epoch 0] Batch 4077, Loss 0.33202147483825684\n",
      "[Training Epoch 0] Batch 4078, Loss 0.34273454546928406\n",
      "[Training Epoch 0] Batch 4079, Loss 0.29478275775909424\n",
      "[Training Epoch 0] Batch 4080, Loss 0.29778096079826355\n",
      "[Training Epoch 0] Batch 4081, Loss 0.30179524421691895\n",
      "[Training Epoch 0] Batch 4082, Loss 0.3268379867076874\n",
      "[Training Epoch 0] Batch 4083, Loss 0.3088028132915497\n",
      "[Training Epoch 0] Batch 4084, Loss 0.3378426730632782\n",
      "[Training Epoch 0] Batch 4085, Loss 0.30280861258506775\n",
      "[Training Epoch 0] Batch 4086, Loss 0.26892173290252686\n",
      "[Training Epoch 0] Batch 4087, Loss 0.29924216866493225\n",
      "[Training Epoch 0] Batch 4088, Loss 0.2897260785102844\n",
      "[Training Epoch 0] Batch 4089, Loss 0.2730010151863098\n",
      "[Training Epoch 0] Batch 4090, Loss 0.32276293635368347\n",
      "[Training Epoch 0] Batch 4091, Loss 0.3295545279979706\n",
      "[Training Epoch 0] Batch 4092, Loss 0.31528976559638977\n",
      "[Training Epoch 0] Batch 4093, Loss 0.2978188097476959\n",
      "[Training Epoch 0] Batch 4094, Loss 0.31876233220100403\n",
      "[Training Epoch 0] Batch 4095, Loss 0.30752086639404297\n",
      "[Training Epoch 0] Batch 4096, Loss 0.29371893405914307\n",
      "[Training Epoch 0] Batch 4097, Loss 0.31668588519096375\n",
      "[Training Epoch 0] Batch 4098, Loss 0.3176746964454651\n",
      "[Training Epoch 0] Batch 4099, Loss 0.32875677943229675\n",
      "[Training Epoch 0] Batch 4100, Loss 0.33960917592048645\n",
      "[Training Epoch 0] Batch 4101, Loss 0.28893691301345825\n",
      "[Training Epoch 0] Batch 4102, Loss 0.2947428226470947\n",
      "[Training Epoch 0] Batch 4103, Loss 0.33267155289649963\n",
      "[Training Epoch 0] Batch 4104, Loss 0.31085067987442017\n",
      "[Training Epoch 0] Batch 4105, Loss 0.30242910981178284\n",
      "[Training Epoch 0] Batch 4106, Loss 0.30038267374038696\n",
      "[Training Epoch 0] Batch 4107, Loss 0.2855607271194458\n",
      "[Training Epoch 0] Batch 4108, Loss 0.2981868386268616\n",
      "[Training Epoch 0] Batch 4109, Loss 0.3024521470069885\n",
      "[Training Epoch 0] Batch 4110, Loss 0.3179415464401245\n",
      "[Training Epoch 0] Batch 4111, Loss 0.3274107575416565\n",
      "[Training Epoch 0] Batch 4112, Loss 0.3021756708621979\n",
      "[Training Epoch 0] Batch 4113, Loss 0.3048176169395447\n",
      "[Training Epoch 0] Batch 4114, Loss 0.32857799530029297\n",
      "[Training Epoch 0] Batch 4115, Loss 0.32389774918556213\n",
      "[Training Epoch 0] Batch 4116, Loss 0.3228655755519867\n",
      "[Training Epoch 0] Batch 4117, Loss 0.31044432520866394\n",
      "[Training Epoch 0] Batch 4118, Loss 0.30798590183258057\n",
      "[Training Epoch 0] Batch 4119, Loss 0.3069615364074707\n",
      "[Training Epoch 0] Batch 4120, Loss 0.30333182215690613\n",
      "[Training Epoch 0] Batch 4121, Loss 0.31171008944511414\n",
      "[Training Epoch 0] Batch 4122, Loss 0.3238617181777954\n",
      "[Training Epoch 0] Batch 4123, Loss 0.30521970987319946\n",
      "[Training Epoch 0] Batch 4124, Loss 0.32231220602989197\n",
      "[Training Epoch 0] Batch 4125, Loss 0.32576805353164673\n",
      "[Training Epoch 0] Batch 4126, Loss 0.29622364044189453\n",
      "[Training Epoch 0] Batch 4127, Loss 0.286943256855011\n",
      "[Training Epoch 0] Batch 4128, Loss 0.31240344047546387\n",
      "[Training Epoch 0] Batch 4129, Loss 0.3270125091075897\n",
      "[Training Epoch 0] Batch 4130, Loss 0.32089394330978394\n",
      "[Training Epoch 0] Batch 4131, Loss 0.31290462613105774\n",
      "[Training Epoch 0] Batch 4132, Loss 0.28128373622894287\n",
      "[Training Epoch 0] Batch 4133, Loss 0.3042808771133423\n",
      "[Training Epoch 0] Batch 4134, Loss 0.3021598160266876\n",
      "[Training Epoch 0] Batch 4135, Loss 0.36211198568344116\n",
      "[Training Epoch 0] Batch 4136, Loss 0.32478582859039307\n",
      "[Training Epoch 0] Batch 4137, Loss 0.32940834760665894\n",
      "[Training Epoch 0] Batch 4138, Loss 0.31835243105888367\n",
      "[Training Epoch 0] Batch 4139, Loss 0.31747081875801086\n",
      "[Training Epoch 0] Batch 4140, Loss 0.2920721769332886\n",
      "[Training Epoch 0] Batch 4141, Loss 0.32029205560684204\n",
      "[Training Epoch 0] Batch 4142, Loss 0.2967853248119354\n",
      "[Training Epoch 0] Batch 4143, Loss 0.2845521867275238\n",
      "[Training Epoch 0] Batch 4144, Loss 0.2901880145072937\n",
      "[Training Epoch 0] Batch 4145, Loss 0.3323444426059723\n",
      "[Training Epoch 0] Batch 4146, Loss 0.2946404218673706\n",
      "[Training Epoch 0] Batch 4147, Loss 0.2969345152378082\n",
      "[Training Epoch 0] Batch 4148, Loss 0.30152180790901184\n",
      "[Training Epoch 0] Batch 4149, Loss 0.34544044733047485\n",
      "[Training Epoch 0] Batch 4150, Loss 0.2739650309085846\n",
      "[Training Epoch 0] Batch 4151, Loss 0.32595095038414\n",
      "[Training Epoch 0] Batch 4152, Loss 0.31234490871429443\n",
      "[Training Epoch 0] Batch 4153, Loss 0.3205468952655792\n",
      "[Training Epoch 0] Batch 4154, Loss 0.30335986614227295\n",
      "[Training Epoch 0] Batch 4155, Loss 0.2945265471935272\n",
      "[Training Epoch 0] Batch 4156, Loss 0.3386930227279663\n",
      "[Training Epoch 0] Batch 4157, Loss 0.328618586063385\n",
      "[Training Epoch 0] Batch 4158, Loss 0.30983439087867737\n",
      "[Training Epoch 0] Batch 4159, Loss 0.30589956045150757\n",
      "[Training Epoch 0] Batch 4160, Loss 0.33657529950141907\n",
      "[Training Epoch 0] Batch 4161, Loss 0.33647042512893677\n",
      "[Training Epoch 0] Batch 4162, Loss 0.2903379797935486\n",
      "[Training Epoch 0] Batch 4163, Loss 0.31361934542655945\n",
      "[Training Epoch 0] Batch 4164, Loss 0.3031567335128784\n",
      "[Training Epoch 0] Batch 4165, Loss 0.30986273288726807\n",
      "[Training Epoch 0] Batch 4166, Loss 0.3166327476501465\n",
      "[Training Epoch 0] Batch 4167, Loss 0.30067017674446106\n",
      "[Training Epoch 0] Batch 4168, Loss 0.31752079725265503\n",
      "[Training Epoch 0] Batch 4169, Loss 0.2814338207244873\n",
      "[Training Epoch 0] Batch 4170, Loss 0.31048858165740967\n",
      "[Training Epoch 0] Batch 4171, Loss 0.3031066060066223\n",
      "[Training Epoch 0] Batch 4172, Loss 0.27488231658935547\n",
      "[Training Epoch 0] Batch 4173, Loss 0.30585211515426636\n",
      "[Training Epoch 0] Batch 4174, Loss 0.316036194562912\n",
      "[Training Epoch 0] Batch 4175, Loss 0.31296539306640625\n",
      "[Training Epoch 0] Batch 4176, Loss 0.30196523666381836\n",
      "[Training Epoch 0] Batch 4177, Loss 0.30874618887901306\n",
      "[Training Epoch 0] Batch 4178, Loss 0.31187406182289124\n",
      "[Training Epoch 0] Batch 4179, Loss 0.29942676424980164\n",
      "[Training Epoch 0] Batch 4180, Loss 0.2933025360107422\n",
      "[Training Epoch 0] Batch 4181, Loss 0.33618444204330444\n",
      "[Training Epoch 0] Batch 4182, Loss 0.3139907121658325\n",
      "[Training Epoch 0] Batch 4183, Loss 0.30514708161354065\n",
      "[Training Epoch 0] Batch 4184, Loss 0.3188346326351166\n",
      "[Training Epoch 0] Batch 4185, Loss 0.2761121988296509\n",
      "[Training Epoch 0] Batch 4186, Loss 0.3400793671607971\n",
      "[Training Epoch 0] Batch 4187, Loss 0.3078767657279968\n",
      "[Training Epoch 0] Batch 4188, Loss 0.2976093590259552\n",
      "[Training Epoch 0] Batch 4189, Loss 0.30692023038864136\n",
      "[Training Epoch 0] Batch 4190, Loss 0.3274264633655548\n",
      "[Training Epoch 0] Batch 4191, Loss 0.3331659436225891\n",
      "[Training Epoch 0] Batch 4192, Loss 0.30129310488700867\n",
      "[Training Epoch 0] Batch 4193, Loss 0.28407251834869385\n",
      "[Training Epoch 0] Batch 4194, Loss 0.2964596748352051\n",
      "[Training Epoch 0] Batch 4195, Loss 0.331694096326828\n",
      "[Training Epoch 0] Batch 4196, Loss 0.322395384311676\n",
      "[Training Epoch 0] Batch 4197, Loss 0.308744341135025\n",
      "[Training Epoch 0] Batch 4198, Loss 0.30417388677597046\n",
      "[Training Epoch 0] Batch 4199, Loss 0.2764706313610077\n",
      "[Training Epoch 0] Batch 4200, Loss 0.29368236660957336\n",
      "[Training Epoch 0] Batch 4201, Loss 0.3089350461959839\n",
      "[Training Epoch 0] Batch 4202, Loss 0.31591418385505676\n",
      "[Training Epoch 0] Batch 4203, Loss 0.30895841121673584\n",
      "[Training Epoch 0] Batch 4204, Loss 0.3104574978351593\n",
      "[Training Epoch 0] Batch 4205, Loss 0.3002345561981201\n",
      "[Training Epoch 0] Batch 4206, Loss 0.3037712574005127\n",
      "[Training Epoch 0] Batch 4207, Loss 0.3018292784690857\n",
      "[Training Epoch 0] Batch 4208, Loss 0.308260440826416\n",
      "[Training Epoch 0] Batch 4209, Loss 0.30031946301460266\n",
      "[Training Epoch 0] Batch 4210, Loss 0.28860071301460266\n",
      "[Training Epoch 0] Batch 4211, Loss 0.2895004153251648\n",
      "[Training Epoch 0] Batch 4212, Loss 0.3184642195701599\n",
      "[Training Epoch 0] Batch 4213, Loss 0.3152076005935669\n",
      "[Training Epoch 0] Batch 4214, Loss 0.2878516912460327\n",
      "[Training Epoch 0] Batch 4215, Loss 0.32118406891822815\n",
      "[Training Epoch 0] Batch 4216, Loss 0.3452253043651581\n",
      "[Training Epoch 0] Batch 4217, Loss 0.30524882674217224\n",
      "[Training Epoch 0] Batch 4218, Loss 0.31177520751953125\n",
      "[Training Epoch 0] Batch 4219, Loss 0.3393670916557312\n",
      "[Training Epoch 0] Batch 4220, Loss 0.3079023063182831\n",
      "[Training Epoch 0] Batch 4221, Loss 0.32314735651016235\n",
      "[Training Epoch 0] Batch 4222, Loss 0.2757261097431183\n",
      "[Training Epoch 0] Batch 4223, Loss 0.2567105293273926\n",
      "[Training Epoch 0] Batch 4224, Loss 0.3263940215110779\n",
      "[Training Epoch 0] Batch 4225, Loss 0.2928495705127716\n",
      "[Training Epoch 0] Batch 4226, Loss 0.30478721857070923\n",
      "[Training Epoch 0] Batch 4227, Loss 0.2724462151527405\n",
      "[Training Epoch 0] Batch 4228, Loss 0.2827221155166626\n",
      "[Training Epoch 0] Batch 4229, Loss 0.29145094752311707\n",
      "[Training Epoch 0] Batch 4230, Loss 0.27488037943840027\n",
      "[Training Epoch 0] Batch 4231, Loss 0.34026187658309937\n",
      "[Training Epoch 0] Batch 4232, Loss 0.3191725015640259\n",
      "[Training Epoch 0] Batch 4233, Loss 0.3105708360671997\n",
      "[Training Epoch 0] Batch 4234, Loss 0.30999839305877686\n",
      "[Training Epoch 0] Batch 4235, Loss 0.3293493688106537\n",
      "[Training Epoch 0] Batch 4236, Loss 0.3392713665962219\n",
      "[Training Epoch 0] Batch 4237, Loss 0.3129838705062866\n",
      "[Training Epoch 0] Batch 4238, Loss 0.30550819635391235\n",
      "[Training Epoch 0] Batch 4239, Loss 0.3177931308746338\n",
      "[Training Epoch 0] Batch 4240, Loss 0.33139562606811523\n",
      "[Training Epoch 0] Batch 4241, Loss 0.29872927069664\n",
      "[Training Epoch 0] Batch 4242, Loss 0.32211819291114807\n",
      "[Training Epoch 0] Batch 4243, Loss 0.2961781919002533\n",
      "[Training Epoch 0] Batch 4244, Loss 0.3127683997154236\n",
      "[Training Epoch 0] Batch 4245, Loss 0.28813430666923523\n",
      "[Training Epoch 0] Batch 4246, Loss 0.3036623001098633\n",
      "[Training Epoch 0] Batch 4247, Loss 0.283666729927063\n",
      "[Training Epoch 0] Batch 4248, Loss 0.2996581792831421\n",
      "[Training Epoch 0] Batch 4249, Loss 0.31467998027801514\n",
      "[Training Epoch 0] Batch 4250, Loss 0.3192492127418518\n",
      "[Training Epoch 0] Batch 4251, Loss 0.29849088191986084\n",
      "[Training Epoch 0] Batch 4252, Loss 0.2892117500305176\n",
      "[Training Epoch 0] Batch 4253, Loss 0.3106490969657898\n",
      "[Training Epoch 0] Batch 4254, Loss 0.33536267280578613\n",
      "[Training Epoch 0] Batch 4255, Loss 0.28936097025871277\n",
      "[Training Epoch 0] Batch 4256, Loss 0.28880006074905396\n",
      "[Training Epoch 0] Batch 4257, Loss 0.3198893964290619\n",
      "[Training Epoch 0] Batch 4258, Loss 0.29233866930007935\n",
      "[Training Epoch 0] Batch 4259, Loss 0.3056936264038086\n",
      "[Training Epoch 0] Batch 4260, Loss 0.3074786961078644\n",
      "[Training Epoch 0] Batch 4261, Loss 0.3326941132545471\n",
      "[Training Epoch 0] Batch 4262, Loss 0.2795703709125519\n",
      "[Training Epoch 0] Batch 4263, Loss 0.3390886187553406\n",
      "[Training Epoch 0] Batch 4264, Loss 0.2968326508998871\n",
      "[Training Epoch 0] Batch 4265, Loss 0.3450348675251007\n",
      "[Training Epoch 0] Batch 4266, Loss 0.3233347237110138\n",
      "[Training Epoch 0] Batch 4267, Loss 0.31242677569389343\n",
      "[Training Epoch 0] Batch 4268, Loss 0.3053132891654968\n",
      "[Training Epoch 0] Batch 4269, Loss 0.30136775970458984\n",
      "[Training Epoch 0] Batch 4270, Loss 0.30264028906822205\n",
      "[Training Epoch 0] Batch 4271, Loss 0.3048866391181946\n",
      "[Training Epoch 0] Batch 4272, Loss 0.2931191921234131\n",
      "[Training Epoch 0] Batch 4273, Loss 0.3155873119831085\n",
      "[Training Epoch 0] Batch 4274, Loss 0.34103894233703613\n",
      "[Training Epoch 0] Batch 4275, Loss 0.3011193573474884\n",
      "[Training Epoch 0] Batch 4276, Loss 0.28782713413238525\n",
      "[Training Epoch 0] Batch 4277, Loss 0.29038262367248535\n",
      "[Training Epoch 0] Batch 4278, Loss 0.2875435948371887\n",
      "[Training Epoch 0] Batch 4279, Loss 0.2962234914302826\n",
      "[Training Epoch 0] Batch 4280, Loss 0.31493642926216125\n",
      "[Training Epoch 0] Batch 4281, Loss 0.2840774953365326\n",
      "[Training Epoch 0] Batch 4282, Loss 0.3082751929759979\n",
      "[Training Epoch 0] Batch 4283, Loss 0.27558213472366333\n",
      "[Training Epoch 0] Batch 4284, Loss 0.3046322464942932\n",
      "[Training Epoch 0] Batch 4285, Loss 0.290272057056427\n",
      "[Training Epoch 0] Batch 4286, Loss 0.26773589849472046\n",
      "[Training Epoch 0] Batch 4287, Loss 0.2974175214767456\n",
      "[Training Epoch 0] Batch 4288, Loss 0.3055999279022217\n",
      "[Training Epoch 0] Batch 4289, Loss 0.33169621229171753\n",
      "[Training Epoch 0] Batch 4290, Loss 0.36375901103019714\n",
      "[Training Epoch 0] Batch 4291, Loss 0.29546624422073364\n",
      "[Training Epoch 0] Batch 4292, Loss 0.3058505654335022\n",
      "[Training Epoch 0] Batch 4293, Loss 0.28259265422821045\n",
      "[Training Epoch 0] Batch 4294, Loss 0.3251250982284546\n",
      "[Training Epoch 0] Batch 4295, Loss 0.33593499660491943\n",
      "[Training Epoch 0] Batch 4296, Loss 0.28351420164108276\n",
      "[Training Epoch 0] Batch 4297, Loss 0.3060760498046875\n",
      "[Training Epoch 0] Batch 4298, Loss 0.2871638238430023\n",
      "[Training Epoch 0] Batch 4299, Loss 0.2956084609031677\n",
      "[Training Epoch 0] Batch 4300, Loss 0.2736656069755554\n",
      "[Training Epoch 0] Batch 4301, Loss 0.3182199001312256\n",
      "[Training Epoch 0] Batch 4302, Loss 0.2774703800678253\n",
      "[Training Epoch 0] Batch 4303, Loss 0.34205740690231323\n",
      "[Training Epoch 0] Batch 4304, Loss 0.3017551898956299\n",
      "[Training Epoch 0] Batch 4305, Loss 0.29191139340400696\n",
      "[Training Epoch 0] Batch 4306, Loss 0.32976818084716797\n",
      "[Training Epoch 0] Batch 4307, Loss 0.29815828800201416\n",
      "[Training Epoch 0] Batch 4308, Loss 0.29382145404815674\n",
      "[Training Epoch 0] Batch 4309, Loss 0.2955721616744995\n",
      "[Training Epoch 0] Batch 4310, Loss 0.2992793321609497\n",
      "[Training Epoch 0] Batch 4311, Loss 0.3256373405456543\n",
      "[Training Epoch 0] Batch 4312, Loss 0.288125216960907\n",
      "[Training Epoch 0] Batch 4313, Loss 0.3262873888015747\n",
      "[Training Epoch 0] Batch 4314, Loss 0.3275986611843109\n",
      "[Training Epoch 0] Batch 4315, Loss 0.30997467041015625\n",
      "[Training Epoch 0] Batch 4316, Loss 0.2915233075618744\n",
      "[Training Epoch 0] Batch 4317, Loss 0.2950098514556885\n",
      "[Training Epoch 0] Batch 4318, Loss 0.285207599401474\n",
      "[Training Epoch 0] Batch 4319, Loss 0.3276344835758209\n",
      "[Training Epoch 0] Batch 4320, Loss 0.31107228994369507\n",
      "[Training Epoch 0] Batch 4321, Loss 0.33220264315605164\n",
      "[Training Epoch 0] Batch 4322, Loss 0.3330497145652771\n",
      "[Training Epoch 0] Batch 4323, Loss 0.32188841700553894\n",
      "[Training Epoch 0] Batch 4324, Loss 0.30047842860221863\n",
      "[Training Epoch 0] Batch 4325, Loss 0.29162096977233887\n",
      "[Training Epoch 0] Batch 4326, Loss 0.324155330657959\n",
      "[Training Epoch 0] Batch 4327, Loss 0.3156372904777527\n",
      "[Training Epoch 0] Batch 4328, Loss 0.3482937216758728\n",
      "[Training Epoch 0] Batch 4329, Loss 0.30030184984207153\n",
      "[Training Epoch 0] Batch 4330, Loss 0.3100493848323822\n",
      "[Training Epoch 0] Batch 4331, Loss 0.3121262192726135\n",
      "[Training Epoch 0] Batch 4332, Loss 0.3153396546840668\n",
      "[Training Epoch 0] Batch 4333, Loss 0.3302014470100403\n",
      "[Training Epoch 0] Batch 4334, Loss 0.30782341957092285\n",
      "[Training Epoch 0] Batch 4335, Loss 0.30466723442077637\n",
      "[Training Epoch 0] Batch 4336, Loss 0.3163658380508423\n",
      "[Training Epoch 0] Batch 4337, Loss 0.2995207607746124\n",
      "[Training Epoch 0] Batch 4338, Loss 0.35278812050819397\n",
      "[Training Epoch 0] Batch 4339, Loss 0.2950500249862671\n",
      "[Training Epoch 0] Batch 4340, Loss 0.2975894808769226\n",
      "[Training Epoch 0] Batch 4341, Loss 0.3073945939540863\n",
      "[Training Epoch 0] Batch 4342, Loss 0.33817946910858154\n",
      "[Training Epoch 0] Batch 4343, Loss 0.3010285496711731\n",
      "[Training Epoch 0] Batch 4344, Loss 0.31501060724258423\n",
      "[Training Epoch 0] Batch 4345, Loss 0.3050069808959961\n",
      "[Training Epoch 0] Batch 4346, Loss 0.29729607701301575\n",
      "[Training Epoch 0] Batch 4347, Loss 0.3326634466648102\n",
      "[Training Epoch 0] Batch 4348, Loss 0.29699259996414185\n",
      "[Training Epoch 0] Batch 4349, Loss 0.309392511844635\n",
      "[Training Epoch 0] Batch 4350, Loss 0.3317982256412506\n",
      "[Training Epoch 0] Batch 4351, Loss 0.2907136082649231\n",
      "[Training Epoch 0] Batch 4352, Loss 0.2986864149570465\n",
      "[Training Epoch 0] Batch 4353, Loss 0.31763729453086853\n",
      "[Training Epoch 0] Batch 4354, Loss 0.29960986971855164\n",
      "[Training Epoch 0] Batch 4355, Loss 0.31124570965766907\n",
      "[Training Epoch 0] Batch 4356, Loss 0.31145915389060974\n",
      "[Training Epoch 0] Batch 4357, Loss 0.28240767121315\n",
      "[Training Epoch 0] Batch 4358, Loss 0.3034745752811432\n",
      "[Training Epoch 0] Batch 4359, Loss 0.30629292130470276\n",
      "[Training Epoch 0] Batch 4360, Loss 0.31338757276535034\n",
      "[Training Epoch 0] Batch 4361, Loss 0.2775392234325409\n",
      "[Training Epoch 0] Batch 4362, Loss 0.2847914695739746\n",
      "[Training Epoch 0] Batch 4363, Loss 0.28950735926628113\n",
      "[Training Epoch 0] Batch 4364, Loss 0.3116499185562134\n",
      "[Training Epoch 0] Batch 4365, Loss 0.2992846369743347\n",
      "[Training Epoch 0] Batch 4366, Loss 0.299709677696228\n",
      "[Training Epoch 0] Batch 4367, Loss 0.30575913190841675\n",
      "[Training Epoch 0] Batch 4368, Loss 0.29728126525878906\n",
      "[Training Epoch 0] Batch 4369, Loss 0.33676213026046753\n",
      "[Training Epoch 0] Batch 4370, Loss 0.3193907141685486\n",
      "[Training Epoch 0] Batch 4371, Loss 0.29509037733078003\n",
      "[Training Epoch 0] Batch 4372, Loss 0.327038049697876\n",
      "[Training Epoch 0] Batch 4373, Loss 0.27718669176101685\n",
      "[Training Epoch 0] Batch 4374, Loss 0.3430881202220917\n",
      "[Training Epoch 0] Batch 4375, Loss 0.35335713624954224\n",
      "[Training Epoch 0] Batch 4376, Loss 0.3130791187286377\n",
      "[Training Epoch 0] Batch 4377, Loss 0.3127768337726593\n",
      "[Training Epoch 0] Batch 4378, Loss 0.3083301782608032\n",
      "[Training Epoch 0] Batch 4379, Loss 0.30267760157585144\n",
      "[Training Epoch 0] Batch 4380, Loss 0.32078540325164795\n",
      "[Training Epoch 0] Batch 4381, Loss 0.28870680928230286\n",
      "[Training Epoch 0] Batch 4382, Loss 0.32399383187294006\n",
      "[Training Epoch 0] Batch 4383, Loss 0.2945430278778076\n",
      "[Training Epoch 0] Batch 4384, Loss 0.30352115631103516\n",
      "[Training Epoch 0] Batch 4385, Loss 0.3223103880882263\n",
      "[Training Epoch 0] Batch 4386, Loss 0.30652788281440735\n",
      "[Training Epoch 0] Batch 4387, Loss 0.29810136556625366\n",
      "[Training Epoch 0] Batch 4388, Loss 0.3210262954235077\n",
      "[Training Epoch 0] Batch 4389, Loss 0.2992430627346039\n",
      "[Training Epoch 0] Batch 4390, Loss 0.2890179753303528\n",
      "[Training Epoch 0] Batch 4391, Loss 0.32786235213279724\n",
      "[Training Epoch 0] Batch 4392, Loss 0.2986631393432617\n",
      "[Training Epoch 0] Batch 4393, Loss 0.3005465865135193\n",
      "[Training Epoch 0] Batch 4394, Loss 0.29392820596694946\n",
      "[Training Epoch 0] Batch 4395, Loss 0.3175114095211029\n",
      "[Training Epoch 0] Batch 4396, Loss 0.3165855407714844\n",
      "[Training Epoch 0] Batch 4397, Loss 0.31086036562919617\n",
      "[Training Epoch 0] Batch 4398, Loss 0.31487929821014404\n",
      "[Training Epoch 0] Batch 4399, Loss 0.31275829672813416\n",
      "[Training Epoch 0] Batch 4400, Loss 0.3444402813911438\n",
      "[Training Epoch 0] Batch 4401, Loss 0.3054850101470947\n",
      "[Training Epoch 0] Batch 4402, Loss 0.29624903202056885\n",
      "[Training Epoch 0] Batch 4403, Loss 0.31118521094322205\n",
      "[Training Epoch 0] Batch 4404, Loss 0.3239130973815918\n",
      "[Training Epoch 0] Batch 4405, Loss 0.32979172468185425\n",
      "[Training Epoch 0] Batch 4406, Loss 0.28954437375068665\n",
      "[Training Epoch 0] Batch 4407, Loss 0.3094339966773987\n",
      "[Training Epoch 0] Batch 4408, Loss 0.31437453627586365\n",
      "[Training Epoch 0] Batch 4409, Loss 0.32462894916534424\n",
      "[Training Epoch 0] Batch 4410, Loss 0.33693933486938477\n",
      "[Training Epoch 0] Batch 4411, Loss 0.2993839383125305\n",
      "[Training Epoch 0] Batch 4412, Loss 0.33483970165252686\n",
      "[Training Epoch 0] Batch 4413, Loss 0.32181915640830994\n",
      "[Training Epoch 0] Batch 4414, Loss 0.3374602198600769\n",
      "[Training Epoch 0] Batch 4415, Loss 0.3214188814163208\n",
      "[Training Epoch 0] Batch 4416, Loss 0.3123478889465332\n",
      "[Training Epoch 0] Batch 4417, Loss 0.309513121843338\n",
      "[Training Epoch 0] Batch 4418, Loss 0.3346208930015564\n",
      "[Training Epoch 0] Batch 4419, Loss 0.29382845759391785\n",
      "[Training Epoch 0] Batch 4420, Loss 0.27618810534477234\n",
      "[Training Epoch 0] Batch 4421, Loss 0.2885478138923645\n",
      "[Training Epoch 0] Batch 4422, Loss 0.28805553913116455\n",
      "[Training Epoch 0] Batch 4423, Loss 0.27945804595947266\n",
      "[Training Epoch 0] Batch 4424, Loss 0.3003707528114319\n",
      "[Training Epoch 0] Batch 4425, Loss 0.30823564529418945\n",
      "[Training Epoch 0] Batch 4426, Loss 0.2973193824291229\n",
      "[Training Epoch 0] Batch 4427, Loss 0.31654831767082214\n",
      "[Training Epoch 0] Batch 4428, Loss 0.31717708706855774\n",
      "[Training Epoch 0] Batch 4429, Loss 0.3051263988018036\n",
      "[Training Epoch 0] Batch 4430, Loss 0.30071139335632324\n",
      "[Training Epoch 0] Batch 4431, Loss 0.3001922369003296\n",
      "[Training Epoch 0] Batch 4432, Loss 0.3036035895347595\n",
      "[Training Epoch 0] Batch 4433, Loss 0.29977941513061523\n",
      "[Training Epoch 0] Batch 4434, Loss 0.2973186671733856\n",
      "[Training Epoch 0] Batch 4435, Loss 0.3000340759754181\n",
      "[Training Epoch 0] Batch 4436, Loss 0.31425970792770386\n",
      "[Training Epoch 0] Batch 4437, Loss 0.29809248447418213\n",
      "[Training Epoch 0] Batch 4438, Loss 0.29950854182243347\n",
      "[Training Epoch 0] Batch 4439, Loss 0.29144081473350525\n",
      "[Training Epoch 0] Batch 4440, Loss 0.3382737636566162\n",
      "[Training Epoch 0] Batch 4441, Loss 0.28439077734947205\n",
      "[Training Epoch 0] Batch 4442, Loss 0.2961481511592865\n",
      "[Training Epoch 0] Batch 4443, Loss 0.32383808493614197\n",
      "[Training Epoch 0] Batch 4444, Loss 0.3057190179824829\n",
      "[Training Epoch 0] Batch 4445, Loss 0.30930981040000916\n",
      "[Training Epoch 0] Batch 4446, Loss 0.3106139302253723\n",
      "[Training Epoch 0] Batch 4447, Loss 0.30376407504081726\n",
      "[Training Epoch 0] Batch 4448, Loss 0.286696195602417\n",
      "[Training Epoch 0] Batch 4449, Loss 0.283756822347641\n",
      "[Training Epoch 0] Batch 4450, Loss 0.3221421539783478\n",
      "[Training Epoch 0] Batch 4451, Loss 0.32453104853630066\n",
      "[Training Epoch 0] Batch 4452, Loss 0.3094453513622284\n",
      "[Training Epoch 0] Batch 4453, Loss 0.35069817304611206\n",
      "[Training Epoch 0] Batch 4454, Loss 0.30306804180145264\n",
      "[Training Epoch 0] Batch 4455, Loss 0.3238098919391632\n",
      "[Training Epoch 0] Batch 4456, Loss 0.3016391694545746\n",
      "[Training Epoch 0] Batch 4457, Loss 0.33228662610054016\n",
      "[Training Epoch 0] Batch 4458, Loss 0.300351619720459\n",
      "[Training Epoch 0] Batch 4459, Loss 0.3266432285308838\n",
      "[Training Epoch 0] Batch 4460, Loss 0.32285237312316895\n",
      "[Training Epoch 0] Batch 4461, Loss 0.296459436416626\n",
      "[Training Epoch 0] Batch 4462, Loss 0.289986789226532\n",
      "[Training Epoch 0] Batch 4463, Loss 0.2964378297328949\n",
      "[Training Epoch 0] Batch 4464, Loss 0.31188929080963135\n",
      "[Training Epoch 0] Batch 4465, Loss 0.3023877441883087\n",
      "[Training Epoch 0] Batch 4466, Loss 0.2958126962184906\n",
      "[Training Epoch 0] Batch 4467, Loss 0.2905607223510742\n",
      "[Training Epoch 0] Batch 4468, Loss 0.3131684958934784\n",
      "[Training Epoch 0] Batch 4469, Loss 0.29388439655303955\n",
      "[Training Epoch 0] Batch 4470, Loss 0.27252197265625\n",
      "[Training Epoch 0] Batch 4471, Loss 0.289964497089386\n",
      "[Training Epoch 0] Batch 4472, Loss 0.30847352743148804\n",
      "[Training Epoch 0] Batch 4473, Loss 0.2932787835597992\n",
      "[Training Epoch 0] Batch 4474, Loss 0.33773306012153625\n",
      "[Training Epoch 0] Batch 4475, Loss 0.2925603687763214\n",
      "[Training Epoch 0] Batch 4476, Loss 0.3173377513885498\n",
      "[Training Epoch 0] Batch 4477, Loss 0.2893712818622589\n",
      "[Training Epoch 0] Batch 4478, Loss 0.30646228790283203\n",
      "[Training Epoch 0] Batch 4479, Loss 0.33971697092056274\n",
      "[Training Epoch 0] Batch 4480, Loss 0.313515841960907\n",
      "[Training Epoch 0] Batch 4481, Loss 0.28469613194465637\n",
      "[Training Epoch 0] Batch 4482, Loss 0.3067567050457001\n",
      "[Training Epoch 0] Batch 4483, Loss 0.3221868872642517\n",
      "[Training Epoch 0] Batch 4484, Loss 0.3099208176136017\n",
      "[Training Epoch 0] Batch 4485, Loss 0.32123303413391113\n",
      "[Training Epoch 0] Batch 4486, Loss 0.3232651948928833\n",
      "[Training Epoch 0] Batch 4487, Loss 0.3178861141204834\n",
      "[Training Epoch 0] Batch 4488, Loss 0.2933847904205322\n",
      "[Training Epoch 0] Batch 4489, Loss 0.3049270808696747\n",
      "[Training Epoch 0] Batch 4490, Loss 0.30511999130249023\n",
      "[Training Epoch 0] Batch 4491, Loss 0.3004739582538605\n",
      "[Training Epoch 0] Batch 4492, Loss 0.3021298944950104\n",
      "[Training Epoch 0] Batch 4493, Loss 0.32530084252357483\n",
      "[Training Epoch 0] Batch 4494, Loss 0.3260229229927063\n",
      "[Training Epoch 0] Batch 4495, Loss 0.3145136535167694\n",
      "[Training Epoch 0] Batch 4496, Loss 0.2914525866508484\n",
      "[Training Epoch 0] Batch 4497, Loss 0.31987234950065613\n",
      "[Training Epoch 0] Batch 4498, Loss 0.30622363090515137\n",
      "[Training Epoch 0] Batch 4499, Loss 0.2769342362880707\n",
      "[Training Epoch 0] Batch 4500, Loss 0.31193214654922485\n",
      "[Training Epoch 0] Batch 4501, Loss 0.3028227686882019\n",
      "[Training Epoch 0] Batch 4502, Loss 0.3024013936519623\n",
      "[Training Epoch 0] Batch 4503, Loss 0.33222201466560364\n",
      "[Training Epoch 0] Batch 4504, Loss 0.3118184208869934\n",
      "[Training Epoch 0] Batch 4505, Loss 0.27761632204055786\n",
      "[Training Epoch 0] Batch 4506, Loss 0.29680874943733215\n",
      "[Training Epoch 0] Batch 4507, Loss 0.278826504945755\n",
      "[Training Epoch 0] Batch 4508, Loss 0.32128801941871643\n",
      "[Training Epoch 0] Batch 4509, Loss 0.29697495698928833\n",
      "[Training Epoch 0] Batch 4510, Loss 0.3023034334182739\n",
      "[Training Epoch 0] Batch 4511, Loss 0.34836894273757935\n",
      "[Training Epoch 0] Batch 4512, Loss 0.29246407747268677\n",
      "[Training Epoch 0] Batch 4513, Loss 0.32186830043792725\n",
      "[Training Epoch 0] Batch 4514, Loss 0.3008039593696594\n",
      "[Training Epoch 0] Batch 4515, Loss 0.2875886857509613\n",
      "[Training Epoch 0] Batch 4516, Loss 0.2878420054912567\n",
      "[Training Epoch 0] Batch 4517, Loss 0.2911539077758789\n",
      "[Training Epoch 0] Batch 4518, Loss 0.30649474263191223\n",
      "[Training Epoch 0] Batch 4519, Loss 0.30836012959480286\n",
      "[Training Epoch 0] Batch 4520, Loss 0.34133827686309814\n",
      "[Training Epoch 0] Batch 4521, Loss 0.3265739977359772\n",
      "[Training Epoch 0] Batch 4522, Loss 0.31110456585884094\n",
      "[Training Epoch 0] Batch 4523, Loss 0.32828912138938904\n",
      "[Training Epoch 0] Batch 4524, Loss 0.33336836099624634\n",
      "[Training Epoch 0] Batch 4525, Loss 0.308276891708374\n",
      "[Training Epoch 0] Batch 4526, Loss 0.32530641555786133\n",
      "[Training Epoch 0] Batch 4527, Loss 0.3332693874835968\n",
      "[Training Epoch 0] Batch 4528, Loss 0.30793994665145874\n",
      "[Training Epoch 0] Batch 4529, Loss 0.32763999700546265\n",
      "[Training Epoch 0] Batch 4530, Loss 0.32205554842948914\n",
      "[Training Epoch 0] Batch 4531, Loss 0.32035988569259644\n",
      "[Training Epoch 0] Batch 4532, Loss 0.2980746924877167\n",
      "[Training Epoch 0] Batch 4533, Loss 0.3297107219696045\n",
      "[Training Epoch 0] Batch 4534, Loss 0.3299800753593445\n",
      "[Training Epoch 0] Batch 4535, Loss 0.30832579731941223\n",
      "[Training Epoch 0] Batch 4536, Loss 0.32792091369628906\n",
      "[Training Epoch 0] Batch 4537, Loss 0.28963348269462585\n",
      "[Training Epoch 0] Batch 4538, Loss 0.2733323872089386\n",
      "[Training Epoch 0] Batch 4539, Loss 0.31524187326431274\n",
      "[Training Epoch 0] Batch 4540, Loss 0.2965766191482544\n",
      "[Training Epoch 0] Batch 4541, Loss 0.28620079159736633\n",
      "[Training Epoch 0] Batch 4542, Loss 0.29689615964889526\n",
      "[Training Epoch 0] Batch 4543, Loss 0.300307035446167\n",
      "[Training Epoch 0] Batch 4544, Loss 0.2830376625061035\n",
      "[Training Epoch 0] Batch 4545, Loss 0.3389156758785248\n",
      "[Training Epoch 0] Batch 4546, Loss 0.3207627236843109\n",
      "[Training Epoch 0] Batch 4547, Loss 0.3079576790332794\n",
      "[Training Epoch 0] Batch 4548, Loss 0.29793331027030945\n",
      "[Training Epoch 0] Batch 4549, Loss 0.31771332025527954\n",
      "[Training Epoch 0] Batch 4550, Loss 0.2930924892425537\n",
      "[Training Epoch 0] Batch 4551, Loss 0.3008495271205902\n",
      "[Training Epoch 0] Batch 4552, Loss 0.3211280405521393\n",
      "[Training Epoch 0] Batch 4553, Loss 0.29626962542533875\n",
      "[Training Epoch 0] Batch 4554, Loss 0.3090716004371643\n",
      "[Training Epoch 0] Batch 4555, Loss 0.29922181367874146\n",
      "[Training Epoch 0] Batch 4556, Loss 0.32048648595809937\n",
      "[Training Epoch 0] Batch 4557, Loss 0.36701637506484985\n",
      "[Training Epoch 0] Batch 4558, Loss 0.3254578113555908\n",
      "[Training Epoch 0] Batch 4559, Loss 0.32884931564331055\n",
      "[Training Epoch 0] Batch 4560, Loss 0.2916136384010315\n",
      "[Training Epoch 0] Batch 4561, Loss 0.2902311682701111\n",
      "[Training Epoch 0] Batch 4562, Loss 0.28446513414382935\n",
      "[Training Epoch 0] Batch 4563, Loss 0.32920318841934204\n",
      "[Training Epoch 0] Batch 4564, Loss 0.31050604581832886\n",
      "[Training Epoch 0] Batch 4565, Loss 0.3096536099910736\n",
      "[Training Epoch 0] Batch 4566, Loss 0.3149021565914154\n",
      "[Training Epoch 0] Batch 4567, Loss 0.2973932921886444\n",
      "[Training Epoch 0] Batch 4568, Loss 0.281588613986969\n",
      "[Training Epoch 0] Batch 4569, Loss 0.3206256330013275\n",
      "[Training Epoch 0] Batch 4570, Loss 0.32028719782829285\n",
      "[Training Epoch 0] Batch 4571, Loss 0.31412431597709656\n",
      "[Training Epoch 0] Batch 4572, Loss 0.3091031014919281\n",
      "[Training Epoch 0] Batch 4573, Loss 0.32791998982429504\n",
      "[Training Epoch 0] Batch 4574, Loss 0.3026124835014343\n",
      "[Training Epoch 0] Batch 4575, Loss 0.3237961530685425\n",
      "[Training Epoch 0] Batch 4576, Loss 0.3107675015926361\n",
      "[Training Epoch 0] Batch 4577, Loss 0.314791738986969\n",
      "[Training Epoch 0] Batch 4578, Loss 0.3042159080505371\n",
      "[Training Epoch 0] Batch 4579, Loss 0.31356534361839294\n",
      "[Training Epoch 0] Batch 4580, Loss 0.2974626421928406\n",
      "[Training Epoch 0] Batch 4581, Loss 0.3044750988483429\n",
      "[Training Epoch 0] Batch 4582, Loss 0.27650290727615356\n",
      "[Training Epoch 0] Batch 4583, Loss 0.30888083577156067\n",
      "[Training Epoch 0] Batch 4584, Loss 0.3022770881652832\n",
      "[Training Epoch 0] Batch 4585, Loss 0.3068996071815491\n",
      "[Training Epoch 0] Batch 4586, Loss 0.3129715323448181\n",
      "[Training Epoch 0] Batch 4587, Loss 0.32230737805366516\n",
      "[Training Epoch 0] Batch 4588, Loss 0.32769230008125305\n",
      "[Training Epoch 0] Batch 4589, Loss 0.3123640716075897\n",
      "[Training Epoch 0] Batch 4590, Loss 0.3144410252571106\n",
      "[Training Epoch 0] Batch 4591, Loss 0.29427677392959595\n",
      "[Training Epoch 0] Batch 4592, Loss 0.33844074606895447\n",
      "[Training Epoch 0] Batch 4593, Loss 0.3210657238960266\n",
      "[Training Epoch 0] Batch 4594, Loss 0.3368535339832306\n",
      "[Training Epoch 0] Batch 4595, Loss 0.31435897946357727\n",
      "[Training Epoch 0] Batch 4596, Loss 0.3070704936981201\n",
      "[Training Epoch 0] Batch 4597, Loss 0.2855282425880432\n",
      "[Training Epoch 0] Batch 4598, Loss 0.26680678129196167\n",
      "[Training Epoch 0] Batch 4599, Loss 0.30136531591415405\n",
      "[Training Epoch 0] Batch 4600, Loss 0.3019304573535919\n",
      "[Training Epoch 0] Batch 4601, Loss 0.32400083541870117\n",
      "[Training Epoch 0] Batch 4602, Loss 0.332576185464859\n",
      "[Training Epoch 0] Batch 4603, Loss 0.30402493476867676\n",
      "[Training Epoch 0] Batch 4604, Loss 0.34690481424331665\n",
      "[Training Epoch 0] Batch 4605, Loss 0.30042704939842224\n",
      "[Training Epoch 0] Batch 4606, Loss 0.29015612602233887\n",
      "[Training Epoch 0] Batch 4607, Loss 0.2828892767429352\n",
      "[Training Epoch 0] Batch 4608, Loss 0.33648422360420227\n",
      "[Training Epoch 0] Batch 4609, Loss 0.2880290150642395\n",
      "[Training Epoch 0] Batch 4610, Loss 0.2554679811000824\n",
      "[Training Epoch 0] Batch 4611, Loss 0.32017576694488525\n",
      "[Training Epoch 0] Batch 4612, Loss 0.30843451619148254\n",
      "[Training Epoch 0] Batch 4613, Loss 0.3193102478981018\n",
      "[Training Epoch 0] Batch 4614, Loss 0.31072238087654114\n",
      "[Training Epoch 0] Batch 4615, Loss 0.33888566493988037\n",
      "[Training Epoch 0] Batch 4616, Loss 0.31456369161605835\n",
      "[Training Epoch 0] Batch 4617, Loss 0.31359636783599854\n",
      "[Training Epoch 0] Batch 4618, Loss 0.3129541873931885\n",
      "[Training Epoch 0] Batch 4619, Loss 0.3174276351928711\n",
      "[Training Epoch 0] Batch 4620, Loss 0.3588027060031891\n",
      "[Training Epoch 0] Batch 4621, Loss 0.3105231821537018\n",
      "[Training Epoch 0] Batch 4622, Loss 0.3162693381309509\n",
      "[Training Epoch 0] Batch 4623, Loss 0.32416799664497375\n",
      "[Training Epoch 0] Batch 4624, Loss 0.3092375695705414\n",
      "[Training Epoch 0] Batch 4625, Loss 0.30627143383026123\n",
      "[Training Epoch 0] Batch 4626, Loss 0.32209455966949463\n",
      "[Training Epoch 0] Batch 4627, Loss 0.2910843789577484\n",
      "[Training Epoch 0] Batch 4628, Loss 0.31559741497039795\n",
      "[Training Epoch 0] Batch 4629, Loss 0.31050992012023926\n",
      "[Training Epoch 0] Batch 4630, Loss 0.2941092252731323\n",
      "[Training Epoch 0] Batch 4631, Loss 0.3249707520008087\n",
      "[Training Epoch 0] Batch 4632, Loss 0.293558269739151\n",
      "[Training Epoch 0] Batch 4633, Loss 0.3491465151309967\n",
      "[Training Epoch 0] Batch 4634, Loss 0.34776365756988525\n",
      "[Training Epoch 0] Batch 4635, Loss 0.2894527018070221\n",
      "[Training Epoch 0] Batch 4636, Loss 0.2666655480861664\n",
      "[Training Epoch 0] Batch 4637, Loss 0.31550008058547974\n",
      "[Training Epoch 0] Batch 4638, Loss 0.30454257130622864\n",
      "[Training Epoch 0] Batch 4639, Loss 0.31807777285575867\n",
      "[Training Epoch 0] Batch 4640, Loss 0.3021048903465271\n",
      "[Training Epoch 0] Batch 4641, Loss 0.3048642575740814\n",
      "[Training Epoch 0] Batch 4642, Loss 0.2974514663219452\n",
      "[Training Epoch 0] Batch 4643, Loss 0.279885858297348\n",
      "[Training Epoch 0] Batch 4644, Loss 0.3055158853530884\n",
      "[Training Epoch 0] Batch 4645, Loss 0.25619134306907654\n",
      "[Training Epoch 0] Batch 4646, Loss 0.28622594475746155\n",
      "[Training Epoch 0] Batch 4647, Loss 0.2854779064655304\n",
      "[Training Epoch 0] Batch 4648, Loss 0.33485132455825806\n",
      "[Training Epoch 0] Batch 4649, Loss 0.28472813963890076\n",
      "[Training Epoch 0] Batch 4650, Loss 0.32328560948371887\n",
      "[Training Epoch 0] Batch 4651, Loss 0.31715530157089233\n",
      "[Training Epoch 0] Batch 4652, Loss 0.29927173256874084\n",
      "[Training Epoch 0] Batch 4653, Loss 0.2776697874069214\n",
      "[Training Epoch 0] Batch 4654, Loss 0.3257529139518738\n",
      "[Training Epoch 0] Batch 4655, Loss 0.31602543592453003\n",
      "[Training Epoch 0] Batch 4656, Loss 0.3123173415660858\n",
      "[Training Epoch 0] Batch 4657, Loss 0.3069624900817871\n",
      "[Training Epoch 0] Batch 4658, Loss 0.3126624822616577\n",
      "[Training Epoch 0] Batch 4659, Loss 0.29372724890708923\n",
      "[Training Epoch 0] Batch 4660, Loss 0.32093724608421326\n",
      "[Training Epoch 0] Batch 4661, Loss 0.2934505343437195\n",
      "[Training Epoch 0] Batch 4662, Loss 0.2903072237968445\n",
      "[Training Epoch 0] Batch 4663, Loss 0.31949537992477417\n",
      "[Training Epoch 0] Batch 4664, Loss 0.2893199920654297\n",
      "[Training Epoch 0] Batch 4665, Loss 0.31121647357940674\n",
      "[Training Epoch 0] Batch 4666, Loss 0.30941611528396606\n",
      "[Training Epoch 0] Batch 4667, Loss 0.28549474477767944\n",
      "[Training Epoch 0] Batch 4668, Loss 0.3144472539424896\n",
      "[Training Epoch 0] Batch 4669, Loss 0.2843782603740692\n",
      "[Training Epoch 0] Batch 4670, Loss 0.3317972421646118\n",
      "[Training Epoch 0] Batch 4671, Loss 0.3513961434364319\n",
      "[Training Epoch 0] Batch 4672, Loss 0.3324128985404968\n",
      "[Training Epoch 0] Batch 4673, Loss 0.32405537366867065\n",
      "[Training Epoch 0] Batch 4674, Loss 0.3357992172241211\n",
      "[Training Epoch 0] Batch 4675, Loss 0.30906540155410767\n",
      "[Training Epoch 0] Batch 4676, Loss 0.3077630400657654\n",
      "[Training Epoch 0] Batch 4677, Loss 0.2839668095111847\n",
      "[Training Epoch 0] Batch 4678, Loss 0.30143624544143677\n",
      "[Training Epoch 0] Batch 4679, Loss 0.33794116973876953\n",
      "[Training Epoch 0] Batch 4680, Loss 0.30533096194267273\n",
      "[Training Epoch 0] Batch 4681, Loss 0.3025050163269043\n",
      "[Training Epoch 0] Batch 4682, Loss 0.3159947991371155\n",
      "[Training Epoch 0] Batch 4683, Loss 0.2901516258716583\n",
      "[Training Epoch 0] Batch 4684, Loss 0.32865065336227417\n",
      "[Training Epoch 0] Batch 4685, Loss 0.3093917965888977\n",
      "[Training Epoch 0] Batch 4686, Loss 0.30530595779418945\n",
      "[Training Epoch 0] Batch 4687, Loss 0.30897223949432373\n",
      "[Training Epoch 0] Batch 4688, Loss 0.30224937200546265\n",
      "[Training Epoch 0] Batch 4689, Loss 0.29276397824287415\n",
      "[Training Epoch 0] Batch 4690, Loss 0.3191089928150177\n",
      "[Training Epoch 0] Batch 4691, Loss 0.30499306321144104\n",
      "[Training Epoch 0] Batch 4692, Loss 0.3078118562698364\n",
      "[Training Epoch 0] Batch 4693, Loss 0.3187382221221924\n",
      "[Training Epoch 0] Batch 4694, Loss 0.3065253496170044\n",
      "[Training Epoch 0] Batch 4695, Loss 0.30115872621536255\n",
      "[Training Epoch 0] Batch 4696, Loss 0.3444400131702423\n",
      "[Training Epoch 0] Batch 4697, Loss 0.30458107590675354\n",
      "[Training Epoch 0] Batch 4698, Loss 0.3271436095237732\n",
      "[Training Epoch 0] Batch 4699, Loss 0.3216506838798523\n",
      "[Training Epoch 0] Batch 4700, Loss 0.28752821683883667\n",
      "[Training Epoch 0] Batch 4701, Loss 0.3221874535083771\n",
      "[Training Epoch 0] Batch 4702, Loss 0.30293896794319153\n",
      "[Training Epoch 0] Batch 4703, Loss 0.28405773639678955\n",
      "[Training Epoch 0] Batch 4704, Loss 0.30278587341308594\n",
      "[Training Epoch 0] Batch 4705, Loss 0.29431629180908203\n",
      "[Training Epoch 0] Batch 4706, Loss 0.30509668588638306\n",
      "[Training Epoch 0] Batch 4707, Loss 0.3082238733768463\n",
      "[Training Epoch 0] Batch 4708, Loss 0.3226640820503235\n",
      "[Training Epoch 0] Batch 4709, Loss 0.29500070214271545\n",
      "[Training Epoch 0] Batch 4710, Loss 0.31327134370803833\n",
      "[Training Epoch 0] Batch 4711, Loss 0.2990798354148865\n",
      "[Training Epoch 0] Batch 4712, Loss 0.2918373644351959\n",
      "[Training Epoch 0] Batch 4713, Loss 0.3197995126247406\n",
      "[Training Epoch 0] Batch 4714, Loss 0.2976778745651245\n",
      "[Training Epoch 0] Batch 4715, Loss 0.30143651366233826\n",
      "[Training Epoch 0] Batch 4716, Loss 0.2994062900543213\n",
      "[Training Epoch 0] Batch 4717, Loss 0.3288404047489166\n",
      "[Training Epoch 0] Batch 4718, Loss 0.31670334935188293\n",
      "[Training Epoch 0] Batch 4719, Loss 0.2860172390937805\n",
      "[Training Epoch 0] Batch 4720, Loss 0.32539862394332886\n",
      "[Training Epoch 0] Batch 4721, Loss 0.2875301241874695\n",
      "[Training Epoch 0] Batch 4722, Loss 0.2774420380592346\n",
      "[Training Epoch 0] Batch 4723, Loss 0.3096502721309662\n",
      "[Training Epoch 0] Batch 4724, Loss 0.28617578744888306\n",
      "[Training Epoch 0] Batch 4725, Loss 0.316719651222229\n",
      "[Training Epoch 0] Batch 4726, Loss 0.31625813245773315\n",
      "[Training Epoch 0] Batch 4727, Loss 0.3328910768032074\n",
      "[Training Epoch 0] Batch 4728, Loss 0.311245858669281\n",
      "[Training Epoch 0] Batch 4729, Loss 0.29794254899024963\n",
      "[Training Epoch 0] Batch 4730, Loss 0.2994531989097595\n",
      "[Training Epoch 0] Batch 4731, Loss 0.2653854489326477\n",
      "[Training Epoch 0] Batch 4732, Loss 0.298981249332428\n",
      "[Training Epoch 0] Batch 4733, Loss 0.30371353030204773\n",
      "[Training Epoch 0] Batch 4734, Loss 0.32906728982925415\n",
      "[Training Epoch 0] Batch 4735, Loss 0.31776028871536255\n",
      "[Training Epoch 0] Batch 4736, Loss 0.31888246536254883\n",
      "[Training Epoch 0] Batch 4737, Loss 0.32662877440452576\n",
      "[Training Epoch 0] Batch 4738, Loss 0.29778289794921875\n",
      "[Training Epoch 0] Batch 4739, Loss 0.302144318819046\n",
      "[Training Epoch 0] Batch 4740, Loss 0.309028685092926\n",
      "[Training Epoch 0] Batch 4741, Loss 0.3007229268550873\n",
      "[Training Epoch 0] Batch 4742, Loss 0.3192119598388672\n",
      "[Training Epoch 0] Batch 4743, Loss 0.32209131121635437\n",
      "[Training Epoch 0] Batch 4744, Loss 0.2856386601924896\n",
      "[Training Epoch 0] Batch 4745, Loss 0.3048759400844574\n",
      "[Training Epoch 0] Batch 4746, Loss 0.30445873737335205\n",
      "[Training Epoch 0] Batch 4747, Loss 0.32639825344085693\n",
      "[Training Epoch 0] Batch 4748, Loss 0.32344457507133484\n",
      "[Training Epoch 0] Batch 4749, Loss 0.3120809495449066\n",
      "[Training Epoch 0] Batch 4750, Loss 0.3394278883934021\n",
      "[Training Epoch 0] Batch 4751, Loss 0.28617388010025024\n",
      "[Training Epoch 0] Batch 4752, Loss 0.30142414569854736\n",
      "[Training Epoch 0] Batch 4753, Loss 0.321362167596817\n",
      "[Training Epoch 0] Batch 4754, Loss 0.31264346837997437\n",
      "[Training Epoch 0] Batch 4755, Loss 0.30521005392074585\n",
      "[Training Epoch 0] Batch 4756, Loss 0.3090305030345917\n",
      "[Training Epoch 0] Batch 4757, Loss 0.3157040774822235\n",
      "[Training Epoch 0] Batch 4758, Loss 0.3254958391189575\n",
      "[Training Epoch 0] Batch 4759, Loss 0.3321104645729065\n",
      "[Training Epoch 0] Batch 4760, Loss 0.2827657163143158\n",
      "[Training Epoch 0] Batch 4761, Loss 0.3119047284126282\n",
      "[Training Epoch 0] Batch 4762, Loss 0.2871425151824951\n",
      "[Training Epoch 0] Batch 4763, Loss 0.3079368770122528\n",
      "[Training Epoch 0] Batch 4764, Loss 0.30223512649536133\n",
      "[Training Epoch 0] Batch 4765, Loss 0.3372400999069214\n",
      "[Training Epoch 0] Batch 4766, Loss 0.28483155369758606\n",
      "[Training Epoch 0] Batch 4767, Loss 0.3070499300956726\n",
      "[Training Epoch 0] Batch 4768, Loss 0.31970009207725525\n",
      "[Training Epoch 0] Batch 4769, Loss 0.312715083360672\n",
      "[Training Epoch 0] Batch 4770, Loss 0.2855190336704254\n",
      "[Training Epoch 0] Batch 4771, Loss 0.28269660472869873\n",
      "[Training Epoch 0] Batch 4772, Loss 0.3299959897994995\n",
      "[Training Epoch 0] Batch 4773, Loss 0.3439962863922119\n",
      "[Training Epoch 0] Batch 4774, Loss 0.32252225279808044\n",
      "[Training Epoch 0] Batch 4775, Loss 0.3294528126716614\n",
      "[Training Epoch 0] Batch 4776, Loss 0.2918897867202759\n",
      "[Training Epoch 0] Batch 4777, Loss 0.318294495344162\n",
      "[Training Epoch 0] Batch 4778, Loss 0.29633745551109314\n",
      "[Training Epoch 0] Batch 4779, Loss 0.3264079689979553\n",
      "[Training Epoch 0] Batch 4780, Loss 0.27598533034324646\n",
      "[Training Epoch 0] Batch 4781, Loss 0.32787024974823\n",
      "[Training Epoch 0] Batch 4782, Loss 0.3257136344909668\n",
      "[Training Epoch 0] Batch 4783, Loss 0.31305524706840515\n",
      "[Training Epoch 0] Batch 4784, Loss 0.31018173694610596\n",
      "[Training Epoch 0] Batch 4785, Loss 0.31513190269470215\n",
      "[Training Epoch 0] Batch 4786, Loss 0.32034793496131897\n",
      "[Training Epoch 0] Batch 4787, Loss 0.338753879070282\n",
      "[Training Epoch 0] Batch 4788, Loss 0.29062241315841675\n",
      "[Training Epoch 0] Batch 4789, Loss 0.33266907930374146\n",
      "[Training Epoch 0] Batch 4790, Loss 0.29607272148132324\n",
      "[Training Epoch 0] Batch 4791, Loss 0.2986047565937042\n",
      "[Training Epoch 0] Batch 4792, Loss 0.3158138692378998\n",
      "[Training Epoch 0] Batch 4793, Loss 0.333230197429657\n",
      "[Training Epoch 0] Batch 4794, Loss 0.2906762361526489\n",
      "[Training Epoch 0] Batch 4795, Loss 0.35214993357658386\n",
      "[Training Epoch 0] Batch 4796, Loss 0.32072919607162476\n",
      "[Training Epoch 0] Batch 4797, Loss 0.29663321375846863\n",
      "[Training Epoch 0] Batch 4798, Loss 0.2825542092323303\n",
      "[Training Epoch 0] Batch 4799, Loss 0.2747437357902527\n",
      "[Training Epoch 0] Batch 4800, Loss 0.31129375100135803\n",
      "[Training Epoch 0] Batch 4801, Loss 0.2981921434402466\n",
      "[Training Epoch 0] Batch 4802, Loss 0.2784128189086914\n",
      "[Training Epoch 0] Batch 4803, Loss 0.32978034019470215\n",
      "[Training Epoch 0] Batch 4804, Loss 0.3068743944168091\n",
      "[Training Epoch 0] Batch 4805, Loss 0.3025698959827423\n",
      "[Training Epoch 0] Batch 4806, Loss 0.30745193362236023\n",
      "[Training Epoch 0] Batch 4807, Loss 0.3039170205593109\n",
      "[Training Epoch 0] Batch 4808, Loss 0.3148643672466278\n",
      "[Training Epoch 0] Batch 4809, Loss 0.31840330362319946\n",
      "[Training Epoch 0] Batch 4810, Loss 0.29582712054252625\n",
      "[Training Epoch 0] Batch 4811, Loss 0.3082144260406494\n",
      "[Training Epoch 0] Batch 4812, Loss 0.30477896332740784\n",
      "[Training Epoch 0] Batch 4813, Loss 0.32509660720825195\n",
      "[Training Epoch 0] Batch 4814, Loss 0.3013347387313843\n",
      "[Training Epoch 0] Batch 4815, Loss 0.3245486617088318\n",
      "[Training Epoch 0] Batch 4816, Loss 0.31058263778686523\n",
      "[Training Epoch 0] Batch 4817, Loss 0.29439839720726013\n",
      "[Training Epoch 0] Batch 4818, Loss 0.2856558859348297\n",
      "[Training Epoch 0] Batch 4819, Loss 0.2717432379722595\n",
      "[Training Epoch 0] Batch 4820, Loss 0.27877509593963623\n",
      "[Training Epoch 0] Batch 4821, Loss 0.30132704973220825\n",
      "[Training Epoch 0] Batch 4822, Loss 0.30640435218811035\n",
      "[Training Epoch 0] Batch 4823, Loss 0.301160991191864\n",
      "[Training Epoch 0] Batch 4824, Loss 0.3070080578327179\n",
      "[Training Epoch 0] Batch 4825, Loss 0.2971450686454773\n",
      "[Training Epoch 0] Batch 4826, Loss 0.362332284450531\n",
      "[Training Epoch 0] Batch 4827, Loss 0.28428882360458374\n",
      "[Training Epoch 0] Batch 4828, Loss 0.2958720922470093\n",
      "[Training Epoch 0] Batch 4829, Loss 0.33449774980545044\n",
      "[Training Epoch 0] Batch 4830, Loss 0.3244483768939972\n",
      "[Training Epoch 0] Batch 4831, Loss 0.3224482834339142\n",
      "[Training Epoch 0] Batch 4832, Loss 0.31957945227622986\n",
      "[Training Epoch 0] Batch 4833, Loss 0.315543532371521\n",
      "[Training Epoch 0] Batch 4834, Loss 0.3249097466468811\n",
      "[Training Epoch 0] Batch 4835, Loss 0.3312239646911621\n",
      "[Training Epoch 0] Batch 4836, Loss 0.29577669501304626\n",
      "[Training Epoch 0] Batch 4837, Loss 0.2963739037513733\n",
      "[Training Epoch 0] Batch 4838, Loss 0.317827045917511\n",
      "[Training Epoch 0] Batch 4839, Loss 0.3148021101951599\n",
      "[Training Epoch 0] Batch 4840, Loss 0.3005114197731018\n",
      "[Training Epoch 0] Batch 4841, Loss 0.3102515935897827\n",
      "[Training Epoch 0] Batch 4842, Loss 0.3264441192150116\n",
      "[Training Epoch 0] Batch 4843, Loss 0.30549538135528564\n",
      "[Training Epoch 0] Batch 4844, Loss 0.31129831075668335\n",
      "[Training Epoch 0] Batch 4845, Loss 0.31334301829338074\n",
      "[Training Epoch 0] Batch 4846, Loss 0.3308929204940796\n",
      "[Training Epoch 0] Batch 4847, Loss 0.303354412317276\n",
      "[Training Epoch 0] Batch 4848, Loss 0.31514525413513184\n",
      "[Training Epoch 0] Batch 4849, Loss 0.3516562879085541\n",
      "[Training Epoch 0] Batch 4850, Loss 0.293276846408844\n",
      "[Training Epoch 0] Batch 4851, Loss 0.3468356728553772\n",
      "[Training Epoch 0] Batch 4852, Loss 0.30126115679740906\n",
      "[Training Epoch 0] Batch 4853, Loss 0.3070170283317566\n",
      "[Training Epoch 0] Batch 4854, Loss 0.3112686276435852\n",
      "[Training Epoch 0] Batch 4855, Loss 0.30813828110694885\n",
      "[Training Epoch 0] Batch 4856, Loss 0.31574100255966187\n",
      "[Training Epoch 0] Batch 4857, Loss 0.29184800386428833\n",
      "[Training Epoch 0] Batch 4858, Loss 0.30427688360214233\n",
      "[Training Epoch 0] Batch 4859, Loss 0.3174750804901123\n",
      "[Training Epoch 0] Batch 4860, Loss 0.3068576753139496\n",
      "[Training Epoch 0] Batch 4861, Loss 0.2981673777103424\n",
      "[Training Epoch 0] Batch 4862, Loss 0.3055691123008728\n",
      "[Training Epoch 0] Batch 4863, Loss 0.3514813184738159\n",
      "[Training Epoch 0] Batch 4864, Loss 0.2852400541305542\n",
      "[Training Epoch 0] Batch 4865, Loss 0.32546043395996094\n",
      "[Training Epoch 0] Batch 4866, Loss 0.3000635504722595\n",
      "[Training Epoch 0] Batch 4867, Loss 0.29564332962036133\n",
      "[Training Epoch 0] Batch 4868, Loss 0.30641329288482666\n",
      "[Training Epoch 0] Batch 4869, Loss 0.28159141540527344\n",
      "[Training Epoch 0] Batch 4870, Loss 0.3162209391593933\n",
      "[Training Epoch 0] Batch 4871, Loss 0.3080099821090698\n",
      "[Training Epoch 0] Batch 4872, Loss 0.3051874041557312\n",
      "[Training Epoch 0] Batch 4873, Loss 0.29291486740112305\n",
      "[Training Epoch 0] Batch 4874, Loss 0.3337017595767975\n",
      "[Training Epoch 0] Batch 4875, Loss 0.2788360118865967\n",
      "[Training Epoch 0] Batch 4876, Loss 0.3339315354824066\n",
      "[Training Epoch 0] Batch 4877, Loss 0.3314877450466156\n",
      "[Training Epoch 0] Batch 4878, Loss 0.3334655165672302\n",
      "[Training Epoch 0] Batch 4879, Loss 0.30745068192481995\n",
      "[Training Epoch 0] Batch 4880, Loss 0.31562238931655884\n",
      "[Training Epoch 0] Batch 4881, Loss 0.32401391863822937\n",
      "[Training Epoch 0] Batch 4882, Loss 0.30820122361183167\n",
      "[Training Epoch 0] Batch 4883, Loss 0.3164890706539154\n",
      "[Training Epoch 0] Batch 4884, Loss 0.3349772095680237\n",
      "[Training Epoch 0] Batch 4885, Loss 0.31691625714302063\n",
      "[Training Epoch 0] Batch 4886, Loss 0.3192002773284912\n",
      "[Training Epoch 0] Batch 4887, Loss 0.30918583273887634\n",
      "[Training Epoch 0] Batch 4888, Loss 0.282724529504776\n",
      "[Training Epoch 0] Batch 4889, Loss 0.2979698181152344\n",
      "[Training Epoch 0] Batch 4890, Loss 0.3366381525993347\n",
      "[Training Epoch 0] Batch 4891, Loss 0.2937932312488556\n",
      "[Training Epoch 0] Batch 4892, Loss 0.2977187931537628\n",
      "[Training Epoch 0] Batch 4893, Loss 0.29565492272377014\n",
      "[Training Epoch 0] Batch 4894, Loss 0.2785611152648926\n",
      "[Training Epoch 0] Batch 4895, Loss 0.31625181436538696\n",
      "[Training Epoch 0] Batch 4896, Loss 0.30030858516693115\n",
      "[Training Epoch 0] Batch 4897, Loss 0.3095622658729553\n",
      "[Training Epoch 0] Batch 4898, Loss 0.31980907917022705\n",
      "[Training Epoch 0] Batch 4899, Loss 0.2872698903083801\n",
      "[Training Epoch 0] Batch 4900, Loss 0.2967097759246826\n",
      "[Training Epoch 0] Batch 4901, Loss 0.3212922513484955\n",
      "[Training Epoch 0] Batch 4902, Loss 0.2980651259422302\n",
      "[Training Epoch 0] Batch 4903, Loss 0.30776143074035645\n",
      "[Training Epoch 0] Batch 4904, Loss 0.29875409603118896\n",
      "[Training Epoch 0] Batch 4905, Loss 0.28584811091423035\n",
      "[Training Epoch 0] Batch 4906, Loss 0.2917852997779846\n",
      "[Training Epoch 0] Batch 4907, Loss 0.2974966764450073\n",
      "[Training Epoch 0] Batch 4908, Loss 0.3107359707355499\n",
      "[Training Epoch 0] Batch 4909, Loss 0.3002578914165497\n",
      "[Training Epoch 0] Batch 4910, Loss 0.3384450078010559\n",
      "[Training Epoch 0] Batch 4911, Loss 0.30415114760398865\n",
      "[Training Epoch 0] Batch 4912, Loss 0.28812870383262634\n",
      "[Training Epoch 0] Batch 4913, Loss 0.3234248757362366\n",
      "[Training Epoch 0] Batch 4914, Loss 0.2898848354816437\n",
      "[Training Epoch 0] Batch 4915, Loss 0.3121180236339569\n",
      "[Training Epoch 0] Batch 4916, Loss 0.29125210642814636\n",
      "[Training Epoch 0] Batch 4917, Loss 0.31119170784950256\n",
      "[Training Epoch 0] Batch 4918, Loss 0.29513850808143616\n",
      "[Training Epoch 0] Batch 4919, Loss 0.30378231406211853\n",
      "[Training Epoch 0] Batch 4920, Loss 0.3344622552394867\n",
      "[Training Epoch 0] Batch 4921, Loss 0.3137039542198181\n",
      "[Training Epoch 0] Batch 4922, Loss 0.29837915301322937\n",
      "[Training Epoch 0] Batch 4923, Loss 0.3064900040626526\n",
      "[Training Epoch 0] Batch 4924, Loss 0.3086252808570862\n",
      "[Training Epoch 0] Batch 4925, Loss 0.27584487199783325\n",
      "[Training Epoch 0] Batch 4926, Loss 0.32391852140426636\n",
      "[Training Epoch 0] Batch 4927, Loss 0.30044928193092346\n",
      "[Training Epoch 0] Batch 4928, Loss 0.33364981412887573\n",
      "[Training Epoch 0] Batch 4929, Loss 0.2898961305618286\n",
      "[Training Epoch 0] Batch 4930, Loss 0.33017152547836304\n",
      "[Training Epoch 0] Batch 4931, Loss 0.30844372510910034\n",
      "[Training Epoch 0] Batch 4932, Loss 0.3124677538871765\n",
      "[Training Epoch 0] Batch 4933, Loss 0.2773904800415039\n",
      "[Training Epoch 0] Batch 4934, Loss 0.28324002027511597\n",
      "[Training Epoch 0] Batch 4935, Loss 0.3333015441894531\n",
      "[Training Epoch 0] Batch 4936, Loss 0.28107523918151855\n",
      "[Training Epoch 0] Batch 4937, Loss 0.32795265316963196\n",
      "[Training Epoch 0] Batch 4938, Loss 0.29493680596351624\n",
      "[Training Epoch 0] Batch 4939, Loss 0.2796761393547058\n",
      "[Training Epoch 0] Batch 4940, Loss 0.30792102217674255\n",
      "[Training Epoch 0] Batch 4941, Loss 0.32342642545700073\n",
      "[Training Epoch 0] Batch 4942, Loss 0.3099249005317688\n",
      "[Training Epoch 0] Batch 4943, Loss 0.3237725496292114\n",
      "[Training Epoch 0] Batch 4944, Loss 0.33774104714393616\n",
      "[Training Epoch 0] Batch 4945, Loss 0.28443700075149536\n",
      "[Training Epoch 0] Batch 4946, Loss 0.2898278832435608\n",
      "[Training Epoch 0] Batch 4947, Loss 0.30730804800987244\n",
      "[Training Epoch 0] Batch 4948, Loss 0.3316079378128052\n",
      "[Training Epoch 0] Batch 4949, Loss 0.27719688415527344\n",
      "[Training Epoch 0] Batch 4950, Loss 0.3349025249481201\n",
      "[Training Epoch 0] Batch 4951, Loss 0.3175225257873535\n",
      "[Training Epoch 0] Batch 4952, Loss 0.27125197649002075\n",
      "[Training Epoch 0] Batch 4953, Loss 0.31179144978523254\n",
      "[Training Epoch 0] Batch 4954, Loss 0.3131636381149292\n",
      "[Training Epoch 0] Batch 4955, Loss 0.2741365432739258\n",
      "[Training Epoch 0] Batch 4956, Loss 0.3153487741947174\n",
      "[Training Epoch 0] Batch 4957, Loss 0.2888336777687073\n",
      "[Training Epoch 0] Batch 4958, Loss 0.3271721303462982\n",
      "[Training Epoch 0] Batch 4959, Loss 0.2983861267566681\n",
      "[Training Epoch 0] Batch 4960, Loss 0.32646089792251587\n",
      "[Training Epoch 0] Batch 4961, Loss 0.30271416902542114\n",
      "[Training Epoch 0] Batch 4962, Loss 0.28224241733551025\n",
      "[Training Epoch 0] Batch 4963, Loss 0.3133849501609802\n",
      "[Training Epoch 0] Batch 4964, Loss 0.2930017411708832\n",
      "[Training Epoch 0] Batch 4965, Loss 0.30476313829421997\n",
      "[Training Epoch 0] Batch 4966, Loss 0.3168848752975464\n",
      "[Training Epoch 0] Batch 4967, Loss 0.38638836145401\n",
      "[Training Epoch 0] Batch 4968, Loss 0.3265989124774933\n",
      "[Training Epoch 0] Batch 4969, Loss 0.33177703619003296\n",
      "[Training Epoch 0] Batch 4970, Loss 0.2878478169441223\n",
      "[Training Epoch 0] Batch 4971, Loss 0.28398194909095764\n",
      "[Training Epoch 0] Batch 4972, Loss 0.2925141155719757\n",
      "[Training Epoch 0] Batch 4973, Loss 0.2820775508880615\n",
      "[Training Epoch 0] Batch 4974, Loss 0.309002161026001\n",
      "[Training Epoch 0] Batch 4975, Loss 0.30389609932899475\n",
      "[Training Epoch 0] Batch 4976, Loss 0.33199426531791687\n",
      "[Training Epoch 0] Batch 4977, Loss 0.3055611252784729\n",
      "[Training Epoch 0] Batch 4978, Loss 0.3278827667236328\n",
      "[Training Epoch 0] Batch 4979, Loss 0.3076946437358856\n",
      "[Training Epoch 0] Batch 4980, Loss 0.32025575637817383\n",
      "[Training Epoch 0] Batch 4981, Loss 0.29790443181991577\n",
      "[Training Epoch 0] Batch 4982, Loss 0.3042794167995453\n",
      "[Training Epoch 0] Batch 4983, Loss 0.32384270429611206\n",
      "[Training Epoch 0] Batch 4984, Loss 0.3287368416786194\n",
      "[Training Epoch 0] Batch 4985, Loss 0.2897418737411499\n",
      "[Training Epoch 0] Batch 4986, Loss 0.27430427074432373\n",
      "[Training Epoch 0] Batch 4987, Loss 0.31906482577323914\n",
      "[Training Epoch 0] Batch 4988, Loss 0.31515824794769287\n",
      "[Training Epoch 0] Batch 4989, Loss 0.3105631172657013\n",
      "[Training Epoch 0] Batch 4990, Loss 0.33151575922966003\n",
      "[Training Epoch 0] Batch 4991, Loss 0.30848371982574463\n",
      "[Training Epoch 0] Batch 4992, Loss 0.32771167159080505\n",
      "[Training Epoch 0] Batch 4993, Loss 0.2970101237297058\n",
      "[Training Epoch 0] Batch 4994, Loss 0.30381202697753906\n",
      "[Training Epoch 0] Batch 4995, Loss 0.3171566128730774\n",
      "[Training Epoch 0] Batch 4996, Loss 0.31233519315719604\n",
      "[Training Epoch 0] Batch 4997, Loss 0.25176405906677246\n",
      "[Training Epoch 0] Batch 4998, Loss 0.3033226430416107\n",
      "[Training Epoch 0] Batch 4999, Loss 0.2833961844444275\n",
      "[Training Epoch 0] Batch 5000, Loss 0.29625341296195984\n",
      "[Training Epoch 0] Batch 5001, Loss 0.3007960021495819\n",
      "[Training Epoch 0] Batch 5002, Loss 0.3328033685684204\n",
      "[Training Epoch 0] Batch 5003, Loss 0.3190315365791321\n",
      "[Training Epoch 0] Batch 5004, Loss 0.28693482279777527\n",
      "[Training Epoch 0] Batch 5005, Loss 0.2664843797683716\n",
      "[Training Epoch 0] Batch 5006, Loss 0.3134320378303528\n",
      "[Training Epoch 0] Batch 5007, Loss 0.29573261737823486\n",
      "[Training Epoch 0] Batch 5008, Loss 0.280166894197464\n",
      "[Training Epoch 0] Batch 5009, Loss 0.3100690543651581\n",
      "[Training Epoch 0] Batch 5010, Loss 0.28886836767196655\n",
      "[Training Epoch 0] Batch 5011, Loss 0.30894726514816284\n",
      "[Training Epoch 0] Batch 5012, Loss 0.2969609200954437\n",
      "[Training Epoch 0] Batch 5013, Loss 0.30298128724098206\n",
      "[Training Epoch 0] Batch 5014, Loss 0.2850143015384674\n",
      "[Training Epoch 0] Batch 5015, Loss 0.32139354944229126\n",
      "[Training Epoch 0] Batch 5016, Loss 0.30402374267578125\n",
      "[Training Epoch 0] Batch 5017, Loss 0.2973042130470276\n",
      "[Training Epoch 0] Batch 5018, Loss 0.27841880917549133\n",
      "[Training Epoch 0] Batch 5019, Loss 0.29960203170776367\n",
      "[Training Epoch 0] Batch 5020, Loss 0.30570822954177856\n",
      "[Training Epoch 0] Batch 5021, Loss 0.28749698400497437\n",
      "[Training Epoch 0] Batch 5022, Loss 0.2774738371372223\n",
      "[Training Epoch 0] Batch 5023, Loss 0.3392166793346405\n",
      "[Training Epoch 0] Batch 5024, Loss 0.3236677944660187\n",
      "[Training Epoch 0] Batch 5025, Loss 0.31686273217201233\n",
      "[Training Epoch 0] Batch 5026, Loss 0.25244593620300293\n",
      "[Training Epoch 0] Batch 5027, Loss 0.2956397831439972\n",
      "[Training Epoch 0] Batch 5028, Loss 0.2966724634170532\n",
      "[Training Epoch 0] Batch 5029, Loss 0.3121238052845001\n",
      "[Training Epoch 0] Batch 5030, Loss 0.283112108707428\n",
      "[Training Epoch 0] Batch 5031, Loss 0.30625972151756287\n",
      "[Training Epoch 0] Batch 5032, Loss 0.32065704464912415\n",
      "[Training Epoch 0] Batch 5033, Loss 0.33298397064208984\n",
      "[Training Epoch 0] Batch 5034, Loss 0.3079032897949219\n",
      "[Training Epoch 0] Batch 5035, Loss 0.30086517333984375\n",
      "[Training Epoch 0] Batch 5036, Loss 0.3092023730278015\n",
      "[Training Epoch 0] Batch 5037, Loss 0.2949472665786743\n",
      "[Training Epoch 0] Batch 5038, Loss 0.28778544068336487\n",
      "[Training Epoch 0] Batch 5039, Loss 0.3127760589122772\n",
      "[Training Epoch 0] Batch 5040, Loss 0.3052234947681427\n",
      "[Training Epoch 0] Batch 5041, Loss 0.3024367392063141\n",
      "[Training Epoch 0] Batch 5042, Loss 0.3299984931945801\n",
      "[Training Epoch 0] Batch 5043, Loss 0.3358291983604431\n",
      "[Training Epoch 0] Batch 5044, Loss 0.3019029498100281\n",
      "[Training Epoch 0] Batch 5045, Loss 0.27392882108688354\n",
      "[Training Epoch 0] Batch 5046, Loss 0.3081166446208954\n",
      "[Training Epoch 0] Batch 5047, Loss 0.305912047624588\n",
      "[Training Epoch 0] Batch 5048, Loss 0.3006836771965027\n",
      "[Training Epoch 0] Batch 5049, Loss 0.3056239187717438\n",
      "[Training Epoch 0] Batch 5050, Loss 0.2963942885398865\n",
      "[Training Epoch 0] Batch 5051, Loss 0.30262285470962524\n",
      "[Training Epoch 0] Batch 5052, Loss 0.33287492394447327\n",
      "[Training Epoch 0] Batch 5053, Loss 0.302505761384964\n",
      "[Training Epoch 0] Batch 5054, Loss 0.3049754500389099\n",
      "[Training Epoch 0] Batch 5055, Loss 0.301177978515625\n",
      "[Training Epoch 0] Batch 5056, Loss 0.2990342676639557\n",
      "[Training Epoch 0] Batch 5057, Loss 0.3007935583591461\n",
      "[Training Epoch 0] Batch 5058, Loss 0.3002709448337555\n",
      "[Training Epoch 0] Batch 5059, Loss 0.33317506313323975\n",
      "[Training Epoch 0] Batch 5060, Loss 0.28149154782295227\n",
      "[Training Epoch 0] Batch 5061, Loss 0.3244019150733948\n",
      "[Training Epoch 0] Batch 5062, Loss 0.2965104877948761\n",
      "[Training Epoch 0] Batch 5063, Loss 0.3171341121196747\n",
      "[Training Epoch 0] Batch 5064, Loss 0.2924823462963104\n",
      "[Training Epoch 0] Batch 5065, Loss 0.302849680185318\n",
      "[Training Epoch 0] Batch 5066, Loss 0.29988783597946167\n",
      "[Training Epoch 0] Batch 5067, Loss 0.31789371371269226\n",
      "[Training Epoch 0] Batch 5068, Loss 0.30281636118888855\n",
      "[Training Epoch 0] Batch 5069, Loss 0.32583263516426086\n",
      "[Training Epoch 0] Batch 5070, Loss 0.3305075764656067\n",
      "[Training Epoch 0] Batch 5071, Loss 0.29622894525527954\n",
      "[Training Epoch 0] Batch 5072, Loss 0.29199719429016113\n",
      "[Training Epoch 0] Batch 5073, Loss 0.29572662711143494\n",
      "[Training Epoch 0] Batch 5074, Loss 0.3231979310512543\n",
      "[Training Epoch 0] Batch 5075, Loss 0.3047494888305664\n",
      "[Training Epoch 0] Batch 5076, Loss 0.30767473578453064\n",
      "[Training Epoch 0] Batch 5077, Loss 0.309744656085968\n",
      "[Training Epoch 0] Batch 5078, Loss 0.2936457097530365\n",
      "[Training Epoch 0] Batch 5079, Loss 0.2762179970741272\n",
      "[Training Epoch 0] Batch 5080, Loss 0.317584753036499\n",
      "[Training Epoch 0] Batch 5081, Loss 0.3037351369857788\n",
      "[Training Epoch 0] Batch 5082, Loss 0.3043140172958374\n",
      "[Training Epoch 0] Batch 5083, Loss 0.3362243175506592\n",
      "[Training Epoch 0] Batch 5084, Loss 0.32079821825027466\n",
      "[Training Epoch 0] Batch 5085, Loss 0.29109641909599304\n",
      "[Training Epoch 0] Batch 5086, Loss 0.3032332956790924\n",
      "[Training Epoch 0] Batch 5087, Loss 0.3014667332172394\n",
      "[Training Epoch 0] Batch 5088, Loss 0.28242480754852295\n",
      "[Training Epoch 0] Batch 5089, Loss 0.29322031140327454\n",
      "[Training Epoch 0] Batch 5090, Loss 0.33415114879608154\n",
      "[Training Epoch 0] Batch 5091, Loss 0.3250448405742645\n",
      "[Training Epoch 0] Batch 5092, Loss 0.2840689718723297\n",
      "[Training Epoch 0] Batch 5093, Loss 0.29705610871315\n",
      "[Training Epoch 0] Batch 5094, Loss 0.31003522872924805\n",
      "[Training Epoch 0] Batch 5095, Loss 0.26910898089408875\n",
      "[Training Epoch 0] Batch 5096, Loss 0.26615601778030396\n",
      "[Training Epoch 0] Batch 5097, Loss 0.333172082901001\n",
      "[Training Epoch 0] Batch 5098, Loss 0.28381747007369995\n",
      "[Training Epoch 0] Batch 5099, Loss 0.32722240686416626\n",
      "[Training Epoch 0] Batch 5100, Loss 0.32637321949005127\n",
      "[Training Epoch 0] Batch 5101, Loss 0.30528053641319275\n",
      "[Training Epoch 0] Batch 5102, Loss 0.3271602392196655\n",
      "[Training Epoch 0] Batch 5103, Loss 0.31247568130493164\n",
      "[Training Epoch 0] Batch 5104, Loss 0.29111844301223755\n",
      "[Training Epoch 0] Batch 5105, Loss 0.27837255597114563\n",
      "[Training Epoch 0] Batch 5106, Loss 0.3266045153141022\n",
      "[Training Epoch 0] Batch 5107, Loss 0.29380106925964355\n",
      "[Training Epoch 0] Batch 5108, Loss 0.33566540479660034\n",
      "[Training Epoch 0] Batch 5109, Loss 0.3447308838367462\n",
      "[Training Epoch 0] Batch 5110, Loss 0.3182666003704071\n",
      "[Training Epoch 0] Batch 5111, Loss 0.3250476121902466\n",
      "[Training Epoch 0] Batch 5112, Loss 0.3296908140182495\n",
      "[Training Epoch 0] Batch 5113, Loss 0.34503138065338135\n",
      "[Training Epoch 0] Batch 5114, Loss 0.3328326940536499\n",
      "[Training Epoch 0] Batch 5115, Loss 0.3140105605125427\n",
      "[Training Epoch 0] Batch 5116, Loss 0.31973549723625183\n",
      "[Training Epoch 0] Batch 5117, Loss 0.36168932914733887\n",
      "[Training Epoch 0] Batch 5118, Loss 0.29261937737464905\n",
      "[Training Epoch 0] Batch 5119, Loss 0.31110700964927673\n",
      "[Training Epoch 0] Batch 5120, Loss 0.2825664281845093\n",
      "[Training Epoch 0] Batch 5121, Loss 0.31088483333587646\n",
      "[Training Epoch 0] Batch 5122, Loss 0.2830784022808075\n",
      "[Training Epoch 0] Batch 5123, Loss 0.3103208839893341\n",
      "[Training Epoch 0] Batch 5124, Loss 0.28089478611946106\n",
      "[Training Epoch 0] Batch 5125, Loss 0.3176792860031128\n",
      "[Training Epoch 0] Batch 5126, Loss 0.35985398292541504\n",
      "[Training Epoch 0] Batch 5127, Loss 0.30617544054985046\n",
      "[Training Epoch 0] Batch 5128, Loss 0.3121117353439331\n",
      "[Training Epoch 0] Batch 5129, Loss 0.3038429617881775\n",
      "[Training Epoch 0] Batch 5130, Loss 0.29721084237098694\n",
      "[Training Epoch 0] Batch 5131, Loss 0.31225866079330444\n",
      "[Training Epoch 0] Batch 5132, Loss 0.3071139454841614\n",
      "[Training Epoch 0] Batch 5133, Loss 0.2826317846775055\n",
      "[Training Epoch 0] Batch 5134, Loss 0.30375659465789795\n",
      "[Training Epoch 0] Batch 5135, Loss 0.33959099650382996\n",
      "[Training Epoch 0] Batch 5136, Loss 0.3000098764896393\n",
      "[Training Epoch 0] Batch 5137, Loss 0.29939785599708557\n",
      "[Training Epoch 0] Batch 5138, Loss 0.31750696897506714\n",
      "[Training Epoch 0] Batch 5139, Loss 0.3229712247848511\n",
      "[Training Epoch 0] Batch 5140, Loss 0.3070017695426941\n",
      "[Training Epoch 0] Batch 5141, Loss 0.29630184173583984\n",
      "[Training Epoch 0] Batch 5142, Loss 0.30930495262145996\n",
      "[Training Epoch 0] Batch 5143, Loss 0.31276702880859375\n",
      "[Training Epoch 0] Batch 5144, Loss 0.29742956161499023\n",
      "[Training Epoch 0] Batch 5145, Loss 0.34501054883003235\n",
      "[Training Epoch 0] Batch 5146, Loss 0.2833671271800995\n",
      "[Training Epoch 0] Batch 5147, Loss 0.3018917143344879\n",
      "[Training Epoch 0] Batch 5148, Loss 0.3006277084350586\n",
      "[Training Epoch 0] Batch 5149, Loss 0.30329614877700806\n",
      "[Training Epoch 0] Batch 5150, Loss 0.3046185076236725\n",
      "[Training Epoch 0] Batch 5151, Loss 0.26043540239334106\n",
      "[Training Epoch 0] Batch 5152, Loss 0.2886259853839874\n",
      "[Training Epoch 0] Batch 5153, Loss 0.3289754390716553\n",
      "[Training Epoch 0] Batch 5154, Loss 0.32237526774406433\n",
      "[Training Epoch 0] Batch 5155, Loss 0.3137913644313812\n",
      "[Training Epoch 0] Batch 5156, Loss 0.3279426395893097\n",
      "[Training Epoch 0] Batch 5157, Loss 0.3288872241973877\n",
      "[Training Epoch 0] Batch 5158, Loss 0.3315044045448303\n",
      "[Training Epoch 0] Batch 5159, Loss 0.30132269859313965\n",
      "[Training Epoch 0] Batch 5160, Loss 0.3047051429748535\n",
      "[Training Epoch 0] Batch 5161, Loss 0.31236109137535095\n",
      "[Training Epoch 0] Batch 5162, Loss 0.31487563252449036\n",
      "[Training Epoch 0] Batch 5163, Loss 0.2989090085029602\n",
      "[Training Epoch 0] Batch 5164, Loss 0.2869967520236969\n",
      "[Training Epoch 0] Batch 5165, Loss 0.3296361267566681\n",
      "[Training Epoch 0] Batch 5166, Loss 0.3293728530406952\n",
      "[Training Epoch 0] Batch 5167, Loss 0.3174053430557251\n",
      "[Training Epoch 0] Batch 5168, Loss 0.2831973135471344\n",
      "[Training Epoch 0] Batch 5169, Loss 0.3233849108219147\n",
      "[Training Epoch 0] Batch 5170, Loss 0.2985219955444336\n",
      "[Training Epoch 0] Batch 5171, Loss 0.2900160849094391\n",
      "[Training Epoch 0] Batch 5172, Loss 0.29693683981895447\n",
      "[Training Epoch 0] Batch 5173, Loss 0.29821154475212097\n",
      "[Training Epoch 0] Batch 5174, Loss 0.28876134753227234\n",
      "[Training Epoch 0] Batch 5175, Loss 0.3071432411670685\n",
      "[Training Epoch 0] Batch 5176, Loss 0.31868577003479004\n",
      "[Training Epoch 0] Batch 5177, Loss 0.27619636058807373\n",
      "[Training Epoch 0] Batch 5178, Loss 0.2964935302734375\n",
      "[Training Epoch 0] Batch 5179, Loss 0.29176265001296997\n",
      "[Training Epoch 0] Batch 5180, Loss 0.31144165992736816\n",
      "[Training Epoch 0] Batch 5181, Loss 0.3096520006656647\n",
      "[Training Epoch 0] Batch 5182, Loss 0.29721683263778687\n",
      "[Training Epoch 0] Batch 5183, Loss 0.30784159898757935\n",
      "[Training Epoch 0] Batch 5184, Loss 0.29659706354141235\n",
      "[Training Epoch 0] Batch 5185, Loss 0.28872594237327576\n",
      "[Training Epoch 0] Batch 5186, Loss 0.34248751401901245\n",
      "[Training Epoch 0] Batch 5187, Loss 0.2744717001914978\n",
      "[Training Epoch 0] Batch 5188, Loss 0.2939515709877014\n",
      "[Training Epoch 0] Batch 5189, Loss 0.30552294850349426\n",
      "[Training Epoch 0] Batch 5190, Loss 0.30583077669143677\n",
      "[Training Epoch 0] Batch 5191, Loss 0.2976624369621277\n",
      "[Training Epoch 0] Batch 5192, Loss 0.28943121433258057\n",
      "[Training Epoch 0] Batch 5193, Loss 0.30721914768218994\n",
      "[Training Epoch 0] Batch 5194, Loss 0.3147444427013397\n",
      "[Training Epoch 0] Batch 5195, Loss 0.31717413663864136\n",
      "[Training Epoch 0] Batch 5196, Loss 0.2978374660015106\n",
      "[Training Epoch 0] Batch 5197, Loss 0.2898726761341095\n",
      "[Training Epoch 0] Batch 5198, Loss 0.2789037823677063\n",
      "[Training Epoch 0] Batch 5199, Loss 0.3386000990867615\n",
      "[Training Epoch 0] Batch 5200, Loss 0.30320701003074646\n",
      "[Training Epoch 0] Batch 5201, Loss 0.3230494260787964\n",
      "[Training Epoch 0] Batch 5202, Loss 0.3204518258571625\n",
      "[Training Epoch 0] Batch 5203, Loss 0.305029034614563\n",
      "[Training Epoch 0] Batch 5204, Loss 0.34617769718170166\n",
      "[Training Epoch 0] Batch 5205, Loss 0.29057756066322327\n",
      "[Training Epoch 0] Batch 5206, Loss 0.3279171586036682\n",
      "[Training Epoch 0] Batch 5207, Loss 0.29064705967903137\n",
      "[Training Epoch 0] Batch 5208, Loss 0.3124580383300781\n",
      "[Training Epoch 0] Batch 5209, Loss 0.3019743859767914\n",
      "[Training Epoch 0] Batch 5210, Loss 0.2987673878669739\n",
      "[Training Epoch 0] Batch 5211, Loss 0.3315674066543579\n",
      "[Training Epoch 0] Batch 5212, Loss 0.310081422328949\n",
      "[Training Epoch 0] Batch 5213, Loss 0.2807427942752838\n",
      "[Training Epoch 0] Batch 5214, Loss 0.3345397412776947\n",
      "[Training Epoch 0] Batch 5215, Loss 0.28333258628845215\n",
      "[Training Epoch 0] Batch 5216, Loss 0.35270676016807556\n",
      "[Training Epoch 0] Batch 5217, Loss 0.30986320972442627\n",
      "[Training Epoch 0] Batch 5218, Loss 0.3056788444519043\n",
      "[Training Epoch 0] Batch 5219, Loss 0.305981308221817\n",
      "[Training Epoch 0] Batch 5220, Loss 0.3195968270301819\n",
      "[Training Epoch 0] Batch 5221, Loss 0.31767022609710693\n",
      "[Training Epoch 0] Batch 5222, Loss 0.3048490285873413\n",
      "[Training Epoch 0] Batch 5223, Loss 0.2956846356391907\n",
      "[Training Epoch 0] Batch 5224, Loss 0.2753990888595581\n",
      "[Training Epoch 0] Batch 5225, Loss 0.3007662892341614\n",
      "[Training Epoch 0] Batch 5226, Loss 0.3246886134147644\n",
      "[Training Epoch 0] Batch 5227, Loss 0.2983940541744232\n",
      "[Training Epoch 0] Batch 5228, Loss 0.3254391849040985\n",
      "[Training Epoch 0] Batch 5229, Loss 0.28952452540397644\n",
      "[Training Epoch 0] Batch 5230, Loss 0.3004656434059143\n",
      "[Training Epoch 0] Batch 5231, Loss 0.2959304749965668\n",
      "[Training Epoch 0] Batch 5232, Loss 0.3340556025505066\n",
      "[Training Epoch 0] Batch 5233, Loss 0.3355690538883209\n",
      "[Training Epoch 0] Batch 5234, Loss 0.3232594430446625\n",
      "[Training Epoch 0] Batch 5235, Loss 0.3011637330055237\n",
      "[Training Epoch 0] Batch 5236, Loss 0.3127060532569885\n",
      "[Training Epoch 0] Batch 5237, Loss 0.3313465118408203\n",
      "[Training Epoch 0] Batch 5238, Loss 0.3118996024131775\n",
      "[Training Epoch 0] Batch 5239, Loss 0.3352472186088562\n",
      "[Training Epoch 0] Batch 5240, Loss 0.32565540075302124\n",
      "[Training Epoch 0] Batch 5241, Loss 0.31494778394699097\n",
      "[Training Epoch 0] Batch 5242, Loss 0.3062792420387268\n",
      "[Training Epoch 0] Batch 5243, Loss 0.30889174342155457\n",
      "[Training Epoch 0] Batch 5244, Loss 0.2868952453136444\n",
      "[Training Epoch 0] Batch 5245, Loss 0.29636150598526\n",
      "[Training Epoch 0] Batch 5246, Loss 0.3073362410068512\n",
      "[Training Epoch 0] Batch 5247, Loss 0.2688007056713104\n",
      "[Training Epoch 0] Batch 5248, Loss 0.2863658666610718\n",
      "[Training Epoch 0] Batch 5249, Loss 0.31048086285591125\n",
      "[Training Epoch 0] Batch 5250, Loss 0.2916436791419983\n",
      "[Training Epoch 0] Batch 5251, Loss 0.32017987966537476\n",
      "[Training Epoch 0] Batch 5252, Loss 0.3170779049396515\n",
      "[Training Epoch 0] Batch 5253, Loss 0.308826744556427\n",
      "[Training Epoch 0] Batch 5254, Loss 0.3126899003982544\n",
      "[Training Epoch 0] Batch 5255, Loss 0.3032059669494629\n",
      "[Training Epoch 0] Batch 5256, Loss 0.3091678321361542\n",
      "[Training Epoch 0] Batch 5257, Loss 0.3452673554420471\n",
      "[Training Epoch 0] Batch 5258, Loss 0.32786983251571655\n",
      "[Training Epoch 0] Batch 5259, Loss 0.2875131070613861\n",
      "[Training Epoch 0] Batch 5260, Loss 0.31042855978012085\n",
      "[Training Epoch 0] Batch 5261, Loss 0.3285004496574402\n",
      "[Training Epoch 0] Batch 5262, Loss 0.28722912073135376\n",
      "[Training Epoch 0] Batch 5263, Loss 0.3081510365009308\n",
      "[Training Epoch 0] Batch 5264, Loss 0.3162042796611786\n",
      "[Training Epoch 0] Batch 5265, Loss 0.30867576599121094\n",
      "[Training Epoch 0] Batch 5266, Loss 0.31202876567840576\n",
      "[Training Epoch 0] Batch 5267, Loss 0.31219562888145447\n",
      "[Training Epoch 0] Batch 5268, Loss 0.31968170404434204\n",
      "[Training Epoch 0] Batch 5269, Loss 0.3009585440158844\n",
      "[Training Epoch 0] Batch 5270, Loss 0.29333046078681946\n",
      "[Training Epoch 0] Batch 5271, Loss 0.34388190507888794\n",
      "[Training Epoch 0] Batch 5272, Loss 0.29780513048171997\n",
      "[Training Epoch 0] Batch 5273, Loss 0.3190401792526245\n",
      "[Training Epoch 0] Batch 5274, Loss 0.28904542326927185\n",
      "[Training Epoch 0] Batch 5275, Loss 0.2999037504196167\n",
      "[Training Epoch 0] Batch 5276, Loss 0.3301234245300293\n",
      "[Training Epoch 0] Batch 5277, Loss 0.29248303174972534\n",
      "[Training Epoch 0] Batch 5278, Loss 0.3446478247642517\n",
      "[Training Epoch 0] Batch 5279, Loss 0.29639583826065063\n",
      "[Training Epoch 0] Batch 5280, Loss 0.28857550024986267\n",
      "[Training Epoch 0] Batch 5281, Loss 0.3173508644104004\n",
      "[Training Epoch 0] Batch 5282, Loss 0.3237868845462799\n",
      "[Training Epoch 0] Batch 5283, Loss 0.3243897259235382\n",
      "[Training Epoch 0] Batch 5284, Loss 0.2840692102909088\n",
      "[Training Epoch 0] Batch 5285, Loss 0.29973307251930237\n",
      "[Training Epoch 0] Batch 5286, Loss 0.3219147026538849\n",
      "[Training Epoch 0] Batch 5287, Loss 0.2923330068588257\n",
      "[Training Epoch 0] Batch 5288, Loss 0.2951246201992035\n",
      "[Training Epoch 0] Batch 5289, Loss 0.3473832905292511\n",
      "[Training Epoch 0] Batch 5290, Loss 0.3211577534675598\n",
      "[Training Epoch 0] Batch 5291, Loss 0.284771591424942\n",
      "[Training Epoch 0] Batch 5292, Loss 0.3429396152496338\n",
      "[Training Epoch 0] Batch 5293, Loss 0.29498106241226196\n",
      "[Training Epoch 0] Batch 5294, Loss 0.3234454393386841\n",
      "[Training Epoch 0] Batch 5295, Loss 0.284567654132843\n",
      "[Training Epoch 0] Batch 5296, Loss 0.28543922305107117\n",
      "[Training Epoch 0] Batch 5297, Loss 0.2879461646080017\n",
      "[Training Epoch 0] Batch 5298, Loss 0.2776511311531067\n",
      "[Training Epoch 0] Batch 5299, Loss 0.2975424528121948\n",
      "[Training Epoch 0] Batch 5300, Loss 0.2834905982017517\n",
      "[Training Epoch 0] Batch 5301, Loss 0.3168175518512726\n",
      "[Training Epoch 0] Batch 5302, Loss 0.28490594029426575\n",
      "[Training Epoch 0] Batch 5303, Loss 0.3331902027130127\n",
      "[Training Epoch 0] Batch 5304, Loss 0.2805802524089813\n",
      "[Training Epoch 0] Batch 5305, Loss 0.29310405254364014\n",
      "[Training Epoch 0] Batch 5306, Loss 0.318193256855011\n",
      "[Training Epoch 0] Batch 5307, Loss 0.3099687695503235\n",
      "[Training Epoch 0] Batch 5308, Loss 0.31990936398506165\n",
      "[Training Epoch 0] Batch 5309, Loss 0.3103258013725281\n",
      "[Training Epoch 0] Batch 5310, Loss 0.2938520908355713\n",
      "[Training Epoch 0] Batch 5311, Loss 0.3039686679840088\n",
      "[Training Epoch 0] Batch 5312, Loss 0.3617205321788788\n",
      "[Training Epoch 0] Batch 5313, Loss 0.32499197125434875\n",
      "[Training Epoch 0] Batch 5314, Loss 0.31627607345581055\n",
      "[Training Epoch 0] Batch 5315, Loss 0.3326836824417114\n",
      "[Training Epoch 0] Batch 5316, Loss 0.3407731056213379\n",
      "[Training Epoch 0] Batch 5317, Loss 0.266655832529068\n",
      "[Training Epoch 0] Batch 5318, Loss 0.3172289729118347\n",
      "[Training Epoch 0] Batch 5319, Loss 0.2852911651134491\n",
      "[Training Epoch 0] Batch 5320, Loss 0.2938011586666107\n",
      "[Training Epoch 0] Batch 5321, Loss 0.2937147617340088\n",
      "[Training Epoch 0] Batch 5322, Loss 0.30833080410957336\n",
      "[Training Epoch 0] Batch 5323, Loss 0.2965921461582184\n",
      "[Training Epoch 0] Batch 5324, Loss 0.3230772316455841\n",
      "[Training Epoch 0] Batch 5325, Loss 0.3195474445819855\n",
      "[Training Epoch 0] Batch 5326, Loss 0.2830435633659363\n",
      "[Training Epoch 0] Batch 5327, Loss 0.297328919172287\n",
      "[Training Epoch 0] Batch 5328, Loss 0.2737870514392853\n",
      "[Training Epoch 0] Batch 5329, Loss 0.3217196464538574\n",
      "[Training Epoch 0] Batch 5330, Loss 0.33097609877586365\n",
      "[Training Epoch 0] Batch 5331, Loss 0.325817734003067\n",
      "[Training Epoch 0] Batch 5332, Loss 0.3134559988975525\n",
      "[Training Epoch 0] Batch 5333, Loss 0.3339649438858032\n",
      "[Training Epoch 0] Batch 5334, Loss 0.28878241777420044\n",
      "[Training Epoch 0] Batch 5335, Loss 0.3080688714981079\n",
      "[Training Epoch 0] Batch 5336, Loss 0.31911787390708923\n",
      "[Training Epoch 0] Batch 5337, Loss 0.3383220136165619\n",
      "[Training Epoch 0] Batch 5338, Loss 0.3123387396335602\n",
      "[Training Epoch 0] Batch 5339, Loss 0.3316000699996948\n",
      "[Training Epoch 0] Batch 5340, Loss 0.29592734575271606\n",
      "[Training Epoch 0] Batch 5341, Loss 0.31055304408073425\n",
      "[Training Epoch 0] Batch 5342, Loss 0.29367074370384216\n",
      "[Training Epoch 0] Batch 5343, Loss 0.3066071569919586\n",
      "[Training Epoch 0] Batch 5344, Loss 0.29084762930870056\n",
      "[Training Epoch 0] Batch 5345, Loss 0.31175029277801514\n",
      "[Training Epoch 0] Batch 5346, Loss 0.29319071769714355\n",
      "[Training Epoch 0] Batch 5347, Loss 0.3011137545108795\n",
      "[Training Epoch 0] Batch 5348, Loss 0.32344913482666016\n",
      "[Training Epoch 0] Batch 5349, Loss 0.29548025131225586\n",
      "[Training Epoch 0] Batch 5350, Loss 0.3205821216106415\n",
      "[Training Epoch 0] Batch 5351, Loss 0.27357447147369385\n",
      "[Training Epoch 0] Batch 5352, Loss 0.2843884527683258\n",
      "[Training Epoch 0] Batch 5353, Loss 0.2437560260295868\n",
      "[Training Epoch 0] Batch 5354, Loss 0.3155379295349121\n",
      "[Training Epoch 0] Batch 5355, Loss 0.3155776858329773\n",
      "[Training Epoch 0] Batch 5356, Loss 0.3077984154224396\n",
      "[Training Epoch 0] Batch 5357, Loss 0.3337872326374054\n",
      "[Training Epoch 0] Batch 5358, Loss 0.30338990688323975\n",
      "[Training Epoch 0] Batch 5359, Loss 0.29691076278686523\n",
      "[Training Epoch 0] Batch 5360, Loss 0.3128988742828369\n",
      "[Training Epoch 0] Batch 5361, Loss 0.2891406714916229\n",
      "[Training Epoch 0] Batch 5362, Loss 0.30303823947906494\n",
      "[Training Epoch 0] Batch 5363, Loss 0.3082024157047272\n",
      "[Training Epoch 0] Batch 5364, Loss 0.2966322898864746\n",
      "[Training Epoch 0] Batch 5365, Loss 0.32458198070526123\n",
      "[Training Epoch 0] Batch 5366, Loss 0.2997332215309143\n",
      "[Training Epoch 0] Batch 5367, Loss 0.3106277585029602\n",
      "[Training Epoch 0] Batch 5368, Loss 0.33236393332481384\n",
      "[Training Epoch 0] Batch 5369, Loss 0.30180445313453674\n",
      "[Training Epoch 0] Batch 5370, Loss 0.3099110722541809\n",
      "[Training Epoch 0] Batch 5371, Loss 0.34025612473487854\n",
      "[Training Epoch 0] Batch 5372, Loss 0.31306350231170654\n",
      "[Training Epoch 0] Batch 5373, Loss 0.311612069606781\n",
      "[Training Epoch 0] Batch 5374, Loss 0.2970544099807739\n",
      "[Training Epoch 0] Batch 5375, Loss 0.329244464635849\n",
      "[Training Epoch 0] Batch 5376, Loss 0.2918780744075775\n",
      "[Training Epoch 0] Batch 5377, Loss 0.30005475878715515\n",
      "[Training Epoch 0] Batch 5378, Loss 0.31775593757629395\n",
      "[Training Epoch 0] Batch 5379, Loss 0.3007153272628784\n",
      "[Training Epoch 0] Batch 5380, Loss 0.3257927894592285\n",
      "[Training Epoch 0] Batch 5381, Loss 0.3412133753299713\n",
      "[Training Epoch 0] Batch 5382, Loss 0.31022781133651733\n",
      "[Training Epoch 0] Batch 5383, Loss 0.302569717168808\n",
      "[Training Epoch 0] Batch 5384, Loss 0.3238769769668579\n",
      "[Training Epoch 0] Batch 5385, Loss 0.33463314175605774\n",
      "[Training Epoch 0] Batch 5386, Loss 0.33502906560897827\n",
      "[Training Epoch 0] Batch 5387, Loss 0.2901540994644165\n",
      "[Training Epoch 0] Batch 5388, Loss 0.31007540225982666\n",
      "[Training Epoch 0] Batch 5389, Loss 0.32405373454093933\n",
      "[Training Epoch 0] Batch 5390, Loss 0.2913716435432434\n",
      "[Training Epoch 0] Batch 5391, Loss 0.29486289620399475\n",
      "[Training Epoch 0] Batch 5392, Loss 0.29575371742248535\n",
      "[Training Epoch 0] Batch 5393, Loss 0.2779519557952881\n",
      "[Training Epoch 0] Batch 5394, Loss 0.31814461946487427\n",
      "[Training Epoch 0] Batch 5395, Loss 0.27305009961128235\n",
      "[Training Epoch 0] Batch 5396, Loss 0.2981465756893158\n",
      "[Training Epoch 0] Batch 5397, Loss 0.30652564764022827\n",
      "[Training Epoch 0] Batch 5398, Loss 0.24816769361495972\n",
      "[Training Epoch 0] Batch 5399, Loss 0.27470970153808594\n",
      "[Training Epoch 0] Batch 5400, Loss 0.3103271424770355\n",
      "[Training Epoch 0] Batch 5401, Loss 0.3062812387943268\n",
      "[Training Epoch 0] Batch 5402, Loss 0.33238863945007324\n",
      "[Training Epoch 0] Batch 5403, Loss 0.31148651242256165\n",
      "[Training Epoch 0] Batch 5404, Loss 0.3013719916343689\n",
      "[Training Epoch 0] Batch 5405, Loss 0.3268654942512512\n",
      "[Training Epoch 0] Batch 5406, Loss 0.3243453800678253\n",
      "[Training Epoch 0] Batch 5407, Loss 0.3251469135284424\n",
      "[Training Epoch 0] Batch 5408, Loss 0.2981416583061218\n",
      "[Training Epoch 0] Batch 5409, Loss 0.30057862401008606\n",
      "[Training Epoch 0] Batch 5410, Loss 0.31645745038986206\n",
      "[Training Epoch 0] Batch 5411, Loss 0.3132794499397278\n",
      "[Training Epoch 0] Batch 5412, Loss 0.29972895979881287\n",
      "[Training Epoch 0] Batch 5413, Loss 0.3155922591686249\n",
      "[Training Epoch 0] Batch 5414, Loss 0.28457432985305786\n",
      "[Training Epoch 0] Batch 5415, Loss 0.3281187415122986\n",
      "[Training Epoch 0] Batch 5416, Loss 0.29052701592445374\n",
      "[Training Epoch 0] Batch 5417, Loss 0.3231939971446991\n",
      "[Training Epoch 0] Batch 5418, Loss 0.34150272607803345\n",
      "[Training Epoch 0] Batch 5419, Loss 0.29755181074142456\n",
      "[Training Epoch 0] Batch 5420, Loss 0.3238711655139923\n",
      "[Training Epoch 0] Batch 5421, Loss 0.2783892750740051\n",
      "[Training Epoch 0] Batch 5422, Loss 0.3174045979976654\n",
      "[Training Epoch 0] Batch 5423, Loss 0.2793343663215637\n",
      "[Training Epoch 0] Batch 5424, Loss 0.3020589351654053\n",
      "[Training Epoch 0] Batch 5425, Loss 0.3095768094062805\n",
      "[Training Epoch 0] Batch 5426, Loss 0.31433743238449097\n",
      "[Training Epoch 0] Batch 5427, Loss 0.30123236775398254\n",
      "[Training Epoch 0] Batch 5428, Loss 0.2778894901275635\n",
      "[Training Epoch 0] Batch 5429, Loss 0.2942200303077698\n",
      "[Training Epoch 0] Batch 5430, Loss 0.31694966554641724\n",
      "[Training Epoch 0] Batch 5431, Loss 0.2927284836769104\n",
      "[Training Epoch 0] Batch 5432, Loss 0.30930018424987793\n",
      "[Training Epoch 0] Batch 5433, Loss 0.30098462104797363\n",
      "[Training Epoch 0] Batch 5434, Loss 0.32869628071784973\n",
      "[Training Epoch 0] Batch 5435, Loss 0.309063196182251\n",
      "[Training Epoch 0] Batch 5436, Loss 0.28074416518211365\n",
      "[Training Epoch 0] Batch 5437, Loss 0.3000193238258362\n",
      "[Training Epoch 0] Batch 5438, Loss 0.3239513039588928\n",
      "[Training Epoch 0] Batch 5439, Loss 0.3149297535419464\n",
      "[Training Epoch 0] Batch 5440, Loss 0.2956674098968506\n",
      "[Training Epoch 0] Batch 5441, Loss 0.32409659028053284\n",
      "[Training Epoch 0] Batch 5442, Loss 0.32111164927482605\n",
      "[Training Epoch 0] Batch 5443, Loss 0.2981112599372864\n",
      "[Training Epoch 0] Batch 5444, Loss 0.3248472809791565\n",
      "[Training Epoch 0] Batch 5445, Loss 0.3052577078342438\n",
      "[Training Epoch 0] Batch 5446, Loss 0.28361254930496216\n",
      "[Training Epoch 0] Batch 5447, Loss 0.3024095296859741\n",
      "[Training Epoch 0] Batch 5448, Loss 0.2842322885990143\n",
      "[Training Epoch 0] Batch 5449, Loss 0.3254702687263489\n",
      "[Training Epoch 0] Batch 5450, Loss 0.2634265720844269\n",
      "[Training Epoch 0] Batch 5451, Loss 0.29123425483703613\n",
      "[Training Epoch 0] Batch 5452, Loss 0.33725786209106445\n",
      "[Training Epoch 0] Batch 5453, Loss 0.2865613102912903\n",
      "[Training Epoch 0] Batch 5454, Loss 0.31488344073295593\n",
      "[Training Epoch 0] Batch 5455, Loss 0.2986769676208496\n",
      "[Training Epoch 0] Batch 5456, Loss 0.3315994143486023\n",
      "[Training Epoch 0] Batch 5457, Loss 0.3051505982875824\n",
      "[Training Epoch 0] Batch 5458, Loss 0.28917744755744934\n",
      "[Training Epoch 0] Batch 5459, Loss 0.2942486107349396\n",
      "[Training Epoch 0] Batch 5460, Loss 0.34199732542037964\n",
      "[Training Epoch 0] Batch 5461, Loss 0.30588585138320923\n",
      "[Training Epoch 0] Batch 5462, Loss 0.309742271900177\n",
      "[Training Epoch 0] Batch 5463, Loss 0.32129180431365967\n",
      "[Training Epoch 0] Batch 5464, Loss 0.2884744703769684\n",
      "[Training Epoch 0] Batch 5465, Loss 0.32030734419822693\n",
      "[Training Epoch 0] Batch 5466, Loss 0.3010004758834839\n",
      "[Training Epoch 0] Batch 5467, Loss 0.29731497168540955\n",
      "[Training Epoch 0] Batch 5468, Loss 0.28105464577674866\n",
      "[Training Epoch 0] Batch 5469, Loss 0.3331068754196167\n",
      "[Training Epoch 0] Batch 5470, Loss 0.3098439872264862\n",
      "[Training Epoch 0] Batch 5471, Loss 0.3161022663116455\n",
      "[Training Epoch 0] Batch 5472, Loss 0.29609259963035583\n",
      "[Training Epoch 0] Batch 5473, Loss 0.28838077187538147\n",
      "[Training Epoch 0] Batch 5474, Loss 0.2931146025657654\n",
      "[Training Epoch 0] Batch 5475, Loss 0.31722772121429443\n",
      "[Training Epoch 0] Batch 5476, Loss 0.31559091806411743\n",
      "[Training Epoch 0] Batch 5477, Loss 0.31446096301078796\n",
      "[Training Epoch 0] Batch 5478, Loss 0.3129323720932007\n",
      "[Training Epoch 0] Batch 5479, Loss 0.2809598445892334\n",
      "[Training Epoch 0] Batch 5480, Loss 0.2918282449245453\n",
      "[Training Epoch 0] Batch 5481, Loss 0.3061859905719757\n",
      "[Training Epoch 0] Batch 5482, Loss 0.3003559112548828\n",
      "[Training Epoch 0] Batch 5483, Loss 0.3163294792175293\n",
      "[Training Epoch 0] Batch 5484, Loss 0.31132805347442627\n",
      "[Training Epoch 0] Batch 5485, Loss 0.2945176064968109\n",
      "[Training Epoch 0] Batch 5486, Loss 0.320654034614563\n",
      "[Training Epoch 0] Batch 5487, Loss 0.2985247075557709\n",
      "[Training Epoch 0] Batch 5488, Loss 0.28200843930244446\n",
      "[Training Epoch 0] Batch 5489, Loss 0.30449777841567993\n",
      "[Training Epoch 0] Batch 5490, Loss 0.3287985324859619\n",
      "[Training Epoch 0] Batch 5491, Loss 0.2976231276988983\n",
      "[Training Epoch 0] Batch 5492, Loss 0.3010547161102295\n",
      "[Training Epoch 0] Batch 5493, Loss 0.32456135749816895\n",
      "[Training Epoch 0] Batch 5494, Loss 0.29129457473754883\n",
      "[Training Epoch 0] Batch 5495, Loss 0.2890811562538147\n",
      "[Training Epoch 0] Batch 5496, Loss 0.311929315328598\n",
      "[Training Epoch 0] Batch 5497, Loss 0.30172836780548096\n",
      "[Training Epoch 0] Batch 5498, Loss 0.26257574558258057\n",
      "[Training Epoch 0] Batch 5499, Loss 0.3348853886127472\n",
      "[Training Epoch 0] Batch 5500, Loss 0.2725137174129486\n",
      "[Training Epoch 0] Batch 5501, Loss 0.2950596809387207\n",
      "[Training Epoch 0] Batch 5502, Loss 0.28545743227005005\n",
      "[Training Epoch 0] Batch 5503, Loss 0.31234779953956604\n",
      "[Training Epoch 0] Batch 5504, Loss 0.29828089475631714\n",
      "[Training Epoch 0] Batch 5505, Loss 0.30392107367515564\n",
      "[Training Epoch 0] Batch 5506, Loss 0.29587996006011963\n",
      "[Training Epoch 0] Batch 5507, Loss 0.3154442310333252\n",
      "[Training Epoch 0] Batch 5508, Loss 0.28204846382141113\n",
      "[Training Epoch 0] Batch 5509, Loss 0.2960355281829834\n",
      "[Training Epoch 0] Batch 5510, Loss 0.3113822340965271\n",
      "[Training Epoch 0] Batch 5511, Loss 0.28192365169525146\n",
      "[Training Epoch 0] Batch 5512, Loss 0.2756700813770294\n",
      "[Training Epoch 0] Batch 5513, Loss 0.30410581827163696\n",
      "[Training Epoch 0] Batch 5514, Loss 0.30829480290412903\n",
      "[Training Epoch 0] Batch 5515, Loss 0.3036586046218872\n",
      "[Training Epoch 0] Batch 5516, Loss 0.30990397930145264\n",
      "[Training Epoch 0] Batch 5517, Loss 0.304631769657135\n",
      "[Training Epoch 0] Batch 5518, Loss 0.26402896642684937\n",
      "[Training Epoch 0] Batch 5519, Loss 0.2888779640197754\n",
      "[Training Epoch 0] Batch 5520, Loss 0.29750698804855347\n",
      "[Training Epoch 0] Batch 5521, Loss 0.3105226755142212\n",
      "[Training Epoch 0] Batch 5522, Loss 0.299033522605896\n",
      "[Training Epoch 0] Batch 5523, Loss 0.31932947039604187\n",
      "[Training Epoch 0] Batch 5524, Loss 0.3052341043949127\n",
      "[Training Epoch 0] Batch 5525, Loss 0.2959980070590973\n",
      "[Training Epoch 0] Batch 5526, Loss 0.2864302396774292\n",
      "[Training Epoch 0] Batch 5527, Loss 0.32151880860328674\n",
      "[Training Epoch 0] Batch 5528, Loss 0.3075539767742157\n",
      "[Training Epoch 0] Batch 5529, Loss 0.29361310601234436\n",
      "[Training Epoch 0] Batch 5530, Loss 0.3024651110172272\n",
      "[Training Epoch 0] Batch 5531, Loss 0.2693924903869629\n",
      "[Training Epoch 0] Batch 5532, Loss 0.3022482991218567\n",
      "[Training Epoch 0] Batch 5533, Loss 0.3129790425300598\n",
      "[Training Epoch 0] Batch 5534, Loss 0.3153160810470581\n",
      "[Training Epoch 0] Batch 5535, Loss 0.3003949820995331\n",
      "[Training Epoch 0] Batch 5536, Loss 0.3015674650669098\n",
      "[Training Epoch 0] Batch 5537, Loss 0.3041483759880066\n",
      "[Training Epoch 0] Batch 5538, Loss 0.3043680191040039\n",
      "[Training Epoch 0] Batch 5539, Loss 0.29407966136932373\n",
      "[Training Epoch 0] Batch 5540, Loss 0.2841143310070038\n",
      "[Training Epoch 0] Batch 5541, Loss 0.28750696778297424\n",
      "[Training Epoch 0] Batch 5542, Loss 0.3370731770992279\n",
      "[Training Epoch 0] Batch 5543, Loss 0.2968440353870392\n",
      "[Training Epoch 0] Batch 5544, Loss 0.2933705449104309\n",
      "[Training Epoch 0] Batch 5545, Loss 0.34067949652671814\n",
      "[Training Epoch 0] Batch 5546, Loss 0.26023197174072266\n",
      "[Training Epoch 0] Batch 5547, Loss 0.30224478244781494\n",
      "[Training Epoch 0] Batch 5548, Loss 0.32964831590652466\n",
      "[Training Epoch 0] Batch 5549, Loss 0.3314819931983948\n",
      "[Training Epoch 0] Batch 5550, Loss 0.30603140592575073\n",
      "[Training Epoch 0] Batch 5551, Loss 0.30145835876464844\n",
      "[Training Epoch 0] Batch 5552, Loss 0.29434195160865784\n",
      "[Training Epoch 0] Batch 5553, Loss 0.33756381273269653\n",
      "[Training Epoch 0] Batch 5554, Loss 0.32000574469566345\n",
      "[Training Epoch 0] Batch 5555, Loss 0.29880937933921814\n",
      "[Training Epoch 0] Batch 5556, Loss 0.2847595810890198\n",
      "[Training Epoch 0] Batch 5557, Loss 0.27177318930625916\n",
      "[Training Epoch 0] Batch 5558, Loss 0.31136104464530945\n",
      "[Training Epoch 0] Batch 5559, Loss 0.32299017906188965\n",
      "[Training Epoch 0] Batch 5560, Loss 0.3198278248310089\n",
      "[Training Epoch 0] Batch 5561, Loss 0.2917790412902832\n",
      "[Training Epoch 0] Batch 5562, Loss 0.3220686614513397\n",
      "[Training Epoch 0] Batch 5563, Loss 0.3090839982032776\n",
      "[Training Epoch 0] Batch 5564, Loss 0.3346377909183502\n",
      "[Training Epoch 0] Batch 5565, Loss 0.3329034149646759\n",
      "[Training Epoch 0] Batch 5566, Loss 0.29282838106155396\n",
      "[Training Epoch 0] Batch 5567, Loss 0.2964406907558441\n",
      "[Training Epoch 0] Batch 5568, Loss 0.27466294169425964\n",
      "[Training Epoch 0] Batch 5569, Loss 0.2822585105895996\n",
      "[Training Epoch 0] Batch 5570, Loss 0.3249766230583191\n",
      "[Training Epoch 0] Batch 5571, Loss 0.30418238043785095\n",
      "[Training Epoch 0] Batch 5572, Loss 0.29470378160476685\n",
      "[Training Epoch 0] Batch 5573, Loss 0.3357408046722412\n",
      "[Training Epoch 0] Batch 5574, Loss 0.2949787378311157\n",
      "[Training Epoch 0] Batch 5575, Loss 0.31257250905036926\n",
      "[Training Epoch 0] Batch 5576, Loss 0.3154023587703705\n",
      "[Training Epoch 0] Batch 5577, Loss 0.30385512113571167\n",
      "[Training Epoch 0] Batch 5578, Loss 0.30310770869255066\n",
      "[Training Epoch 0] Batch 5579, Loss 0.30395060777664185\n",
      "[Training Epoch 0] Batch 5580, Loss 0.2938397228717804\n",
      "[Training Epoch 0] Batch 5581, Loss 0.30817291140556335\n",
      "[Training Epoch 0] Batch 5582, Loss 0.29643988609313965\n",
      "[Training Epoch 0] Batch 5583, Loss 0.3198031783103943\n",
      "[Training Epoch 0] Batch 5584, Loss 0.30100783705711365\n",
      "[Training Epoch 0] Batch 5585, Loss 0.30127888917922974\n",
      "[Training Epoch 0] Batch 5586, Loss 0.324104368686676\n",
      "[Evluating Epoch 0] HR = 0.9182, NDCG = 0.6364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan\\Desktop\\GitHub_public\\neural-collaborative-filtering\\src\\metrics.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config['num_epoch']):\n",
    "    print('Epoch {} starts !'.format(epoch))\n",
    "    print('-' * 80)\n",
    "    train_loader = sample_generator1.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    hit_ratio, ndcg = engine.evaluate(evaluate_data1, epoch_id=epoch)\n",
    "    engine.save(config['alias'], epoch, hit_ratio, ndcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

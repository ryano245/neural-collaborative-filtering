{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gmf import GMFEngine\n",
    "from mlp import MLPEngine\n",
    "from neumf import NeuMFEngine\n",
    "from data import SampleGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_config = {'alias': 'gmf_factor8neg4-implict',\n",
    "              'num_epoch': 1,\n",
    "              'batch_size': 1024,\n",
    "              # 'optimizer': 'sgd',\n",
    "              # 'sgd_lr': 1e-3,\n",
    "              # 'sgd_momentum': 0.9,\n",
    "              # 'optimizer': 'rmsprop',\n",
    "              # 'rmsprop_lr': 1e-3,\n",
    "              # 'rmsprop_alpha': 0.99,\n",
    "              # 'rmsprop_momentum': 0,\n",
    "              'optimizer': 'adam',\n",
    "              'adam_lr': 1e-3,\n",
    "              'num_users': 7985,\n",
    "              'num_items': 4498,\n",
    "              'latent_dim': 8,\n",
    "              'num_negative': 4,\n",
    "              'l2_regularization': 0, # 0.01\n",
    "              'use_cuda': False,\n",
    "              'device_id': 0,\n",
    "              'model_dir':'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6040\n",
    "3703\n",
    "----\n",
    "6040\n",
    "1878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_config = {'alias': 'mlp_factor8neg4_bz256_166432168_pretrain_reg_0.0000001',\n",
    "              'num_epoch': 200,\n",
    "              'batch_size': 256,  # 1024,\n",
    "              'optimizer': 'adam',\n",
    "              'adam_lr': 1e-3,\n",
    "              'num_users': 6040,\n",
    "              'num_items': 3706,\n",
    "              'latent_dim': 8,\n",
    "              'num_negative': 4,\n",
    "              'layers': [16,64,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
    "              'l2_regularization': 0.0000001,  # MLP model is sensitive to hyper params\n",
    "              'use_cuda': False,\n",
    "              'device_id': 7,\n",
    "              'pretrain': True,\n",
    "              'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n",
    "              'model_dir':'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_config = {'alias': 'pretrain_neumf_factor8neg4',\n",
    "                'num_epoch': 200,\n",
    "                'batch_size': 1024,\n",
    "                'optimizer': 'adam',\n",
    "                'adam_lr': 1e-3,\n",
    "                'num_users': 6040,\n",
    "                'num_items': 3706,\n",
    "                'latent_dim_mf': 8,\n",
    "                'latent_dim_mlp': 8,\n",
    "                'num_negative': 4,\n",
    "                'layers': [16,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
    "                'l2_regularization': 0.01,\n",
    "                'use_cuda': False,\n",
    "                'device_id': 7,\n",
    "                'pretrain': True,\n",
    "                'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n",
    "                'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_Epoch100_HR0.5606_NDCG0.2463.model'),\n",
    "                'model_dir':'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "ml1m_dir = 'data/ml-1m/ratings.dat'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'],  engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "# print(ml1m_rating.head())\n",
    "\n",
    "rating_range = ml1m_rating['rating'].min(), ml1m_rating['rating'].max()\n",
    "\n",
    "print(rating_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(ml1m_rating['timestamp'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial user_id\n",
      "     uid\n",
      "0      1\n",
      "53     2\n",
      "182    3\n",
      "233    4\n",
      "254    5\n",
      "final user_id\n",
      "     uid  userId\n",
      "0      1       0\n",
      "53     2       1\n",
      "182    3       2\n",
      "233    4       3\n",
      "254    5       4\n",
      "initial ml1m_rating \n",
      "   uid   mid  rating  timestamp  userId\n",
      "0    1  1193       5  978300760       0\n",
      "1    1   661       3  978302109       0\n",
      "2    1   914       3  978301968       0\n",
      "3    1  3408       4  978300275       0\n",
      "4    1  2355       5  978824291       0\n",
      "initial user_id\n",
      "    mid\n",
      "0  1193\n",
      "1   661\n",
      "2   914\n",
      "3  3408\n",
      "4  2355\n",
      "final user_id\n",
      "    mid  itemId\n",
      "0  1193       0\n",
      "1   661       1\n",
      "2   914       2\n",
      "3  3408       3\n",
      "4  2355       4\n",
      "final ml1m_rating \n",
      "   uid   mid  rating  timestamp  userId  itemId\n",
      "0    1  1193       5  978300760       0       0\n",
      "1    1   661       3  978302109       0       1\n",
      "2    1   914       3  978301968       0       2\n",
      "3    1  3408       4  978300275       0       3\n",
      "4    1  2355       5  978824291       0       4\n",
      "final final ml1m_rating \n",
      "   userId  itemId  rating  timestamp\n",
      "0       0       0       5  978300760\n",
      "1       0       1       3  978302109\n",
      "2       0       2       3  978301968\n",
      "3       0       3       4  978300275\n",
      "4       0       4       5  978824291\n",
      "Range of userId is [0, 6039]\n",
      "Range of itemId is [0, 3705]\n"
     ]
    }
   ],
   "source": [
    "# Reindex\n",
    "\n",
    "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
    "\n",
    "print(\"initial user_id\")\n",
    "print(user_id.head())\n",
    "\n",
    "user_id['userId'] = np.arange(len(user_id)) \n",
    "\n",
    "print(\"final user_id\")\n",
    "print(user_id.head())\n",
    "\n",
    "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n",
    "\n",
    "print(\"initial ml1m_rating \")\n",
    "print(ml1m_rating.head())\n",
    "\n",
    "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "\n",
    "print(\"initial user_id\")\n",
    "print(item_id.head())\n",
    "\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "\n",
    "print(\"final user_id\")\n",
    "print(item_id.head())\n",
    "\n",
    "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n",
    "\n",
    "print(\"final ml1m_rating \")\n",
    "print(ml1m_rating.head())\n",
    "\n",
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "\n",
    "print(\"final final ml1m_rating \")\n",
    "print(ml1m_rating.head())\n",
    "\n",
    "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000209\n"
     ]
    }
   ],
   "source": [
    "print(len(ml1m_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  itemId  rating  timestamp\n",
      "0       0       0       5  978300760\n",
      "1       0       1       3  978302109\n",
      "2       0       2       3  978301968\n",
      "3       0       3       4  978300275\n",
      "4       0       4       5  978824291\n"
     ]
    }
   ],
   "source": [
    "print(ml1m_rating.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: rating, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(ml1m_rating['rating'][ml1m_rating['rating'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           42.0\n",
      "1           23.0\n",
      "2           28.0\n",
      "3           47.0\n",
      "4            4.0\n",
      "           ...  \n",
      "1000204    161.0\n",
      "1000205    293.0\n",
      "1000206    305.0\n",
      "1000207    234.0\n",
      "1000208    246.0\n",
      "Name: timestamp, Length: 1000209, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(ml1m_rating.groupby(['userId'])['timestamp'].rank(method='first', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan\\Desktop\\GitHub_public\\neural-collaborative-filtering\\src\\data.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings['rating'][ratings['rating'] > 0] = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 2)    userId                                   interacted_items\n",
      "0       0  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
      "1       1  {0, 18, 20, 42, 47, 48, 52, 53, 54, 55, 56, 57...\n",
      "2       2  {128, 4, 5, 22, 166, 168, 41, 44, 175, 176, 17...\n",
      "3       3  {139, 26, 156, 43, 44, 48, 63, 64, 208, 209, 2...\n",
      "4       4  {3, 4, 9, 18, 27, 38, 39, 43, 48, 51, 59, 62, ...\n",
      "-------\n",
      "(6040, 3)    userId                                   interacted_items  \\\n",
      "0       0  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "1       1  {0, 18, 20, 42, 47, 48, 52, 53, 54, 55, 56, 57...   \n",
      "2       2  {128, 4, 5, 22, 166, 168, 41, 44, 175, 176, 17...   \n",
      "3       3  {139, 26, 156, 43, 44, 48, 63, 64, 208, 209, 2...   \n",
      "4       4  {3, 4, 9, 18, 27, 38, 39, 43, 48, 51, 59, 62, ...   \n",
      "\n",
      "                                      negative_items  \n",
      "0  {53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 6...  \n",
      "1  {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n",
      "2  {0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
      "3  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
      "4  {0, 1, 2, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, ...  \n",
      "--------\n",
      "(6040, 4)    userId                                   interacted_items  \\\n",
      "0       0  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "1       1  {0, 18, 20, 42, 47, 48, 52, 53, 54, 55, 56, 57...   \n",
      "2       2  {128, 4, 5, 22, 166, 168, 41, 44, 175, 176, 17...   \n",
      "3       3  {139, 26, 156, 43, 44, 48, 63, 64, 208, 209, 2...   \n",
      "4       4  {3, 4, 9, 18, 27, 38, 39, 43, 48, 51, 59, 62, ...   \n",
      "\n",
      "                                      negative_items  \\\n",
      "0  {53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 6...   \n",
      "1  {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
      "2  {0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
      "3  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
      "4  {0, 1, 2, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, ...   \n",
      "\n",
      "                                    negative_samples  \n",
      "0  [3511, 1630, 3157, 3699, 1775, 218, 1113, 2147...  \n",
      "1  [640, 2371, 1491, 3465, 2342, 961, 3403, 2599,...  \n",
      "2  [2838, 354, 121, 560, 2651, 823, 2534, 3451, 2...  \n",
      "3  [2250, 1171, 573, 1004, 3142, 1994, 1463, 2520...  \n",
      "4  [598, 978, 685, 2689, 2857, 1011, 1436, 1344, ...  \n",
      "6040\n",
      "3703\n",
      "----\n",
      "6040\n",
      "1878\n"
     ]
    }
   ],
   "source": [
    "# DataLoader for training\n",
    "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "evaluate_data = sample_generator.evaluate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the exact model\n",
    "config = gmf_config\n",
    "engine = GMFEngine(config)\n",
    "# config = mlp_config\n",
    "# engine = MLPEngine(config)\n",
    "# config = neumf_config\n",
    "# engine = NeuMFEngine(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 starts !\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.8320139646530151\n",
      "[Training Epoch 0] Batch 1, Loss 0.8574685454368591\n",
      "[Training Epoch 0] Batch 2, Loss 0.865487813949585\n",
      "[Training Epoch 0] Batch 3, Loss 0.8466443419456482\n",
      "[Training Epoch 0] Batch 4, Loss 0.8294466733932495\n",
      "[Training Epoch 0] Batch 5, Loss 0.8464298844337463\n",
      "[Training Epoch 0] Batch 6, Loss 0.8484600186347961\n",
      "[Training Epoch 0] Batch 7, Loss 0.8535138964653015\n",
      "[Training Epoch 0] Batch 8, Loss 0.8483507633209229\n",
      "[Training Epoch 0] Batch 9, Loss 0.8167732954025269\n",
      "[Training Epoch 0] Batch 10, Loss 0.8496021032333374\n",
      "[Training Epoch 0] Batch 11, Loss 0.8444838523864746\n",
      "[Training Epoch 0] Batch 12, Loss 0.8398794531822205\n",
      "[Training Epoch 0] Batch 13, Loss 0.8328506946563721\n",
      "[Training Epoch 0] Batch 14, Loss 0.859897255897522\n",
      "[Training Epoch 0] Batch 15, Loss 0.8263131380081177\n",
      "[Training Epoch 0] Batch 16, Loss 0.8426458835601807\n",
      "[Training Epoch 0] Batch 17, Loss 0.8440468311309814\n",
      "[Training Epoch 0] Batch 18, Loss 0.8462972044944763\n",
      "[Training Epoch 0] Batch 19, Loss 0.8143078088760376\n",
      "[Training Epoch 0] Batch 20, Loss 0.8370324969291687\n",
      "[Training Epoch 0] Batch 21, Loss 0.8236186504364014\n",
      "[Training Epoch 0] Batch 22, Loss 0.8423983454704285\n",
      "[Training Epoch 0] Batch 23, Loss 0.82331782579422\n",
      "[Training Epoch 0] Batch 24, Loss 0.8322733640670776\n",
      "[Training Epoch 0] Batch 25, Loss 0.8416308164596558\n",
      "[Training Epoch 0] Batch 26, Loss 0.8319880962371826\n",
      "[Training Epoch 0] Batch 27, Loss 0.8275946974754333\n",
      "[Training Epoch 0] Batch 28, Loss 0.832485556602478\n",
      "[Training Epoch 0] Batch 29, Loss 0.823711633682251\n",
      "[Training Epoch 0] Batch 30, Loss 0.8346378207206726\n",
      "[Training Epoch 0] Batch 31, Loss 0.8262269496917725\n",
      "[Training Epoch 0] Batch 32, Loss 0.8197609186172485\n",
      "[Training Epoch 0] Batch 33, Loss 0.8423056602478027\n",
      "[Training Epoch 0] Batch 34, Loss 0.8404482007026672\n",
      "[Training Epoch 0] Batch 35, Loss 0.8090112805366516\n",
      "[Training Epoch 0] Batch 36, Loss 0.8208929896354675\n",
      "[Training Epoch 0] Batch 37, Loss 0.829657793045044\n",
      "[Training Epoch 0] Batch 38, Loss 0.8226932287216187\n",
      "[Training Epoch 0] Batch 39, Loss 0.8209960460662842\n",
      "[Training Epoch 0] Batch 40, Loss 0.8312064409255981\n",
      "[Training Epoch 0] Batch 41, Loss 0.8497729897499084\n",
      "[Training Epoch 0] Batch 42, Loss 0.8196775913238525\n",
      "[Training Epoch 0] Batch 43, Loss 0.8200933337211609\n",
      "[Training Epoch 0] Batch 44, Loss 0.8232884407043457\n",
      "[Training Epoch 0] Batch 45, Loss 0.8174058198928833\n",
      "[Training Epoch 0] Batch 46, Loss 0.8157954216003418\n",
      "[Training Epoch 0] Batch 47, Loss 0.8327328562736511\n",
      "[Training Epoch 0] Batch 48, Loss 0.8048567771911621\n",
      "[Training Epoch 0] Batch 49, Loss 0.8058075308799744\n",
      "[Training Epoch 0] Batch 50, Loss 0.809018075466156\n",
      "[Training Epoch 0] Batch 51, Loss 0.8152030110359192\n",
      "[Training Epoch 0] Batch 52, Loss 0.8037738800048828\n",
      "[Training Epoch 0] Batch 53, Loss 0.8145653009414673\n",
      "[Training Epoch 0] Batch 54, Loss 0.819148063659668\n",
      "[Training Epoch 0] Batch 55, Loss 0.8221628069877625\n",
      "[Training Epoch 0] Batch 56, Loss 0.8207632899284363\n",
      "[Training Epoch 0] Batch 57, Loss 0.8339784145355225\n",
      "[Training Epoch 0] Batch 58, Loss 0.8118486404418945\n",
      "[Training Epoch 0] Batch 59, Loss 0.8010250329971313\n",
      "[Training Epoch 0] Batch 60, Loss 0.8156887292861938\n",
      "[Training Epoch 0] Batch 61, Loss 0.8102304339408875\n",
      "[Training Epoch 0] Batch 62, Loss 0.8046696186065674\n",
      "[Training Epoch 0] Batch 63, Loss 0.8154078125953674\n",
      "[Training Epoch 0] Batch 64, Loss 0.8036786913871765\n",
      "[Training Epoch 0] Batch 65, Loss 0.8093901872634888\n",
      "[Training Epoch 0] Batch 66, Loss 0.8200975060462952\n",
      "[Training Epoch 0] Batch 67, Loss 0.816601037979126\n",
      "[Training Epoch 0] Batch 68, Loss 0.8114454746246338\n",
      "[Training Epoch 0] Batch 69, Loss 0.8206827044487\n",
      "[Training Epoch 0] Batch 70, Loss 0.7929478287696838\n",
      "[Training Epoch 0] Batch 71, Loss 0.8119285106658936\n",
      "[Training Epoch 0] Batch 72, Loss 0.7990139722824097\n",
      "[Training Epoch 0] Batch 73, Loss 0.8049796223640442\n",
      "[Training Epoch 0] Batch 74, Loss 0.8005520105361938\n",
      "[Training Epoch 0] Batch 75, Loss 0.796036958694458\n",
      "[Training Epoch 0] Batch 76, Loss 0.8227604031562805\n",
      "[Training Epoch 0] Batch 77, Loss 0.7897217273712158\n",
      "[Training Epoch 0] Batch 78, Loss 0.8060550689697266\n",
      "[Training Epoch 0] Batch 79, Loss 0.8042985200881958\n",
      "[Training Epoch 0] Batch 80, Loss 0.7954393029212952\n",
      "[Training Epoch 0] Batch 81, Loss 0.7856646180152893\n",
      "[Training Epoch 0] Batch 82, Loss 0.7838624119758606\n",
      "[Training Epoch 0] Batch 83, Loss 0.7963929772377014\n",
      "[Training Epoch 0] Batch 84, Loss 0.798099160194397\n",
      "[Training Epoch 0] Batch 85, Loss 0.7719682455062866\n",
      "[Training Epoch 0] Batch 86, Loss 0.8113204836845398\n",
      "[Training Epoch 0] Batch 87, Loss 0.807424008846283\n",
      "[Training Epoch 0] Batch 88, Loss 0.7945381999015808\n",
      "[Training Epoch 0] Batch 89, Loss 0.8067059516906738\n",
      "[Training Epoch 0] Batch 90, Loss 0.7965434193611145\n",
      "[Training Epoch 0] Batch 91, Loss 0.7944531440734863\n",
      "[Training Epoch 0] Batch 92, Loss 0.7798483967781067\n",
      "[Training Epoch 0] Batch 93, Loss 0.7802954316139221\n",
      "[Training Epoch 0] Batch 94, Loss 0.7916116714477539\n",
      "[Training Epoch 0] Batch 95, Loss 0.7968358993530273\n",
      "[Training Epoch 0] Batch 96, Loss 0.8006105422973633\n",
      "[Training Epoch 0] Batch 97, Loss 0.790467381477356\n",
      "[Training Epoch 0] Batch 98, Loss 0.7729934453964233\n",
      "[Training Epoch 0] Batch 99, Loss 0.7881906628608704\n",
      "[Training Epoch 0] Batch 100, Loss 0.7881313562393188\n",
      "[Training Epoch 0] Batch 101, Loss 0.7826769351959229\n",
      "[Training Epoch 0] Batch 102, Loss 0.7870232462882996\n",
      "[Training Epoch 0] Batch 103, Loss 0.7643260955810547\n",
      "[Training Epoch 0] Batch 104, Loss 0.7835562229156494\n",
      "[Training Epoch 0] Batch 105, Loss 0.7908384203910828\n",
      "[Training Epoch 0] Batch 106, Loss 0.7770603895187378\n",
      "[Training Epoch 0] Batch 107, Loss 0.7918365001678467\n",
      "[Training Epoch 0] Batch 108, Loss 0.7728330492973328\n",
      "[Training Epoch 0] Batch 109, Loss 0.78999924659729\n",
      "[Training Epoch 0] Batch 110, Loss 0.771399974822998\n",
      "[Training Epoch 0] Batch 111, Loss 0.776150107383728\n",
      "[Training Epoch 0] Batch 112, Loss 0.7706444263458252\n",
      "[Training Epoch 0] Batch 113, Loss 0.7779625058174133\n",
      "[Training Epoch 0] Batch 114, Loss 0.7833963632583618\n",
      "[Training Epoch 0] Batch 115, Loss 0.7777854204177856\n",
      "[Training Epoch 0] Batch 116, Loss 0.7692544460296631\n",
      "[Training Epoch 0] Batch 117, Loss 0.776277482509613\n",
      "[Training Epoch 0] Batch 118, Loss 0.776330828666687\n",
      "[Training Epoch 0] Batch 119, Loss 0.7864032983779907\n",
      "[Training Epoch 0] Batch 120, Loss 0.7819007635116577\n",
      "[Training Epoch 0] Batch 121, Loss 0.7805770635604858\n",
      "[Training Epoch 0] Batch 122, Loss 0.7820299863815308\n",
      "[Training Epoch 0] Batch 123, Loss 0.7622563242912292\n",
      "[Training Epoch 0] Batch 124, Loss 0.7745097875595093\n",
      "[Training Epoch 0] Batch 125, Loss 0.7819198369979858\n",
      "[Training Epoch 0] Batch 126, Loss 0.7688360810279846\n",
      "[Training Epoch 0] Batch 127, Loss 0.785271942615509\n",
      "[Training Epoch 0] Batch 128, Loss 0.7865820527076721\n",
      "[Training Epoch 0] Batch 129, Loss 0.7684572339057922\n",
      "[Training Epoch 0] Batch 130, Loss 0.766141414642334\n",
      "[Training Epoch 0] Batch 131, Loss 0.782386064529419\n",
      "[Training Epoch 0] Batch 132, Loss 0.7746769785881042\n",
      "[Training Epoch 0] Batch 133, Loss 0.764441728591919\n",
      "[Training Epoch 0] Batch 134, Loss 0.7673989534378052\n",
      "[Training Epoch 0] Batch 135, Loss 0.7718127369880676\n",
      "[Training Epoch 0] Batch 136, Loss 0.7591454982757568\n",
      "[Training Epoch 0] Batch 137, Loss 0.7657396793365479\n",
      "[Training Epoch 0] Batch 138, Loss 0.7695392966270447\n",
      "[Training Epoch 0] Batch 139, Loss 0.7687689065933228\n",
      "[Training Epoch 0] Batch 140, Loss 0.7682368755340576\n",
      "[Training Epoch 0] Batch 141, Loss 0.7627972960472107\n",
      "[Training Epoch 0] Batch 142, Loss 0.7635834813117981\n",
      "[Training Epoch 0] Batch 143, Loss 0.7649056315422058\n",
      "[Training Epoch 0] Batch 144, Loss 0.7679803371429443\n",
      "[Training Epoch 0] Batch 145, Loss 0.7798507213592529\n",
      "[Training Epoch 0] Batch 146, Loss 0.758456826210022\n",
      "[Training Epoch 0] Batch 147, Loss 0.7756946086883545\n",
      "[Training Epoch 0] Batch 148, Loss 0.7623763680458069\n",
      "[Training Epoch 0] Batch 149, Loss 0.7724632024765015\n",
      "[Training Epoch 0] Batch 150, Loss 0.758124589920044\n",
      "[Training Epoch 0] Batch 151, Loss 0.7562163472175598\n",
      "[Training Epoch 0] Batch 152, Loss 0.7599145174026489\n",
      "[Training Epoch 0] Batch 153, Loss 0.7571108341217041\n",
      "[Training Epoch 0] Batch 154, Loss 0.7583966255187988\n",
      "[Training Epoch 0] Batch 155, Loss 0.7704789638519287\n",
      "[Training Epoch 0] Batch 156, Loss 0.7729134559631348\n",
      "[Training Epoch 0] Batch 157, Loss 0.7449417114257812\n",
      "[Training Epoch 0] Batch 158, Loss 0.7655883431434631\n",
      "[Training Epoch 0] Batch 159, Loss 0.769010066986084\n",
      "[Training Epoch 0] Batch 160, Loss 0.7652975916862488\n",
      "[Training Epoch 0] Batch 161, Loss 0.7553890943527222\n",
      "[Training Epoch 0] Batch 162, Loss 0.7587687969207764\n",
      "[Training Epoch 0] Batch 163, Loss 0.7689175009727478\n",
      "[Training Epoch 0] Batch 164, Loss 0.761694073677063\n",
      "[Training Epoch 0] Batch 165, Loss 0.7562699317932129\n",
      "[Training Epoch 0] Batch 166, Loss 0.7563074827194214\n",
      "[Training Epoch 0] Batch 167, Loss 0.7466974854469299\n",
      "[Training Epoch 0] Batch 168, Loss 0.7461832761764526\n",
      "[Training Epoch 0] Batch 169, Loss 0.7392188310623169\n",
      "[Training Epoch 0] Batch 170, Loss 0.7517000436782837\n",
      "[Training Epoch 0] Batch 171, Loss 0.7595129013061523\n",
      "[Training Epoch 0] Batch 172, Loss 0.7542234659194946\n",
      "[Training Epoch 0] Batch 173, Loss 0.7597618103027344\n",
      "[Training Epoch 0] Batch 174, Loss 0.7542898058891296\n",
      "[Training Epoch 0] Batch 175, Loss 0.7664702534675598\n",
      "[Training Epoch 0] Batch 176, Loss 0.754328727722168\n",
      "[Training Epoch 0] Batch 177, Loss 0.7528752684593201\n",
      "[Training Epoch 0] Batch 178, Loss 0.7532432079315186\n",
      "[Training Epoch 0] Batch 179, Loss 0.7496389746665955\n",
      "[Training Epoch 0] Batch 180, Loss 0.7465485334396362\n",
      "[Training Epoch 0] Batch 181, Loss 0.7488983869552612\n",
      "[Training Epoch 0] Batch 182, Loss 0.7420691251754761\n",
      "[Training Epoch 0] Batch 183, Loss 0.7496277093887329\n",
      "[Training Epoch 0] Batch 184, Loss 0.7492387890815735\n",
      "[Training Epoch 0] Batch 185, Loss 0.755713164806366\n",
      "[Training Epoch 0] Batch 186, Loss 0.7553136348724365\n",
      "[Training Epoch 0] Batch 187, Loss 0.7451766729354858\n",
      "[Training Epoch 0] Batch 188, Loss 0.7480619549751282\n",
      "[Training Epoch 0] Batch 189, Loss 0.7562404870986938\n",
      "[Training Epoch 0] Batch 190, Loss 0.7497639060020447\n",
      "[Training Epoch 0] Batch 191, Loss 0.7413771748542786\n",
      "[Training Epoch 0] Batch 192, Loss 0.7386801242828369\n",
      "[Training Epoch 0] Batch 193, Loss 0.7452578544616699\n",
      "[Training Epoch 0] Batch 194, Loss 0.7466017007827759\n",
      "[Training Epoch 0] Batch 195, Loss 0.7449395060539246\n",
      "[Training Epoch 0] Batch 196, Loss 0.744261622428894\n",
      "[Training Epoch 0] Batch 197, Loss 0.74369215965271\n",
      "[Training Epoch 0] Batch 198, Loss 0.7498569488525391\n",
      "[Training Epoch 0] Batch 199, Loss 0.7372643947601318\n",
      "[Training Epoch 0] Batch 200, Loss 0.7385402917861938\n",
      "[Training Epoch 0] Batch 201, Loss 0.7482753992080688\n",
      "[Training Epoch 0] Batch 202, Loss 0.7493714690208435\n",
      "[Training Epoch 0] Batch 203, Loss 0.7470694184303284\n",
      "[Training Epoch 0] Batch 204, Loss 0.7361468076705933\n",
      "[Training Epoch 0] Batch 205, Loss 0.7516480684280396\n",
      "[Training Epoch 0] Batch 206, Loss 0.7423304319381714\n",
      "[Training Epoch 0] Batch 207, Loss 0.7352921962738037\n",
      "[Training Epoch 0] Batch 208, Loss 0.7418224811553955\n",
      "[Training Epoch 0] Batch 209, Loss 0.7485765218734741\n",
      "[Training Epoch 0] Batch 210, Loss 0.7431540489196777\n",
      "[Training Epoch 0] Batch 211, Loss 0.7310106158256531\n",
      "[Training Epoch 0] Batch 212, Loss 0.730262279510498\n",
      "[Training Epoch 0] Batch 213, Loss 0.7408829927444458\n",
      "[Training Epoch 0] Batch 214, Loss 0.73450767993927\n",
      "[Training Epoch 0] Batch 215, Loss 0.739237904548645\n",
      "[Training Epoch 0] Batch 216, Loss 0.7343946695327759\n",
      "[Training Epoch 0] Batch 217, Loss 0.7252155542373657\n",
      "[Training Epoch 0] Batch 218, Loss 0.7467647790908813\n",
      "[Training Epoch 0] Batch 219, Loss 0.7335899472236633\n",
      "[Training Epoch 0] Batch 220, Loss 0.7361851930618286\n",
      "[Training Epoch 0] Batch 221, Loss 0.733241856098175\n",
      "[Training Epoch 0] Batch 222, Loss 0.7368658185005188\n",
      "[Training Epoch 0] Batch 223, Loss 0.7347193956375122\n",
      "[Training Epoch 0] Batch 224, Loss 0.7302350401878357\n",
      "[Training Epoch 0] Batch 225, Loss 0.7281345725059509\n",
      "[Training Epoch 0] Batch 226, Loss 0.727655291557312\n",
      "[Training Epoch 0] Batch 227, Loss 0.7308586835861206\n",
      "[Training Epoch 0] Batch 228, Loss 0.7328432202339172\n",
      "[Training Epoch 0] Batch 229, Loss 0.739372730255127\n",
      "[Training Epoch 0] Batch 230, Loss 0.7296591401100159\n",
      "[Training Epoch 0] Batch 231, Loss 0.7269910573959351\n",
      "[Training Epoch 0] Batch 232, Loss 0.7350612282752991\n",
      "[Training Epoch 0] Batch 233, Loss 0.7258450388908386\n",
      "[Training Epoch 0] Batch 234, Loss 0.733198881149292\n",
      "[Training Epoch 0] Batch 235, Loss 0.7242996096611023\n",
      "[Training Epoch 0] Batch 236, Loss 0.7287203073501587\n",
      "[Training Epoch 0] Batch 237, Loss 0.7321761250495911\n",
      "[Training Epoch 0] Batch 238, Loss 0.7245227694511414\n",
      "[Training Epoch 0] Batch 239, Loss 0.7239987254142761\n",
      "[Training Epoch 0] Batch 240, Loss 0.722096860408783\n",
      "[Training Epoch 0] Batch 241, Loss 0.7281646132469177\n",
      "[Training Epoch 0] Batch 242, Loss 0.7219883799552917\n",
      "[Training Epoch 0] Batch 243, Loss 0.7311150431632996\n",
      "[Training Epoch 0] Batch 244, Loss 0.7199263572692871\n",
      "[Training Epoch 0] Batch 245, Loss 0.7252364754676819\n",
      "[Training Epoch 0] Batch 246, Loss 0.7201197743415833\n",
      "[Training Epoch 0] Batch 247, Loss 0.7202113270759583\n",
      "[Training Epoch 0] Batch 248, Loss 0.7330055236816406\n",
      "[Training Epoch 0] Batch 249, Loss 0.715126097202301\n",
      "[Training Epoch 0] Batch 250, Loss 0.7245236039161682\n",
      "[Training Epoch 0] Batch 251, Loss 0.7214618921279907\n",
      "[Training Epoch 0] Batch 252, Loss 0.7196022868156433\n",
      "[Training Epoch 0] Batch 253, Loss 0.7225687503814697\n",
      "[Training Epoch 0] Batch 254, Loss 0.719839334487915\n",
      "[Training Epoch 0] Batch 255, Loss 0.7218165397644043\n",
      "[Training Epoch 0] Batch 256, Loss 0.7305415272712708\n",
      "[Training Epoch 0] Batch 257, Loss 0.7151755094528198\n",
      "[Training Epoch 0] Batch 258, Loss 0.7201921939849854\n",
      "[Training Epoch 0] Batch 259, Loss 0.7208371758460999\n",
      "[Training Epoch 0] Batch 260, Loss 0.7144389152526855\n",
      "[Training Epoch 0] Batch 261, Loss 0.716560423374176\n",
      "[Training Epoch 0] Batch 262, Loss 0.7163687944412231\n",
      "[Training Epoch 0] Batch 263, Loss 0.718510627746582\n",
      "[Training Epoch 0] Batch 264, Loss 0.7168416976928711\n",
      "[Training Epoch 0] Batch 265, Loss 0.7165462374687195\n",
      "[Training Epoch 0] Batch 266, Loss 0.7152190804481506\n",
      "[Training Epoch 0] Batch 267, Loss 0.7164580821990967\n",
      "[Training Epoch 0] Batch 268, Loss 0.723179817199707\n",
      "[Training Epoch 0] Batch 269, Loss 0.7175886631011963\n",
      "[Training Epoch 0] Batch 270, Loss 0.7129122018814087\n",
      "[Training Epoch 0] Batch 271, Loss 0.7158178687095642\n",
      "[Training Epoch 0] Batch 272, Loss 0.7186628580093384\n",
      "[Training Epoch 0] Batch 273, Loss 0.7131858468055725\n",
      "[Training Epoch 0] Batch 274, Loss 0.7162632942199707\n",
      "[Training Epoch 0] Batch 275, Loss 0.7214877605438232\n",
      "[Training Epoch 0] Batch 276, Loss 0.7139303088188171\n",
      "[Training Epoch 0] Batch 277, Loss 0.7212106585502625\n",
      "[Training Epoch 0] Batch 278, Loss 0.7125645875930786\n",
      "[Training Epoch 0] Batch 279, Loss 0.7083996534347534\n",
      "[Training Epoch 0] Batch 280, Loss 0.7143787145614624\n",
      "[Training Epoch 0] Batch 281, Loss 0.7149227261543274\n",
      "[Training Epoch 0] Batch 282, Loss 0.715011715888977\n",
      "[Training Epoch 0] Batch 283, Loss 0.7115686535835266\n",
      "[Training Epoch 0] Batch 284, Loss 0.7119990587234497\n",
      "[Training Epoch 0] Batch 285, Loss 0.7131130695343018\n",
      "[Training Epoch 0] Batch 286, Loss 0.7132473587989807\n",
      "[Training Epoch 0] Batch 287, Loss 0.7153137922286987\n",
      "[Training Epoch 0] Batch 288, Loss 0.7135672569274902\n",
      "[Training Epoch 0] Batch 289, Loss 0.7032272815704346\n",
      "[Training Epoch 0] Batch 290, Loss 0.7080918550491333\n",
      "[Training Epoch 0] Batch 291, Loss 0.7030848860740662\n",
      "[Training Epoch 0] Batch 292, Loss 0.7097113132476807\n",
      "[Training Epoch 0] Batch 293, Loss 0.7055764198303223\n",
      "[Training Epoch 0] Batch 294, Loss 0.7116405367851257\n",
      "[Training Epoch 0] Batch 295, Loss 0.6999996900558472\n",
      "[Training Epoch 0] Batch 296, Loss 0.7064354419708252\n",
      "[Training Epoch 0] Batch 297, Loss 0.7100163102149963\n",
      "[Training Epoch 0] Batch 298, Loss 0.7088914513587952\n",
      "[Training Epoch 0] Batch 299, Loss 0.7047247290611267\n",
      "[Training Epoch 0] Batch 300, Loss 0.7088805437088013\n",
      "[Training Epoch 0] Batch 301, Loss 0.706003725528717\n",
      "[Training Epoch 0] Batch 302, Loss 0.7029599547386169\n",
      "[Training Epoch 0] Batch 303, Loss 0.7070314884185791\n",
      "[Training Epoch 0] Batch 304, Loss 0.702319324016571\n",
      "[Training Epoch 0] Batch 305, Loss 0.7043125629425049\n",
      "[Training Epoch 0] Batch 306, Loss 0.7008321285247803\n",
      "[Training Epoch 0] Batch 307, Loss 0.7002788782119751\n",
      "[Training Epoch 0] Batch 308, Loss 0.7033647894859314\n",
      "[Training Epoch 0] Batch 309, Loss 0.6984400749206543\n",
      "[Training Epoch 0] Batch 310, Loss 0.7028958797454834\n",
      "[Training Epoch 0] Batch 311, Loss 0.7015435695648193\n",
      "[Training Epoch 0] Batch 312, Loss 0.6997624039649963\n",
      "[Training Epoch 0] Batch 313, Loss 0.7068274021148682\n",
      "[Training Epoch 0] Batch 314, Loss 0.6993134617805481\n",
      "[Training Epoch 0] Batch 315, Loss 0.6990307569503784\n",
      "[Training Epoch 0] Batch 316, Loss 0.6962660551071167\n",
      "[Training Epoch 0] Batch 317, Loss 0.7010589838027954\n",
      "[Training Epoch 0] Batch 318, Loss 0.7035693526268005\n",
      "[Training Epoch 0] Batch 319, Loss 0.7034404873847961\n",
      "[Training Epoch 0] Batch 320, Loss 0.7002933025360107\n",
      "[Training Epoch 0] Batch 321, Loss 0.7024612426757812\n",
      "[Training Epoch 0] Batch 322, Loss 0.7007723450660706\n",
      "[Training Epoch 0] Batch 323, Loss 0.6954341530799866\n",
      "[Training Epoch 0] Batch 324, Loss 0.6953065395355225\n",
      "[Training Epoch 0] Batch 325, Loss 0.6989792585372925\n",
      "[Training Epoch 0] Batch 326, Loss 0.6980441808700562\n",
      "[Training Epoch 0] Batch 327, Loss 0.6940110325813293\n",
      "[Training Epoch 0] Batch 328, Loss 0.6969609260559082\n",
      "[Training Epoch 0] Batch 329, Loss 0.6977037787437439\n",
      "[Training Epoch 0] Batch 330, Loss 0.6911603212356567\n",
      "[Training Epoch 0] Batch 331, Loss 0.692999005317688\n",
      "[Training Epoch 0] Batch 332, Loss 0.6939812898635864\n",
      "[Training Epoch 0] Batch 333, Loss 0.6963358521461487\n",
      "[Training Epoch 0] Batch 334, Loss 0.6958884596824646\n",
      "[Training Epoch 0] Batch 335, Loss 0.6937021017074585\n",
      "[Training Epoch 0] Batch 336, Loss 0.6948466300964355\n",
      "[Training Epoch 0] Batch 337, Loss 0.699504554271698\n",
      "[Training Epoch 0] Batch 338, Loss 0.6973296403884888\n",
      "[Training Epoch 0] Batch 339, Loss 0.6908552646636963\n",
      "[Training Epoch 0] Batch 340, Loss 0.6919125318527222\n",
      "[Training Epoch 0] Batch 341, Loss 0.6857385635375977\n",
      "[Training Epoch 0] Batch 342, Loss 0.6909429430961609\n",
      "[Training Epoch 0] Batch 343, Loss 0.6898788213729858\n",
      "[Training Epoch 0] Batch 344, Loss 0.693255603313446\n",
      "[Training Epoch 0] Batch 345, Loss 0.6921245455741882\n",
      "[Training Epoch 0] Batch 346, Loss 0.691972553730011\n",
      "[Training Epoch 0] Batch 347, Loss 0.6893805265426636\n",
      "[Training Epoch 0] Batch 348, Loss 0.6871797442436218\n",
      "[Training Epoch 0] Batch 349, Loss 0.6906865835189819\n",
      "[Training Epoch 0] Batch 350, Loss 0.69105464220047\n",
      "[Training Epoch 0] Batch 351, Loss 0.6931307911872864\n",
      "[Training Epoch 0] Batch 352, Loss 0.689689576625824\n",
      "[Training Epoch 0] Batch 353, Loss 0.6874959468841553\n",
      "[Training Epoch 0] Batch 354, Loss 0.6878940463066101\n",
      "[Training Epoch 0] Batch 355, Loss 0.6868775486946106\n",
      "[Training Epoch 0] Batch 356, Loss 0.6893482208251953\n",
      "[Training Epoch 0] Batch 357, Loss 0.6891254186630249\n",
      "[Training Epoch 0] Batch 358, Loss 0.6872252225875854\n",
      "[Training Epoch 0] Batch 359, Loss 0.6890544891357422\n",
      "[Training Epoch 0] Batch 360, Loss 0.6858255863189697\n",
      "[Training Epoch 0] Batch 361, Loss 0.6856753826141357\n",
      "[Training Epoch 0] Batch 362, Loss 0.687718391418457\n",
      "[Training Epoch 0] Batch 363, Loss 0.6834994554519653\n",
      "[Training Epoch 0] Batch 364, Loss 0.6854050755500793\n",
      "[Training Epoch 0] Batch 365, Loss 0.6844077706336975\n",
      "[Training Epoch 0] Batch 366, Loss 0.6870472431182861\n",
      "[Training Epoch 0] Batch 367, Loss 0.6874231696128845\n",
      "[Training Epoch 0] Batch 368, Loss 0.6840991973876953\n",
      "[Training Epoch 0] Batch 369, Loss 0.6835825443267822\n",
      "[Training Epoch 0] Batch 370, Loss 0.6847270727157593\n",
      "[Training Epoch 0] Batch 371, Loss 0.6823856234550476\n",
      "[Training Epoch 0] Batch 372, Loss 0.6824382543563843\n",
      "[Training Epoch 0] Batch 373, Loss 0.6844155788421631\n",
      "[Training Epoch 0] Batch 374, Loss 0.6850764155387878\n",
      "[Training Epoch 0] Batch 375, Loss 0.6815959811210632\n",
      "[Training Epoch 0] Batch 376, Loss 0.6801831126213074\n",
      "[Training Epoch 0] Batch 377, Loss 0.6815651655197144\n",
      "[Training Epoch 0] Batch 378, Loss 0.680855393409729\n",
      "[Training Epoch 0] Batch 379, Loss 0.680802583694458\n",
      "[Training Epoch 0] Batch 380, Loss 0.6820554733276367\n",
      "[Training Epoch 0] Batch 381, Loss 0.6829472780227661\n",
      "[Training Epoch 0] Batch 382, Loss 0.6830838322639465\n",
      "[Training Epoch 0] Batch 383, Loss 0.6794902682304382\n",
      "[Training Epoch 0] Batch 384, Loss 0.6820234060287476\n",
      "[Training Epoch 0] Batch 385, Loss 0.6816996335983276\n",
      "[Training Epoch 0] Batch 386, Loss 0.6816345453262329\n",
      "[Training Epoch 0] Batch 387, Loss 0.6777097582817078\n",
      "[Training Epoch 0] Batch 388, Loss 0.6785621047019958\n",
      "[Training Epoch 0] Batch 389, Loss 0.6789662837982178\n",
      "[Training Epoch 0] Batch 390, Loss 0.6774846315383911\n",
      "[Training Epoch 0] Batch 391, Loss 0.6787922382354736\n",
      "[Training Epoch 0] Batch 392, Loss 0.6781218647956848\n",
      "[Training Epoch 0] Batch 393, Loss 0.6790143251419067\n",
      "[Training Epoch 0] Batch 394, Loss 0.6774868965148926\n",
      "[Training Epoch 0] Batch 395, Loss 0.681208610534668\n",
      "[Training Epoch 0] Batch 396, Loss 0.6763660311698914\n",
      "[Training Epoch 0] Batch 397, Loss 0.6764277815818787\n",
      "[Training Epoch 0] Batch 398, Loss 0.6780111789703369\n",
      "[Training Epoch 0] Batch 399, Loss 0.6782816648483276\n",
      "[Training Epoch 0] Batch 400, Loss 0.6749283075332642\n",
      "[Training Epoch 0] Batch 401, Loss 0.6749799251556396\n",
      "[Training Epoch 0] Batch 402, Loss 0.6746480464935303\n",
      "[Training Epoch 0] Batch 403, Loss 0.6755942106246948\n",
      "[Training Epoch 0] Batch 404, Loss 0.6767290234565735\n",
      "[Training Epoch 0] Batch 405, Loss 0.674863874912262\n",
      "[Training Epoch 0] Batch 406, Loss 0.6759669780731201\n",
      "[Training Epoch 0] Batch 407, Loss 0.673027515411377\n",
      "[Training Epoch 0] Batch 408, Loss 0.6767398715019226\n",
      "[Training Epoch 0] Batch 409, Loss 0.6736711263656616\n",
      "[Training Epoch 0] Batch 410, Loss 0.6731311678886414\n",
      "[Training Epoch 0] Batch 411, Loss 0.6706445217132568\n",
      "[Training Epoch 0] Batch 412, Loss 0.673615038394928\n",
      "[Training Epoch 0] Batch 413, Loss 0.6761441230773926\n",
      "[Training Epoch 0] Batch 414, Loss 0.6718735098838806\n",
      "[Training Epoch 0] Batch 415, Loss 0.6707894802093506\n",
      "[Training Epoch 0] Batch 416, Loss 0.6710597276687622\n",
      "[Training Epoch 0] Batch 417, Loss 0.6738817691802979\n",
      "[Training Epoch 0] Batch 418, Loss 0.6739246845245361\n",
      "[Training Epoch 0] Batch 419, Loss 0.6691784858703613\n",
      "[Training Epoch 0] Batch 420, Loss 0.6688209176063538\n",
      "[Training Epoch 0] Batch 421, Loss 0.6693547964096069\n",
      "[Training Epoch 0] Batch 422, Loss 0.671260416507721\n",
      "[Training Epoch 0] Batch 423, Loss 0.669462263584137\n",
      "[Training Epoch 0] Batch 424, Loss 0.6730207800865173\n",
      "[Training Epoch 0] Batch 425, Loss 0.671694278717041\n",
      "[Training Epoch 0] Batch 426, Loss 0.6709170341491699\n",
      "[Training Epoch 0] Batch 427, Loss 0.6707350015640259\n",
      "[Training Epoch 0] Batch 428, Loss 0.6690067648887634\n",
      "[Training Epoch 0] Batch 429, Loss 0.6684433221817017\n",
      "[Training Epoch 0] Batch 430, Loss 0.6704769134521484\n",
      "[Training Epoch 0] Batch 431, Loss 0.6696662306785583\n",
      "[Training Epoch 0] Batch 432, Loss 0.669882595539093\n",
      "[Training Epoch 0] Batch 433, Loss 0.6703844666481018\n",
      "[Training Epoch 0] Batch 434, Loss 0.6681041121482849\n",
      "[Training Epoch 0] Batch 435, Loss 0.6652963161468506\n",
      "[Training Epoch 0] Batch 436, Loss 0.6682776212692261\n",
      "[Training Epoch 0] Batch 437, Loss 0.6655008792877197\n",
      "[Training Epoch 0] Batch 438, Loss 0.6648287177085876\n",
      "[Training Epoch 0] Batch 439, Loss 0.6627189517021179\n",
      "[Training Epoch 0] Batch 440, Loss 0.6693835854530334\n",
      "[Training Epoch 0] Batch 441, Loss 0.6676057577133179\n",
      "[Training Epoch 0] Batch 442, Loss 0.6665678024291992\n",
      "[Training Epoch 0] Batch 443, Loss 0.6635118126869202\n",
      "[Training Epoch 0] Batch 444, Loss 0.6665611267089844\n",
      "[Training Epoch 0] Batch 445, Loss 0.6649250388145447\n",
      "[Training Epoch 0] Batch 446, Loss 0.6622824668884277\n",
      "[Training Epoch 0] Batch 447, Loss 0.6639598608016968\n",
      "[Training Epoch 0] Batch 448, Loss 0.6639178991317749\n",
      "[Training Epoch 0] Batch 449, Loss 0.665440559387207\n",
      "[Training Epoch 0] Batch 450, Loss 0.6637046933174133\n",
      "[Training Epoch 0] Batch 451, Loss 0.6672021746635437\n",
      "[Training Epoch 0] Batch 452, Loss 0.6623989343643188\n",
      "[Training Epoch 0] Batch 453, Loss 0.6616661548614502\n",
      "[Training Epoch 0] Batch 454, Loss 0.6598775386810303\n",
      "[Training Epoch 0] Batch 455, Loss 0.6639922261238098\n",
      "[Training Epoch 0] Batch 456, Loss 0.6604829430580139\n",
      "[Training Epoch 0] Batch 457, Loss 0.66182541847229\n",
      "[Training Epoch 0] Batch 458, Loss 0.660888671875\n",
      "[Training Epoch 0] Batch 459, Loss 0.6628575921058655\n",
      "[Training Epoch 0] Batch 460, Loss 0.6627669334411621\n",
      "[Training Epoch 0] Batch 461, Loss 0.6583954691886902\n",
      "[Training Epoch 0] Batch 462, Loss 0.6614706516265869\n",
      "[Training Epoch 0] Batch 463, Loss 0.6600874066352844\n",
      "[Training Epoch 0] Batch 464, Loss 0.6637293696403503\n",
      "[Training Epoch 0] Batch 465, Loss 0.6574509143829346\n",
      "[Training Epoch 0] Batch 466, Loss 0.6619378328323364\n",
      "[Training Epoch 0] Batch 467, Loss 0.6589565277099609\n",
      "[Training Epoch 0] Batch 468, Loss 0.6586834192276001\n",
      "[Training Epoch 0] Batch 469, Loss 0.6560033559799194\n",
      "[Training Epoch 0] Batch 470, Loss 0.6588497757911682\n",
      "[Training Epoch 0] Batch 471, Loss 0.6601396799087524\n",
      "[Training Epoch 0] Batch 472, Loss 0.6578806042671204\n",
      "[Training Epoch 0] Batch 473, Loss 0.6600780487060547\n",
      "[Training Epoch 0] Batch 474, Loss 0.656093180179596\n",
      "[Training Epoch 0] Batch 475, Loss 0.6558638215065002\n",
      "[Training Epoch 0] Batch 476, Loss 0.6600494980812073\n",
      "[Training Epoch 0] Batch 477, Loss 0.6593396067619324\n",
      "[Training Epoch 0] Batch 478, Loss 0.6591567397117615\n",
      "[Training Epoch 0] Batch 479, Loss 0.6570806503295898\n",
      "[Training Epoch 0] Batch 480, Loss 0.659127414226532\n",
      "[Training Epoch 0] Batch 481, Loss 0.6549063920974731\n",
      "[Training Epoch 0] Batch 482, Loss 0.6578484177589417\n",
      "[Training Epoch 0] Batch 483, Loss 0.6554449796676636\n",
      "[Training Epoch 0] Batch 484, Loss 0.6552132368087769\n",
      "[Training Epoch 0] Batch 485, Loss 0.6537807583808899\n",
      "[Training Epoch 0] Batch 486, Loss 0.6546758413314819\n",
      "[Training Epoch 0] Batch 487, Loss 0.6533569097518921\n",
      "[Training Epoch 0] Batch 488, Loss 0.6571363210678101\n",
      "[Training Epoch 0] Batch 489, Loss 0.6528527140617371\n",
      "[Training Epoch 0] Batch 490, Loss 0.6566613912582397\n",
      "[Training Epoch 0] Batch 491, Loss 0.6533113718032837\n",
      "[Training Epoch 0] Batch 492, Loss 0.6550167798995972\n",
      "[Training Epoch 0] Batch 493, Loss 0.6534937620162964\n",
      "[Training Epoch 0] Batch 494, Loss 0.65416020154953\n",
      "[Training Epoch 0] Batch 495, Loss 0.6571958065032959\n",
      "[Training Epoch 0] Batch 496, Loss 0.6564041972160339\n",
      "[Training Epoch 0] Batch 497, Loss 0.6562952399253845\n",
      "[Training Epoch 0] Batch 498, Loss 0.6532806754112244\n",
      "[Training Epoch 0] Batch 499, Loss 0.6532819271087646\n",
      "[Training Epoch 0] Batch 500, Loss 0.6505165100097656\n",
      "[Training Epoch 0] Batch 501, Loss 0.6529540419578552\n",
      "[Training Epoch 0] Batch 502, Loss 0.6527867913246155\n",
      "[Training Epoch 0] Batch 503, Loss 0.6528973579406738\n",
      "[Training Epoch 0] Batch 504, Loss 0.649156928062439\n",
      "[Training Epoch 0] Batch 505, Loss 0.6519220471382141\n",
      "[Training Epoch 0] Batch 506, Loss 0.6507200002670288\n",
      "[Training Epoch 0] Batch 507, Loss 0.6496264338493347\n",
      "[Training Epoch 0] Batch 508, Loss 0.6466286182403564\n",
      "[Training Epoch 0] Batch 509, Loss 0.6485021114349365\n",
      "[Training Epoch 0] Batch 510, Loss 0.6496587991714478\n",
      "[Training Epoch 0] Batch 511, Loss 0.6451267004013062\n",
      "[Training Epoch 0] Batch 512, Loss 0.6517140865325928\n",
      "[Training Epoch 0] Batch 513, Loss 0.6472529172897339\n",
      "[Training Epoch 0] Batch 514, Loss 0.6475372910499573\n",
      "[Training Epoch 0] Batch 515, Loss 0.6478983759880066\n",
      "[Training Epoch 0] Batch 516, Loss 0.6469431519508362\n",
      "[Training Epoch 0] Batch 517, Loss 0.6480838656425476\n",
      "[Training Epoch 0] Batch 518, Loss 0.6481542587280273\n",
      "[Training Epoch 0] Batch 519, Loss 0.6479725241661072\n",
      "[Training Epoch 0] Batch 520, Loss 0.6442208886146545\n",
      "[Training Epoch 0] Batch 521, Loss 0.6468898057937622\n",
      "[Training Epoch 0] Batch 522, Loss 0.6471081376075745\n",
      "[Training Epoch 0] Batch 523, Loss 0.6486644148826599\n",
      "[Training Epoch 0] Batch 524, Loss 0.6428927779197693\n",
      "[Training Epoch 0] Batch 525, Loss 0.6459996700286865\n",
      "[Training Epoch 0] Batch 526, Loss 0.6437894105911255\n",
      "[Training Epoch 0] Batch 527, Loss 0.6490989327430725\n",
      "[Training Epoch 0] Batch 528, Loss 0.6436154842376709\n",
      "[Training Epoch 0] Batch 529, Loss 0.6449143290519714\n",
      "[Training Epoch 0] Batch 530, Loss 0.6449347734451294\n",
      "[Training Epoch 0] Batch 531, Loss 0.6423556208610535\n",
      "[Training Epoch 0] Batch 532, Loss 0.6430705785751343\n",
      "[Training Epoch 0] Batch 533, Loss 0.6424946784973145\n",
      "[Training Epoch 0] Batch 534, Loss 0.6455956697463989\n",
      "[Training Epoch 0] Batch 535, Loss 0.6455432772636414\n",
      "[Training Epoch 0] Batch 536, Loss 0.64052414894104\n",
      "[Training Epoch 0] Batch 537, Loss 0.6487090587615967\n",
      "[Training Epoch 0] Batch 538, Loss 0.639071524143219\n",
      "[Training Epoch 0] Batch 539, Loss 0.6441535949707031\n",
      "[Training Epoch 0] Batch 540, Loss 0.6458141803741455\n",
      "[Training Epoch 0] Batch 541, Loss 0.641480028629303\n",
      "[Training Epoch 0] Batch 542, Loss 0.6428147554397583\n",
      "[Training Epoch 0] Batch 543, Loss 0.6424421072006226\n",
      "[Training Epoch 0] Batch 544, Loss 0.6393437385559082\n",
      "[Training Epoch 0] Batch 545, Loss 0.64349365234375\n",
      "[Training Epoch 0] Batch 546, Loss 0.6437895894050598\n",
      "[Training Epoch 0] Batch 547, Loss 0.6399950981140137\n",
      "[Training Epoch 0] Batch 548, Loss 0.6443060040473938\n",
      "[Training Epoch 0] Batch 549, Loss 0.6418567895889282\n",
      "[Training Epoch 0] Batch 550, Loss 0.64009690284729\n",
      "[Training Epoch 0] Batch 551, Loss 0.6452078223228455\n",
      "[Training Epoch 0] Batch 552, Loss 0.6394487619400024\n",
      "[Training Epoch 0] Batch 553, Loss 0.6416485905647278\n",
      "[Training Epoch 0] Batch 554, Loss 0.639401376247406\n",
      "[Training Epoch 0] Batch 555, Loss 0.6410040855407715\n",
      "[Training Epoch 0] Batch 556, Loss 0.6409130692481995\n",
      "[Training Epoch 0] Batch 557, Loss 0.6386686563491821\n",
      "[Training Epoch 0] Batch 558, Loss 0.6417833566665649\n",
      "[Training Epoch 0] Batch 559, Loss 0.6335574984550476\n",
      "[Training Epoch 0] Batch 560, Loss 0.6382336616516113\n",
      "[Training Epoch 0] Batch 561, Loss 0.6356415748596191\n",
      "[Training Epoch 0] Batch 562, Loss 0.6360628008842468\n",
      "[Training Epoch 0] Batch 563, Loss 0.6317498087882996\n",
      "[Training Epoch 0] Batch 564, Loss 0.6386632323265076\n",
      "[Training Epoch 0] Batch 565, Loss 0.6394044160842896\n",
      "[Training Epoch 0] Batch 566, Loss 0.6351830959320068\n",
      "[Training Epoch 0] Batch 567, Loss 0.6367272734642029\n",
      "[Training Epoch 0] Batch 568, Loss 0.6351446509361267\n",
      "[Training Epoch 0] Batch 569, Loss 0.6356481313705444\n",
      "[Training Epoch 0] Batch 570, Loss 0.6411739587783813\n",
      "[Training Epoch 0] Batch 571, Loss 0.6345452070236206\n",
      "[Training Epoch 0] Batch 572, Loss 0.6344108581542969\n",
      "[Training Epoch 0] Batch 573, Loss 0.6443380117416382\n",
      "[Training Epoch 0] Batch 574, Loss 0.6331770420074463\n",
      "[Training Epoch 0] Batch 575, Loss 0.6340304613113403\n",
      "[Training Epoch 0] Batch 576, Loss 0.6340183019638062\n",
      "[Training Epoch 0] Batch 577, Loss 0.634555995464325\n",
      "[Training Epoch 0] Batch 578, Loss 0.630319356918335\n",
      "[Training Epoch 0] Batch 579, Loss 0.6327428817749023\n",
      "[Training Epoch 0] Batch 580, Loss 0.6292504072189331\n",
      "[Training Epoch 0] Batch 581, Loss 0.6391820907592773\n",
      "[Training Epoch 0] Batch 582, Loss 0.633554995059967\n",
      "[Training Epoch 0] Batch 583, Loss 0.6307533979415894\n",
      "[Training Epoch 0] Batch 584, Loss 0.632492184638977\n",
      "[Training Epoch 0] Batch 585, Loss 0.6302634477615356\n",
      "[Training Epoch 0] Batch 586, Loss 0.6385837197303772\n",
      "[Training Epoch 0] Batch 587, Loss 0.6306283473968506\n",
      "[Training Epoch 0] Batch 588, Loss 0.6316919326782227\n",
      "[Training Epoch 0] Batch 589, Loss 0.6308273077011108\n",
      "[Training Epoch 0] Batch 590, Loss 0.6294808983802795\n",
      "[Training Epoch 0] Batch 591, Loss 0.6352449059486389\n",
      "[Training Epoch 0] Batch 592, Loss 0.6286303997039795\n",
      "[Training Epoch 0] Batch 593, Loss 0.6314001679420471\n",
      "[Training Epoch 0] Batch 594, Loss 0.6323940753936768\n",
      "[Training Epoch 0] Batch 595, Loss 0.6343832612037659\n",
      "[Training Epoch 0] Batch 596, Loss 0.628359317779541\n",
      "[Training Epoch 0] Batch 597, Loss 0.6330291628837585\n",
      "[Training Epoch 0] Batch 598, Loss 0.6295709609985352\n",
      "[Training Epoch 0] Batch 599, Loss 0.6308366656303406\n",
      "[Training Epoch 0] Batch 600, Loss 0.6297392845153809\n",
      "[Training Epoch 0] Batch 601, Loss 0.6316527724266052\n",
      "[Training Epoch 0] Batch 602, Loss 0.6270958185195923\n",
      "[Training Epoch 0] Batch 603, Loss 0.6325727105140686\n",
      "[Training Epoch 0] Batch 604, Loss 0.6293631196022034\n",
      "[Training Epoch 0] Batch 605, Loss 0.630245566368103\n",
      "[Training Epoch 0] Batch 606, Loss 0.6277281641960144\n",
      "[Training Epoch 0] Batch 607, Loss 0.6303554773330688\n",
      "[Training Epoch 0] Batch 608, Loss 0.6337666511535645\n",
      "[Training Epoch 0] Batch 609, Loss 0.6280002593994141\n",
      "[Training Epoch 0] Batch 610, Loss 0.6319888234138489\n",
      "[Training Epoch 0] Batch 611, Loss 0.629771888256073\n",
      "[Training Epoch 0] Batch 612, Loss 0.627156138420105\n",
      "[Training Epoch 0] Batch 613, Loss 0.627439022064209\n",
      "[Training Epoch 0] Batch 614, Loss 0.6248050928115845\n",
      "[Training Epoch 0] Batch 615, Loss 0.6308742165565491\n",
      "[Training Epoch 0] Batch 616, Loss 0.6274118423461914\n",
      "[Training Epoch 0] Batch 617, Loss 0.6300498247146606\n",
      "[Training Epoch 0] Batch 618, Loss 0.6291378736495972\n",
      "[Training Epoch 0] Batch 619, Loss 0.6308155059814453\n",
      "[Training Epoch 0] Batch 620, Loss 0.6280854940414429\n",
      "[Training Epoch 0] Batch 621, Loss 0.6266561150550842\n",
      "[Training Epoch 0] Batch 622, Loss 0.6269315481185913\n",
      "[Training Epoch 0] Batch 623, Loss 0.6276339292526245\n",
      "[Training Epoch 0] Batch 624, Loss 0.6227082014083862\n",
      "[Training Epoch 0] Batch 625, Loss 0.6234041452407837\n",
      "[Training Epoch 0] Batch 626, Loss 0.6229156851768494\n",
      "[Training Epoch 0] Batch 627, Loss 0.6215411424636841\n",
      "[Training Epoch 0] Batch 628, Loss 0.6236705780029297\n",
      "[Training Epoch 0] Batch 629, Loss 0.6277625560760498\n",
      "[Training Epoch 0] Batch 630, Loss 0.6185187101364136\n",
      "[Training Epoch 0] Batch 631, Loss 0.6283583641052246\n",
      "[Training Epoch 0] Batch 632, Loss 0.6222884654998779\n",
      "[Training Epoch 0] Batch 633, Loss 0.6210440397262573\n",
      "[Training Epoch 0] Batch 634, Loss 0.6213571429252625\n",
      "[Training Epoch 0] Batch 635, Loss 0.6321632266044617\n",
      "[Training Epoch 0] Batch 636, Loss 0.6257392764091492\n",
      "[Training Epoch 0] Batch 637, Loss 0.6268765926361084\n",
      "[Training Epoch 0] Batch 638, Loss 0.6264919638633728\n",
      "[Training Epoch 0] Batch 639, Loss 0.6245635747909546\n",
      "[Training Epoch 0] Batch 640, Loss 0.629310667514801\n",
      "[Training Epoch 0] Batch 641, Loss 0.6250851154327393\n",
      "[Training Epoch 0] Batch 642, Loss 0.6189642548561096\n",
      "[Training Epoch 0] Batch 643, Loss 0.6217069625854492\n",
      "[Training Epoch 0] Batch 644, Loss 0.6135892271995544\n",
      "[Training Epoch 0] Batch 645, Loss 0.6193434000015259\n",
      "[Training Epoch 0] Batch 646, Loss 0.6250801682472229\n",
      "[Training Epoch 0] Batch 647, Loss 0.6277421712875366\n",
      "[Training Epoch 0] Batch 648, Loss 0.6206418871879578\n",
      "[Training Epoch 0] Batch 649, Loss 0.6177679300308228\n",
      "[Training Epoch 0] Batch 650, Loss 0.6239858269691467\n",
      "[Training Epoch 0] Batch 651, Loss 0.6231023073196411\n",
      "[Training Epoch 0] Batch 652, Loss 0.6231366395950317\n",
      "[Training Epoch 0] Batch 653, Loss 0.6138389110565186\n",
      "[Training Epoch 0] Batch 654, Loss 0.6243941187858582\n",
      "[Training Epoch 0] Batch 655, Loss 0.6187283992767334\n",
      "[Training Epoch 0] Batch 656, Loss 0.6159030795097351\n",
      "[Training Epoch 0] Batch 657, Loss 0.6207998991012573\n",
      "[Training Epoch 0] Batch 658, Loss 0.6215565204620361\n",
      "[Training Epoch 0] Batch 659, Loss 0.6204841732978821\n",
      "[Training Epoch 0] Batch 660, Loss 0.6163207292556763\n",
      "[Training Epoch 0] Batch 661, Loss 0.6185972690582275\n",
      "[Training Epoch 0] Batch 662, Loss 0.6238747239112854\n",
      "[Training Epoch 0] Batch 663, Loss 0.6200987696647644\n",
      "[Training Epoch 0] Batch 664, Loss 0.6199166774749756\n",
      "[Training Epoch 0] Batch 665, Loss 0.6160628795623779\n",
      "[Training Epoch 0] Batch 666, Loss 0.6166959404945374\n",
      "[Training Epoch 0] Batch 667, Loss 0.6205323338508606\n",
      "[Training Epoch 0] Batch 668, Loss 0.6150325536727905\n",
      "[Training Epoch 0] Batch 669, Loss 0.618552565574646\n",
      "[Training Epoch 0] Batch 670, Loss 0.6192370653152466\n",
      "[Training Epoch 0] Batch 671, Loss 0.6197052001953125\n",
      "[Training Epoch 0] Batch 672, Loss 0.6157230734825134\n",
      "[Training Epoch 0] Batch 673, Loss 0.6171849966049194\n",
      "[Training Epoch 0] Batch 674, Loss 0.6141926050186157\n",
      "[Training Epoch 0] Batch 675, Loss 0.617969274520874\n",
      "[Training Epoch 0] Batch 676, Loss 0.6166629195213318\n",
      "[Training Epoch 0] Batch 677, Loss 0.616387665271759\n",
      "[Training Epoch 0] Batch 678, Loss 0.6164703965187073\n",
      "[Training Epoch 0] Batch 679, Loss 0.6107842922210693\n",
      "[Training Epoch 0] Batch 680, Loss 0.6131957173347473\n",
      "[Training Epoch 0] Batch 681, Loss 0.6197311878204346\n",
      "[Training Epoch 0] Batch 682, Loss 0.6124418377876282\n",
      "[Training Epoch 0] Batch 683, Loss 0.6117454171180725\n",
      "[Training Epoch 0] Batch 684, Loss 0.6124716401100159\n",
      "[Training Epoch 0] Batch 685, Loss 0.61118483543396\n",
      "[Training Epoch 0] Batch 686, Loss 0.6118755340576172\n",
      "[Training Epoch 0] Batch 687, Loss 0.6169044375419617\n",
      "[Training Epoch 0] Batch 688, Loss 0.612004280090332\n",
      "[Training Epoch 0] Batch 689, Loss 0.6119745969772339\n",
      "[Training Epoch 0] Batch 690, Loss 0.6156771779060364\n",
      "[Training Epoch 0] Batch 691, Loss 0.60595703125\n",
      "[Training Epoch 0] Batch 692, Loss 0.6204953193664551\n",
      "[Training Epoch 0] Batch 693, Loss 0.6148818135261536\n",
      "[Training Epoch 0] Batch 694, Loss 0.6137214303016663\n",
      "[Training Epoch 0] Batch 695, Loss 0.6122004389762878\n",
      "[Training Epoch 0] Batch 696, Loss 0.6130779981613159\n",
      "[Training Epoch 0] Batch 697, Loss 0.6164789199829102\n",
      "[Training Epoch 0] Batch 698, Loss 0.6152216196060181\n",
      "[Training Epoch 0] Batch 699, Loss 0.6155702471733093\n",
      "[Training Epoch 0] Batch 700, Loss 0.6108055114746094\n",
      "[Training Epoch 0] Batch 701, Loss 0.6157487630844116\n",
      "[Training Epoch 0] Batch 702, Loss 0.6113353371620178\n",
      "[Training Epoch 0] Batch 703, Loss 0.609734058380127\n",
      "[Training Epoch 0] Batch 704, Loss 0.6126234531402588\n",
      "[Training Epoch 0] Batch 705, Loss 0.6141130924224854\n",
      "[Training Epoch 0] Batch 706, Loss 0.6146251559257507\n",
      "[Training Epoch 0] Batch 707, Loss 0.6115735769271851\n",
      "[Training Epoch 0] Batch 708, Loss 0.6106335520744324\n",
      "[Training Epoch 0] Batch 709, Loss 0.6104335784912109\n",
      "[Training Epoch 0] Batch 710, Loss 0.6109563112258911\n",
      "[Training Epoch 0] Batch 711, Loss 0.6101152300834656\n",
      "[Training Epoch 0] Batch 712, Loss 0.6020220518112183\n",
      "[Training Epoch 0] Batch 713, Loss 0.6118419766426086\n",
      "[Training Epoch 0] Batch 714, Loss 0.6099113821983337\n",
      "[Training Epoch 0] Batch 715, Loss 0.6181707978248596\n",
      "[Training Epoch 0] Batch 716, Loss 0.6132969856262207\n",
      "[Training Epoch 0] Batch 717, Loss 0.6140514612197876\n",
      "[Training Epoch 0] Batch 718, Loss 0.6153159141540527\n",
      "[Training Epoch 0] Batch 719, Loss 0.6050238609313965\n",
      "[Training Epoch 0] Batch 720, Loss 0.6066340208053589\n",
      "[Training Epoch 0] Batch 721, Loss 0.6139553189277649\n",
      "[Training Epoch 0] Batch 722, Loss 0.6121708750724792\n",
      "[Training Epoch 0] Batch 723, Loss 0.6113239526748657\n",
      "[Training Epoch 0] Batch 724, Loss 0.6046549677848816\n",
      "[Training Epoch 0] Batch 725, Loss 0.6077858805656433\n",
      "[Training Epoch 0] Batch 726, Loss 0.6101621985435486\n",
      "[Training Epoch 0] Batch 727, Loss 0.61063152551651\n",
      "[Training Epoch 0] Batch 728, Loss 0.6081913113594055\n",
      "[Training Epoch 0] Batch 729, Loss 0.6052650213241577\n",
      "[Training Epoch 0] Batch 730, Loss 0.6043201088905334\n",
      "[Training Epoch 0] Batch 731, Loss 0.6090762615203857\n",
      "[Training Epoch 0] Batch 732, Loss 0.609772264957428\n",
      "[Training Epoch 0] Batch 733, Loss 0.6072080135345459\n",
      "[Training Epoch 0] Batch 734, Loss 0.6035003662109375\n",
      "[Training Epoch 0] Batch 735, Loss 0.5971618890762329\n",
      "[Training Epoch 0] Batch 736, Loss 0.6087228059768677\n",
      "[Training Epoch 0] Batch 737, Loss 0.6037919521331787\n",
      "[Training Epoch 0] Batch 738, Loss 0.6035968661308289\n",
      "[Training Epoch 0] Batch 739, Loss 0.6038981080055237\n",
      "[Training Epoch 0] Batch 740, Loss 0.6006253361701965\n",
      "[Training Epoch 0] Batch 741, Loss 0.6046116352081299\n",
      "[Training Epoch 0] Batch 742, Loss 0.6131919622421265\n",
      "[Training Epoch 0] Batch 743, Loss 0.6007845401763916\n",
      "[Training Epoch 0] Batch 744, Loss 0.609629213809967\n",
      "[Training Epoch 0] Batch 745, Loss 0.60621178150177\n",
      "[Training Epoch 0] Batch 746, Loss 0.6106306314468384\n",
      "[Training Epoch 0] Batch 747, Loss 0.6120665669441223\n",
      "[Training Epoch 0] Batch 748, Loss 0.6047481894493103\n",
      "[Training Epoch 0] Batch 749, Loss 0.6039676666259766\n",
      "[Training Epoch 0] Batch 750, Loss 0.601116955280304\n",
      "[Training Epoch 0] Batch 751, Loss 0.5953264236450195\n",
      "[Training Epoch 0] Batch 752, Loss 0.6052937507629395\n",
      "[Training Epoch 0] Batch 753, Loss 0.6037331819534302\n",
      "[Training Epoch 0] Batch 754, Loss 0.6058136224746704\n",
      "[Training Epoch 0] Batch 755, Loss 0.5997194051742554\n",
      "[Training Epoch 0] Batch 756, Loss 0.6044971942901611\n",
      "[Training Epoch 0] Batch 757, Loss 0.6057618260383606\n",
      "[Training Epoch 0] Batch 758, Loss 0.603217363357544\n",
      "[Training Epoch 0] Batch 759, Loss 0.6021187901496887\n",
      "[Training Epoch 0] Batch 760, Loss 0.6065953373908997\n",
      "[Training Epoch 0] Batch 761, Loss 0.5998581647872925\n",
      "[Training Epoch 0] Batch 762, Loss 0.5943590402603149\n",
      "[Training Epoch 0] Batch 763, Loss 0.6031877994537354\n",
      "[Training Epoch 0] Batch 764, Loss 0.600261926651001\n",
      "[Training Epoch 0] Batch 765, Loss 0.6027630567550659\n",
      "[Training Epoch 0] Batch 766, Loss 0.5966786742210388\n",
      "[Training Epoch 0] Batch 767, Loss 0.6014043092727661\n",
      "[Training Epoch 0] Batch 768, Loss 0.6087520122528076\n",
      "[Training Epoch 0] Batch 769, Loss 0.6025068759918213\n",
      "[Training Epoch 0] Batch 770, Loss 0.6009618639945984\n",
      "[Training Epoch 0] Batch 771, Loss 0.6009470820426941\n",
      "[Training Epoch 0] Batch 772, Loss 0.5969864130020142\n",
      "[Training Epoch 0] Batch 773, Loss 0.5984065532684326\n",
      "[Training Epoch 0] Batch 774, Loss 0.6000001430511475\n",
      "[Training Epoch 0] Batch 775, Loss 0.5999096632003784\n",
      "[Training Epoch 0] Batch 776, Loss 0.5979288816452026\n",
      "[Training Epoch 0] Batch 777, Loss 0.6008382439613342\n",
      "[Training Epoch 0] Batch 778, Loss 0.5982451438903809\n",
      "[Training Epoch 0] Batch 779, Loss 0.5947381854057312\n",
      "[Training Epoch 0] Batch 780, Loss 0.6029164791107178\n",
      "[Training Epoch 0] Batch 781, Loss 0.5964519381523132\n",
      "[Training Epoch 0] Batch 782, Loss 0.6026041507720947\n",
      "[Training Epoch 0] Batch 783, Loss 0.5957364439964294\n",
      "[Training Epoch 0] Batch 784, Loss 0.5968711972236633\n",
      "[Training Epoch 0] Batch 785, Loss 0.5980244874954224\n",
      "[Training Epoch 0] Batch 786, Loss 0.5928722620010376\n",
      "[Training Epoch 0] Batch 787, Loss 0.5983938574790955\n",
      "[Training Epoch 0] Batch 788, Loss 0.5923117995262146\n",
      "[Training Epoch 0] Batch 789, Loss 0.5926479697227478\n",
      "[Training Epoch 0] Batch 790, Loss 0.599906325340271\n",
      "[Training Epoch 0] Batch 791, Loss 0.5899211168289185\n",
      "[Training Epoch 0] Batch 792, Loss 0.600115180015564\n",
      "[Training Epoch 0] Batch 793, Loss 0.5853790640830994\n",
      "[Training Epoch 0] Batch 794, Loss 0.5970509648323059\n",
      "[Training Epoch 0] Batch 795, Loss 0.6013385653495789\n",
      "[Training Epoch 0] Batch 796, Loss 0.592326283454895\n",
      "[Training Epoch 0] Batch 797, Loss 0.5962895750999451\n",
      "[Training Epoch 0] Batch 798, Loss 0.5958756804466248\n",
      "[Training Epoch 0] Batch 799, Loss 0.5976250767707825\n",
      "[Training Epoch 0] Batch 800, Loss 0.5975382924079895\n",
      "[Training Epoch 0] Batch 801, Loss 0.5992788076400757\n",
      "[Training Epoch 0] Batch 802, Loss 0.5934422612190247\n",
      "[Training Epoch 0] Batch 803, Loss 0.5921247005462646\n",
      "[Training Epoch 0] Batch 804, Loss 0.5930443406105042\n",
      "[Training Epoch 0] Batch 805, Loss 0.6014043688774109\n",
      "[Training Epoch 0] Batch 806, Loss 0.5985276699066162\n",
      "[Training Epoch 0] Batch 807, Loss 0.5910879373550415\n",
      "[Training Epoch 0] Batch 808, Loss 0.5829634666442871\n",
      "[Training Epoch 0] Batch 809, Loss 0.5900803804397583\n",
      "[Training Epoch 0] Batch 810, Loss 0.5957536101341248\n",
      "[Training Epoch 0] Batch 811, Loss 0.5998376607894897\n",
      "[Training Epoch 0] Batch 812, Loss 0.5968595743179321\n",
      "[Training Epoch 0] Batch 813, Loss 0.5938239097595215\n",
      "[Training Epoch 0] Batch 814, Loss 0.5998094081878662\n",
      "[Training Epoch 0] Batch 815, Loss 0.5943366885185242\n",
      "[Training Epoch 0] Batch 816, Loss 0.5852231383323669\n",
      "[Training Epoch 0] Batch 817, Loss 0.5899925827980042\n",
      "[Training Epoch 0] Batch 818, Loss 0.5898621678352356\n",
      "[Training Epoch 0] Batch 819, Loss 0.5876551270484924\n",
      "[Training Epoch 0] Batch 820, Loss 0.5963581204414368\n",
      "[Training Epoch 0] Batch 821, Loss 0.5905098915100098\n",
      "[Training Epoch 0] Batch 822, Loss 0.6010043025016785\n",
      "[Training Epoch 0] Batch 823, Loss 0.5899354219436646\n",
      "[Training Epoch 0] Batch 824, Loss 0.5829389691352844\n",
      "[Training Epoch 0] Batch 825, Loss 0.5868136882781982\n",
      "[Training Epoch 0] Batch 826, Loss 0.5919155478477478\n",
      "[Training Epoch 0] Batch 827, Loss 0.5892831683158875\n",
      "[Training Epoch 0] Batch 828, Loss 0.5868812799453735\n",
      "[Training Epoch 0] Batch 829, Loss 0.5917192697525024\n",
      "[Training Epoch 0] Batch 830, Loss 0.5915912389755249\n",
      "[Training Epoch 0] Batch 831, Loss 0.5936880111694336\n",
      "[Training Epoch 0] Batch 832, Loss 0.5901170372962952\n",
      "[Training Epoch 0] Batch 833, Loss 0.5865715146064758\n",
      "[Training Epoch 0] Batch 834, Loss 0.6003500819206238\n",
      "[Training Epoch 0] Batch 835, Loss 0.5883517265319824\n",
      "[Training Epoch 0] Batch 836, Loss 0.5855519771575928\n",
      "[Training Epoch 0] Batch 837, Loss 0.5868615508079529\n",
      "[Training Epoch 0] Batch 838, Loss 0.5953056812286377\n",
      "[Training Epoch 0] Batch 839, Loss 0.592510461807251\n",
      "[Training Epoch 0] Batch 840, Loss 0.5960456132888794\n",
      "[Training Epoch 0] Batch 841, Loss 0.5862598419189453\n",
      "[Training Epoch 0] Batch 842, Loss 0.5805377960205078\n",
      "[Training Epoch 0] Batch 843, Loss 0.5874695777893066\n",
      "[Training Epoch 0] Batch 844, Loss 0.5915651321411133\n",
      "[Training Epoch 0] Batch 845, Loss 0.5902139544487\n",
      "[Training Epoch 0] Batch 846, Loss 0.592867374420166\n",
      "[Training Epoch 0] Batch 847, Loss 0.5829358100891113\n",
      "[Training Epoch 0] Batch 848, Loss 0.5802302360534668\n",
      "[Training Epoch 0] Batch 849, Loss 0.5935243368148804\n",
      "[Training Epoch 0] Batch 850, Loss 0.5836238861083984\n",
      "[Training Epoch 0] Batch 851, Loss 0.5909788608551025\n",
      "[Training Epoch 0] Batch 852, Loss 0.5885753631591797\n",
      "[Training Epoch 0] Batch 853, Loss 0.5820316076278687\n",
      "[Training Epoch 0] Batch 854, Loss 0.5927035212516785\n",
      "[Training Epoch 0] Batch 855, Loss 0.580793023109436\n",
      "[Training Epoch 0] Batch 856, Loss 0.5974364876747131\n",
      "[Training Epoch 0] Batch 857, Loss 0.5874029994010925\n",
      "[Training Epoch 0] Batch 858, Loss 0.5888993740081787\n",
      "[Training Epoch 0] Batch 859, Loss 0.5729930996894836\n",
      "[Training Epoch 0] Batch 860, Loss 0.5837169885635376\n",
      "[Training Epoch 0] Batch 861, Loss 0.5892889499664307\n",
      "[Training Epoch 0] Batch 862, Loss 0.5960338711738586\n",
      "[Training Epoch 0] Batch 863, Loss 0.5886218547821045\n",
      "[Training Epoch 0] Batch 864, Loss 0.5935510396957397\n",
      "[Training Epoch 0] Batch 865, Loss 0.5862979888916016\n",
      "[Training Epoch 0] Batch 866, Loss 0.5826182961463928\n",
      "[Training Epoch 0] Batch 867, Loss 0.5895305871963501\n",
      "[Training Epoch 0] Batch 868, Loss 0.5894259214401245\n",
      "[Training Epoch 0] Batch 869, Loss 0.5893088579177856\n",
      "[Training Epoch 0] Batch 870, Loss 0.5862460136413574\n",
      "[Training Epoch 0] Batch 871, Loss 0.576142430305481\n",
      "[Training Epoch 0] Batch 872, Loss 0.5934200882911682\n",
      "[Training Epoch 0] Batch 873, Loss 0.5879994034767151\n",
      "[Training Epoch 0] Batch 874, Loss 0.5862178802490234\n",
      "[Training Epoch 0] Batch 875, Loss 0.5735651254653931\n",
      "[Training Epoch 0] Batch 876, Loss 0.596680223941803\n",
      "[Training Epoch 0] Batch 877, Loss 0.5723196268081665\n",
      "[Training Epoch 0] Batch 878, Loss 0.582427978515625\n",
      "[Training Epoch 0] Batch 879, Loss 0.577130138874054\n",
      "[Training Epoch 0] Batch 880, Loss 0.5781758427619934\n",
      "[Training Epoch 0] Batch 881, Loss 0.580535888671875\n",
      "[Training Epoch 0] Batch 882, Loss 0.5817632675170898\n",
      "[Training Epoch 0] Batch 883, Loss 0.5848305821418762\n",
      "[Training Epoch 0] Batch 884, Loss 0.5817881226539612\n",
      "[Training Epoch 0] Batch 885, Loss 0.5812011957168579\n",
      "[Training Epoch 0] Batch 886, Loss 0.5747273564338684\n",
      "[Training Epoch 0] Batch 887, Loss 0.5842759013175964\n",
      "[Training Epoch 0] Batch 888, Loss 0.5938016176223755\n",
      "[Training Epoch 0] Batch 889, Loss 0.5698211193084717\n",
      "[Training Epoch 0] Batch 890, Loss 0.597210168838501\n",
      "[Training Epoch 0] Batch 891, Loss 0.5877014994621277\n",
      "[Training Epoch 0] Batch 892, Loss 0.570494532585144\n",
      "[Training Epoch 0] Batch 893, Loss 0.5795044898986816\n",
      "[Training Epoch 0] Batch 894, Loss 0.5833038687705994\n",
      "[Training Epoch 0] Batch 895, Loss 0.5852266550064087\n",
      "[Training Epoch 0] Batch 896, Loss 0.5817857980728149\n",
      "[Training Epoch 0] Batch 897, Loss 0.5783687233924866\n",
      "[Training Epoch 0] Batch 898, Loss 0.5853878259658813\n",
      "[Training Epoch 0] Batch 899, Loss 0.5783273577690125\n",
      "[Training Epoch 0] Batch 900, Loss 0.5786892175674438\n",
      "[Training Epoch 0] Batch 901, Loss 0.585128128528595\n",
      "[Training Epoch 0] Batch 902, Loss 0.580290675163269\n",
      "[Training Epoch 0] Batch 903, Loss 0.5805447101593018\n",
      "[Training Epoch 0] Batch 904, Loss 0.5794113278388977\n",
      "[Training Epoch 0] Batch 905, Loss 0.5831731557846069\n",
      "[Training Epoch 0] Batch 906, Loss 0.577459454536438\n",
      "[Training Epoch 0] Batch 907, Loss 0.5806340575218201\n",
      "[Training Epoch 0] Batch 908, Loss 0.5787716507911682\n",
      "[Training Epoch 0] Batch 909, Loss 0.5866038799285889\n",
      "[Training Epoch 0] Batch 910, Loss 0.5912971496582031\n",
      "[Training Epoch 0] Batch 911, Loss 0.5779065489768982\n",
      "[Training Epoch 0] Batch 912, Loss 0.575613260269165\n",
      "[Training Epoch 0] Batch 913, Loss 0.5730075836181641\n",
      "[Training Epoch 0] Batch 914, Loss 0.5808625817298889\n",
      "[Training Epoch 0] Batch 915, Loss 0.5796656608581543\n",
      "[Training Epoch 0] Batch 916, Loss 0.5671480298042297\n",
      "[Training Epoch 0] Batch 917, Loss 0.5876807570457458\n",
      "[Training Epoch 0] Batch 918, Loss 0.5765623450279236\n",
      "[Training Epoch 0] Batch 919, Loss 0.5703479051589966\n",
      "[Training Epoch 0] Batch 920, Loss 0.5788849592208862\n",
      "[Training Epoch 0] Batch 921, Loss 0.5707939267158508\n",
      "[Training Epoch 0] Batch 922, Loss 0.5746641755104065\n",
      "[Training Epoch 0] Batch 923, Loss 0.5744487643241882\n",
      "[Training Epoch 0] Batch 924, Loss 0.5760342478752136\n",
      "[Training Epoch 0] Batch 925, Loss 0.5761216878890991\n",
      "[Training Epoch 0] Batch 926, Loss 0.5685679316520691\n",
      "[Training Epoch 0] Batch 927, Loss 0.572063684463501\n",
      "[Training Epoch 0] Batch 928, Loss 0.5748775601387024\n",
      "[Training Epoch 0] Batch 929, Loss 0.5745952725410461\n",
      "[Training Epoch 0] Batch 930, Loss 0.5893537998199463\n",
      "[Training Epoch 0] Batch 931, Loss 0.5832815766334534\n",
      "[Training Epoch 0] Batch 932, Loss 0.5787731409072876\n",
      "[Training Epoch 0] Batch 933, Loss 0.5822396278381348\n",
      "[Training Epoch 0] Batch 934, Loss 0.5744909048080444\n",
      "[Training Epoch 0] Batch 935, Loss 0.5797611474990845\n",
      "[Training Epoch 0] Batch 936, Loss 0.574050784111023\n",
      "[Training Epoch 0] Batch 937, Loss 0.5855852365493774\n",
      "[Training Epoch 0] Batch 938, Loss 0.5726210474967957\n",
      "[Training Epoch 0] Batch 939, Loss 0.5761126279830933\n",
      "[Training Epoch 0] Batch 940, Loss 0.5761008858680725\n",
      "[Training Epoch 0] Batch 941, Loss 0.5932067036628723\n",
      "[Training Epoch 0] Batch 942, Loss 0.5739696025848389\n",
      "[Training Epoch 0] Batch 943, Loss 0.566972017288208\n",
      "[Training Epoch 0] Batch 944, Loss 0.5772327184677124\n",
      "[Training Epoch 0] Batch 945, Loss 0.5773528814315796\n",
      "[Training Epoch 0] Batch 946, Loss 0.5819267630577087\n",
      "[Training Epoch 0] Batch 947, Loss 0.5704032182693481\n",
      "[Training Epoch 0] Batch 948, Loss 0.5714355111122131\n",
      "[Training Epoch 0] Batch 949, Loss 0.5702230334281921\n",
      "[Training Epoch 0] Batch 950, Loss 0.5701080560684204\n",
      "[Training Epoch 0] Batch 951, Loss 0.57764732837677\n",
      "[Training Epoch 0] Batch 952, Loss 0.5736910104751587\n",
      "[Training Epoch 0] Batch 953, Loss 0.5826445817947388\n",
      "[Training Epoch 0] Batch 954, Loss 0.570738673210144\n",
      "[Training Epoch 0] Batch 955, Loss 0.5776925683021545\n",
      "[Training Epoch 0] Batch 956, Loss 0.5767698287963867\n",
      "[Training Epoch 0] Batch 957, Loss 0.5676978826522827\n",
      "[Training Epoch 0] Batch 958, Loss 0.5646965503692627\n",
      "[Training Epoch 0] Batch 959, Loss 0.569507896900177\n",
      "[Training Epoch 0] Batch 960, Loss 0.5684655904769897\n",
      "[Training Epoch 0] Batch 961, Loss 0.563463032245636\n",
      "[Training Epoch 0] Batch 962, Loss 0.5695497989654541\n",
      "[Training Epoch 0] Batch 963, Loss 0.573101818561554\n",
      "[Training Epoch 0] Batch 964, Loss 0.5745542049407959\n",
      "[Training Epoch 0] Batch 965, Loss 0.564863383769989\n",
      "[Training Epoch 0] Batch 966, Loss 0.5693387985229492\n",
      "[Training Epoch 0] Batch 967, Loss 0.5717807412147522\n",
      "[Training Epoch 0] Batch 968, Loss 0.5710110068321228\n",
      "[Training Epoch 0] Batch 969, Loss 0.5818935632705688\n",
      "[Training Epoch 0] Batch 970, Loss 0.5686427354812622\n",
      "[Training Epoch 0] Batch 971, Loss 0.5854868292808533\n",
      "[Training Epoch 0] Batch 972, Loss 0.5606704354286194\n",
      "[Training Epoch 0] Batch 973, Loss 0.5625020861625671\n",
      "[Training Epoch 0] Batch 974, Loss 0.566585123538971\n",
      "[Training Epoch 0] Batch 975, Loss 0.5682642459869385\n",
      "[Training Epoch 0] Batch 976, Loss 0.5681225657463074\n",
      "[Training Epoch 0] Batch 977, Loss 0.5918525457382202\n",
      "[Training Epoch 0] Batch 978, Loss 0.5752301812171936\n",
      "[Training Epoch 0] Batch 979, Loss 0.567067563533783\n",
      "[Training Epoch 0] Batch 980, Loss 0.5740828514099121\n",
      "[Training Epoch 0] Batch 981, Loss 0.5636443495750427\n",
      "[Training Epoch 0] Batch 982, Loss 0.577359676361084\n",
      "[Training Epoch 0] Batch 983, Loss 0.5741854310035706\n",
      "[Training Epoch 0] Batch 984, Loss 0.566148042678833\n",
      "[Training Epoch 0] Batch 985, Loss 0.5647911429405212\n",
      "[Training Epoch 0] Batch 986, Loss 0.5633266568183899\n",
      "[Training Epoch 0] Batch 987, Loss 0.5675225257873535\n",
      "[Training Epoch 0] Batch 988, Loss 0.5617567896842957\n",
      "[Training Epoch 0] Batch 989, Loss 0.5784719586372375\n",
      "[Training Epoch 0] Batch 990, Loss 0.5620675683021545\n",
      "[Training Epoch 0] Batch 991, Loss 0.5734290480613708\n",
      "[Training Epoch 0] Batch 992, Loss 0.5731037259101868\n",
      "[Training Epoch 0] Batch 993, Loss 0.5736492872238159\n",
      "[Training Epoch 0] Batch 994, Loss 0.5793466567993164\n",
      "[Training Epoch 0] Batch 995, Loss 0.567646861076355\n",
      "[Training Epoch 0] Batch 996, Loss 0.5688571333885193\n",
      "[Training Epoch 0] Batch 997, Loss 0.5666344165802002\n",
      "[Training Epoch 0] Batch 998, Loss 0.5634955167770386\n",
      "[Training Epoch 0] Batch 999, Loss 0.568908154964447\n",
      "[Training Epoch 0] Batch 1000, Loss 0.5692239999771118\n",
      "[Training Epoch 0] Batch 1001, Loss 0.5685022473335266\n",
      "[Training Epoch 0] Batch 1002, Loss 0.5748470425605774\n",
      "[Training Epoch 0] Batch 1003, Loss 0.5780138969421387\n",
      "[Training Epoch 0] Batch 1004, Loss 0.5824925899505615\n",
      "[Training Epoch 0] Batch 1005, Loss 0.5783993601799011\n",
      "[Training Epoch 0] Batch 1006, Loss 0.5713083744049072\n",
      "[Training Epoch 0] Batch 1007, Loss 0.5733829736709595\n",
      "[Training Epoch 0] Batch 1008, Loss 0.5706702470779419\n",
      "[Training Epoch 0] Batch 1009, Loss 0.5673440098762512\n",
      "[Training Epoch 0] Batch 1010, Loss 0.569922149181366\n",
      "[Training Epoch 0] Batch 1011, Loss 0.5708956122398376\n",
      "[Training Epoch 0] Batch 1012, Loss 0.5652452707290649\n",
      "[Training Epoch 0] Batch 1013, Loss 0.5713508129119873\n",
      "[Training Epoch 0] Batch 1014, Loss 0.5780299305915833\n",
      "[Training Epoch 0] Batch 1015, Loss 0.5695004463195801\n",
      "[Training Epoch 0] Batch 1016, Loss 0.5659694075584412\n",
      "[Training Epoch 0] Batch 1017, Loss 0.5612350702285767\n",
      "[Training Epoch 0] Batch 1018, Loss 0.5690681338310242\n",
      "[Training Epoch 0] Batch 1019, Loss 0.5724446773529053\n",
      "[Training Epoch 0] Batch 1020, Loss 0.5612819194793701\n",
      "[Training Epoch 0] Batch 1021, Loss 0.5641433000564575\n",
      "[Training Epoch 0] Batch 1022, Loss 0.5698066353797913\n",
      "[Training Epoch 0] Batch 1023, Loss 0.5654783248901367\n",
      "[Training Epoch 0] Batch 1024, Loss 0.5639733672142029\n",
      "[Training Epoch 0] Batch 1025, Loss 0.5661916136741638\n",
      "[Training Epoch 0] Batch 1026, Loss 0.560431718826294\n",
      "[Training Epoch 0] Batch 1027, Loss 0.5596759915351868\n",
      "[Training Epoch 0] Batch 1028, Loss 0.5613874793052673\n",
      "[Training Epoch 0] Batch 1029, Loss 0.5731650590896606\n",
      "[Training Epoch 0] Batch 1030, Loss 0.5611684322357178\n",
      "[Training Epoch 0] Batch 1031, Loss 0.5621513724327087\n",
      "[Training Epoch 0] Batch 1032, Loss 0.5652700066566467\n",
      "[Training Epoch 0] Batch 1033, Loss 0.5646523237228394\n",
      "[Training Epoch 0] Batch 1034, Loss 0.5515596866607666\n",
      "[Training Epoch 0] Batch 1035, Loss 0.5606998205184937\n",
      "[Training Epoch 0] Batch 1036, Loss 0.5647153258323669\n",
      "[Training Epoch 0] Batch 1037, Loss 0.5672423839569092\n",
      "[Training Epoch 0] Batch 1038, Loss 0.5731750726699829\n",
      "[Training Epoch 0] Batch 1039, Loss 0.5786091089248657\n",
      "[Training Epoch 0] Batch 1040, Loss 0.5510990619659424\n",
      "[Training Epoch 0] Batch 1041, Loss 0.5503509044647217\n",
      "[Training Epoch 0] Batch 1042, Loss 0.5698844194412231\n",
      "[Training Epoch 0] Batch 1043, Loss 0.5773266553878784\n",
      "[Training Epoch 0] Batch 1044, Loss 0.5676885843276978\n",
      "[Training Epoch 0] Batch 1045, Loss 0.5653929710388184\n",
      "[Training Epoch 0] Batch 1046, Loss 0.5770730972290039\n",
      "[Training Epoch 0] Batch 1047, Loss 0.5738683938980103\n",
      "[Training Epoch 0] Batch 1048, Loss 0.5626261830329895\n",
      "[Training Epoch 0] Batch 1049, Loss 0.5842518210411072\n",
      "[Training Epoch 0] Batch 1050, Loss 0.5685839653015137\n",
      "[Training Epoch 0] Batch 1051, Loss 0.554826021194458\n",
      "[Training Epoch 0] Batch 1052, Loss 0.5733911991119385\n",
      "[Training Epoch 0] Batch 1053, Loss 0.5600225925445557\n",
      "[Training Epoch 0] Batch 1054, Loss 0.5517060160636902\n",
      "[Training Epoch 0] Batch 1055, Loss 0.5540074706077576\n",
      "[Training Epoch 0] Batch 1056, Loss 0.5768482089042664\n",
      "[Training Epoch 0] Batch 1057, Loss 0.5669530630111694\n",
      "[Training Epoch 0] Batch 1058, Loss 0.5576332807540894\n",
      "[Training Epoch 0] Batch 1059, Loss 0.5449402928352356\n",
      "[Training Epoch 0] Batch 1060, Loss 0.553992748260498\n",
      "[Training Epoch 0] Batch 1061, Loss 0.5598251819610596\n",
      "[Training Epoch 0] Batch 1062, Loss 0.5598673224449158\n",
      "[Training Epoch 0] Batch 1063, Loss 0.5614029169082642\n",
      "[Training Epoch 0] Batch 1064, Loss 0.556076169013977\n",
      "[Training Epoch 0] Batch 1065, Loss 0.5589193105697632\n",
      "[Training Epoch 0] Batch 1066, Loss 0.5490441918373108\n",
      "[Training Epoch 0] Batch 1067, Loss 0.5561637878417969\n",
      "[Training Epoch 0] Batch 1068, Loss 0.5557555556297302\n",
      "[Training Epoch 0] Batch 1069, Loss 0.5587458610534668\n",
      "[Training Epoch 0] Batch 1070, Loss 0.5699480175971985\n",
      "[Training Epoch 0] Batch 1071, Loss 0.5665115118026733\n",
      "[Training Epoch 0] Batch 1072, Loss 0.5681027173995972\n",
      "[Training Epoch 0] Batch 1073, Loss 0.5565128922462463\n",
      "[Training Epoch 0] Batch 1074, Loss 0.5521381497383118\n",
      "[Training Epoch 0] Batch 1075, Loss 0.560652494430542\n",
      "[Training Epoch 0] Batch 1076, Loss 0.5626564025878906\n",
      "[Training Epoch 0] Batch 1077, Loss 0.5502652525901794\n",
      "[Training Epoch 0] Batch 1078, Loss 0.5670803189277649\n",
      "[Training Epoch 0] Batch 1079, Loss 0.5664027333259583\n",
      "[Training Epoch 0] Batch 1080, Loss 0.557421088218689\n",
      "[Training Epoch 0] Batch 1081, Loss 0.5568686723709106\n",
      "[Training Epoch 0] Batch 1082, Loss 0.5544414520263672\n",
      "[Training Epoch 0] Batch 1083, Loss 0.5581807494163513\n",
      "[Training Epoch 0] Batch 1084, Loss 0.5608199834823608\n",
      "[Training Epoch 0] Batch 1085, Loss 0.5693564414978027\n",
      "[Training Epoch 0] Batch 1086, Loss 0.5572874546051025\n",
      "[Training Epoch 0] Batch 1087, Loss 0.5634004473686218\n",
      "[Training Epoch 0] Batch 1088, Loss 0.5655090808868408\n",
      "[Training Epoch 0] Batch 1089, Loss 0.5724555253982544\n",
      "[Training Epoch 0] Batch 1090, Loss 0.552514910697937\n",
      "[Training Epoch 0] Batch 1091, Loss 0.5589413642883301\n",
      "[Training Epoch 0] Batch 1092, Loss 0.5640679001808167\n",
      "[Training Epoch 0] Batch 1093, Loss 0.5628610253334045\n",
      "[Training Epoch 0] Batch 1094, Loss 0.5553673505783081\n",
      "[Training Epoch 0] Batch 1095, Loss 0.5579410195350647\n",
      "[Training Epoch 0] Batch 1096, Loss 0.5524922609329224\n",
      "[Training Epoch 0] Batch 1097, Loss 0.553239107131958\n",
      "[Training Epoch 0] Batch 1098, Loss 0.5682522058486938\n",
      "[Training Epoch 0] Batch 1099, Loss 0.5601577758789062\n",
      "[Training Epoch 0] Batch 1100, Loss 0.5601537823677063\n",
      "[Training Epoch 0] Batch 1101, Loss 0.5411396622657776\n",
      "[Training Epoch 0] Batch 1102, Loss 0.5699790716171265\n",
      "[Training Epoch 0] Batch 1103, Loss 0.548656165599823\n",
      "[Training Epoch 0] Batch 1104, Loss 0.5631334185600281\n",
      "[Training Epoch 0] Batch 1105, Loss 0.5635258555412292\n",
      "[Training Epoch 0] Batch 1106, Loss 0.5549107789993286\n",
      "[Training Epoch 0] Batch 1107, Loss 0.5643285512924194\n",
      "[Training Epoch 0] Batch 1108, Loss 0.5548298954963684\n",
      "[Training Epoch 0] Batch 1109, Loss 0.5659704208374023\n",
      "[Training Epoch 0] Batch 1110, Loss 0.5513704419136047\n",
      "[Training Epoch 0] Batch 1111, Loss 0.5667740702629089\n",
      "[Training Epoch 0] Batch 1112, Loss 0.5534623861312866\n",
      "[Training Epoch 0] Batch 1113, Loss 0.5609700083732605\n",
      "[Training Epoch 0] Batch 1114, Loss 0.5435624122619629\n",
      "[Training Epoch 0] Batch 1115, Loss 0.5647470355033875\n",
      "[Training Epoch 0] Batch 1116, Loss 0.5510468482971191\n",
      "[Training Epoch 0] Batch 1117, Loss 0.541917085647583\n",
      "[Training Epoch 0] Batch 1118, Loss 0.5549914240837097\n",
      "[Training Epoch 0] Batch 1119, Loss 0.5542458891868591\n",
      "[Training Epoch 0] Batch 1120, Loss 0.5645495057106018\n",
      "[Training Epoch 0] Batch 1121, Loss 0.5495815277099609\n",
      "[Training Epoch 0] Batch 1122, Loss 0.5498150587081909\n",
      "[Training Epoch 0] Batch 1123, Loss 0.5657159090042114\n",
      "[Training Epoch 0] Batch 1124, Loss 0.5571189522743225\n",
      "[Training Epoch 0] Batch 1125, Loss 0.5643928050994873\n",
      "[Training Epoch 0] Batch 1126, Loss 0.5704936981201172\n",
      "[Training Epoch 0] Batch 1127, Loss 0.5501828193664551\n",
      "[Training Epoch 0] Batch 1128, Loss 0.5479989051818848\n",
      "[Training Epoch 0] Batch 1129, Loss 0.5587533712387085\n",
      "[Training Epoch 0] Batch 1130, Loss 0.5568142533302307\n",
      "[Training Epoch 0] Batch 1131, Loss 0.555694580078125\n",
      "[Training Epoch 0] Batch 1132, Loss 0.550187885761261\n",
      "[Training Epoch 0] Batch 1133, Loss 0.5566515326499939\n",
      "[Training Epoch 0] Batch 1134, Loss 0.5561747550964355\n",
      "[Training Epoch 0] Batch 1135, Loss 0.5499790906906128\n",
      "[Training Epoch 0] Batch 1136, Loss 0.5510923862457275\n",
      "[Training Epoch 0] Batch 1137, Loss 0.5421342849731445\n",
      "[Training Epoch 0] Batch 1138, Loss 0.548420786857605\n",
      "[Training Epoch 0] Batch 1139, Loss 0.5607661604881287\n",
      "[Training Epoch 0] Batch 1140, Loss 0.5663012266159058\n",
      "[Training Epoch 0] Batch 1141, Loss 0.5617254376411438\n",
      "[Training Epoch 0] Batch 1142, Loss 0.557237982749939\n",
      "[Training Epoch 0] Batch 1143, Loss 0.5654675960540771\n",
      "[Training Epoch 0] Batch 1144, Loss 0.5469837784767151\n",
      "[Training Epoch 0] Batch 1145, Loss 0.5518662929534912\n",
      "[Training Epoch 0] Batch 1146, Loss 0.5529552102088928\n",
      "[Training Epoch 0] Batch 1147, Loss 0.5582367777824402\n",
      "[Training Epoch 0] Batch 1148, Loss 0.5581347346305847\n",
      "[Training Epoch 0] Batch 1149, Loss 0.5467122793197632\n",
      "[Training Epoch 0] Batch 1150, Loss 0.5565872192382812\n",
      "[Training Epoch 0] Batch 1151, Loss 0.537368655204773\n",
      "[Training Epoch 0] Batch 1152, Loss 0.5499520897865295\n",
      "[Training Epoch 0] Batch 1153, Loss 0.5570690631866455\n",
      "[Training Epoch 0] Batch 1154, Loss 0.5612354278564453\n",
      "[Training Epoch 0] Batch 1155, Loss 0.560619056224823\n",
      "[Training Epoch 0] Batch 1156, Loss 0.5433788299560547\n",
      "[Training Epoch 0] Batch 1157, Loss 0.5463147163391113\n",
      "[Training Epoch 0] Batch 1158, Loss 0.5618444085121155\n",
      "[Training Epoch 0] Batch 1159, Loss 0.5490230321884155\n",
      "[Training Epoch 0] Batch 1160, Loss 0.5466698408126831\n",
      "[Training Epoch 0] Batch 1161, Loss 0.5509545803070068\n",
      "[Training Epoch 0] Batch 1162, Loss 0.5502166748046875\n",
      "[Training Epoch 0] Batch 1163, Loss 0.5465897917747498\n",
      "[Training Epoch 0] Batch 1164, Loss 0.5511521100997925\n",
      "[Training Epoch 0] Batch 1165, Loss 0.548088550567627\n",
      "[Training Epoch 0] Batch 1166, Loss 0.5317742228507996\n",
      "[Training Epoch 0] Batch 1167, Loss 0.5565046072006226\n",
      "[Training Epoch 0] Batch 1168, Loss 0.5483019948005676\n",
      "[Training Epoch 0] Batch 1169, Loss 0.5565330982208252\n",
      "[Training Epoch 0] Batch 1170, Loss 0.5568090081214905\n",
      "[Training Epoch 0] Batch 1171, Loss 0.5565729141235352\n",
      "[Training Epoch 0] Batch 1172, Loss 0.5493423938751221\n",
      "[Training Epoch 0] Batch 1173, Loss 0.546772301197052\n",
      "[Training Epoch 0] Batch 1174, Loss 0.5573177933692932\n",
      "[Training Epoch 0] Batch 1175, Loss 0.5443124771118164\n",
      "[Training Epoch 0] Batch 1176, Loss 0.5556325316429138\n",
      "[Training Epoch 0] Batch 1177, Loss 0.5572150349617004\n",
      "[Training Epoch 0] Batch 1178, Loss 0.5547927021980286\n",
      "[Training Epoch 0] Batch 1179, Loss 0.5341156721115112\n",
      "[Training Epoch 0] Batch 1180, Loss 0.5592143535614014\n",
      "[Training Epoch 0] Batch 1181, Loss 0.5515173077583313\n",
      "[Training Epoch 0] Batch 1182, Loss 0.5466694831848145\n",
      "[Training Epoch 0] Batch 1183, Loss 0.5599807500839233\n",
      "[Training Epoch 0] Batch 1184, Loss 0.5653060674667358\n",
      "[Training Epoch 0] Batch 1185, Loss 0.5519117712974548\n",
      "[Training Epoch 0] Batch 1186, Loss 0.5467790961265564\n",
      "[Training Epoch 0] Batch 1187, Loss 0.572229266166687\n",
      "[Training Epoch 0] Batch 1188, Loss 0.5466920733451843\n",
      "[Training Epoch 0] Batch 1189, Loss 0.5477238297462463\n",
      "[Training Epoch 0] Batch 1190, Loss 0.5447306632995605\n",
      "[Training Epoch 0] Batch 1191, Loss 0.5503795146942139\n",
      "[Training Epoch 0] Batch 1192, Loss 0.5384008884429932\n",
      "[Training Epoch 0] Batch 1193, Loss 0.5486581921577454\n",
      "[Training Epoch 0] Batch 1194, Loss 0.5642275810241699\n",
      "[Training Epoch 0] Batch 1195, Loss 0.5594098567962646\n",
      "[Training Epoch 0] Batch 1196, Loss 0.5455740690231323\n",
      "[Training Epoch 0] Batch 1197, Loss 0.5375992655754089\n",
      "[Training Epoch 0] Batch 1198, Loss 0.5575653910636902\n",
      "[Training Epoch 0] Batch 1199, Loss 0.5526636838912964\n",
      "[Training Epoch 0] Batch 1200, Loss 0.5708808898925781\n",
      "[Training Epoch 0] Batch 1201, Loss 0.5509999990463257\n",
      "[Training Epoch 0] Batch 1202, Loss 0.5479791164398193\n",
      "[Training Epoch 0] Batch 1203, Loss 0.5430471301078796\n",
      "[Training Epoch 0] Batch 1204, Loss 0.5496124625205994\n",
      "[Training Epoch 0] Batch 1205, Loss 0.5532145500183105\n",
      "[Training Epoch 0] Batch 1206, Loss 0.5444018840789795\n",
      "[Training Epoch 0] Batch 1207, Loss 0.5455722808837891\n",
      "[Training Epoch 0] Batch 1208, Loss 0.5473776459693909\n",
      "[Training Epoch 0] Batch 1209, Loss 0.5498512983322144\n",
      "[Training Epoch 0] Batch 1210, Loss 0.5341083407402039\n",
      "[Training Epoch 0] Batch 1211, Loss 0.5385513305664062\n",
      "[Training Epoch 0] Batch 1212, Loss 0.5308992266654968\n",
      "[Training Epoch 0] Batch 1213, Loss 0.5469911694526672\n",
      "[Training Epoch 0] Batch 1214, Loss 0.5598514080047607\n",
      "[Training Epoch 0] Batch 1215, Loss 0.5482478141784668\n",
      "[Training Epoch 0] Batch 1216, Loss 0.5486845374107361\n",
      "[Training Epoch 0] Batch 1217, Loss 0.5425124168395996\n",
      "[Training Epoch 0] Batch 1218, Loss 0.5437003970146179\n",
      "[Training Epoch 0] Batch 1219, Loss 0.5484187602996826\n",
      "[Training Epoch 0] Batch 1220, Loss 0.5319651961326599\n",
      "[Training Epoch 0] Batch 1221, Loss 0.5438269972801208\n",
      "[Training Epoch 0] Batch 1222, Loss 0.5469189882278442\n",
      "[Training Epoch 0] Batch 1223, Loss 0.545946478843689\n",
      "[Training Epoch 0] Batch 1224, Loss 0.5538318753242493\n",
      "[Training Epoch 0] Batch 1225, Loss 0.5581120252609253\n",
      "[Training Epoch 0] Batch 1226, Loss 0.53653883934021\n",
      "[Training Epoch 0] Batch 1227, Loss 0.560674786567688\n",
      "[Training Epoch 0] Batch 1228, Loss 0.5492805242538452\n",
      "[Training Epoch 0] Batch 1229, Loss 0.5475329160690308\n",
      "[Training Epoch 0] Batch 1230, Loss 0.5501949191093445\n",
      "[Training Epoch 0] Batch 1231, Loss 0.5513578057289124\n",
      "[Training Epoch 0] Batch 1232, Loss 0.5379465222358704\n",
      "[Training Epoch 0] Batch 1233, Loss 0.5491713881492615\n",
      "[Training Epoch 0] Batch 1234, Loss 0.5431525707244873\n",
      "[Training Epoch 0] Batch 1235, Loss 0.5375396013259888\n",
      "[Training Epoch 0] Batch 1236, Loss 0.5530104637145996\n",
      "[Training Epoch 0] Batch 1237, Loss 0.5423732995986938\n",
      "[Training Epoch 0] Batch 1238, Loss 0.5255842804908752\n",
      "[Training Epoch 0] Batch 1239, Loss 0.530036211013794\n",
      "[Training Epoch 0] Batch 1240, Loss 0.5573870539665222\n",
      "[Training Epoch 0] Batch 1241, Loss 0.5704232454299927\n",
      "[Training Epoch 0] Batch 1242, Loss 0.5652889013290405\n",
      "[Training Epoch 0] Batch 1243, Loss 0.5343366861343384\n",
      "[Training Epoch 0] Batch 1244, Loss 0.5504406094551086\n",
      "[Training Epoch 0] Batch 1245, Loss 0.5395004749298096\n",
      "[Training Epoch 0] Batch 1246, Loss 0.5333559513092041\n",
      "[Training Epoch 0] Batch 1247, Loss 0.5459035634994507\n",
      "[Training Epoch 0] Batch 1248, Loss 0.5348597764968872\n",
      "[Training Epoch 0] Batch 1249, Loss 0.5510607361793518\n",
      "[Training Epoch 0] Batch 1250, Loss 0.5551475286483765\n",
      "[Training Epoch 0] Batch 1251, Loss 0.5423774719238281\n",
      "[Training Epoch 0] Batch 1252, Loss 0.5515491962432861\n",
      "[Training Epoch 0] Batch 1253, Loss 0.5353251695632935\n",
      "[Training Epoch 0] Batch 1254, Loss 0.5395916104316711\n",
      "[Training Epoch 0] Batch 1255, Loss 0.5307209491729736\n",
      "[Training Epoch 0] Batch 1256, Loss 0.5453765988349915\n",
      "[Training Epoch 0] Batch 1257, Loss 0.5424734354019165\n",
      "[Training Epoch 0] Batch 1258, Loss 0.5403045415878296\n",
      "[Training Epoch 0] Batch 1259, Loss 0.541600227355957\n",
      "[Training Epoch 0] Batch 1260, Loss 0.5493730306625366\n",
      "[Training Epoch 0] Batch 1261, Loss 0.5608015060424805\n",
      "[Training Epoch 0] Batch 1262, Loss 0.5527325868606567\n",
      "[Training Epoch 0] Batch 1263, Loss 0.559756338596344\n",
      "[Training Epoch 0] Batch 1264, Loss 0.5291357636451721\n",
      "[Training Epoch 0] Batch 1265, Loss 0.5379458665847778\n",
      "[Training Epoch 0] Batch 1266, Loss 0.550758957862854\n",
      "[Training Epoch 0] Batch 1267, Loss 0.5581885576248169\n",
      "[Training Epoch 0] Batch 1268, Loss 0.5469406247138977\n",
      "[Training Epoch 0] Batch 1269, Loss 0.5429530739784241\n",
      "[Training Epoch 0] Batch 1270, Loss 0.5544770956039429\n",
      "[Training Epoch 0] Batch 1271, Loss 0.5315548181533813\n",
      "[Training Epoch 0] Batch 1272, Loss 0.5581375956535339\n",
      "[Training Epoch 0] Batch 1273, Loss 0.5683451890945435\n",
      "[Training Epoch 0] Batch 1274, Loss 0.5369798541069031\n",
      "[Training Epoch 0] Batch 1275, Loss 0.5555869340896606\n",
      "[Training Epoch 0] Batch 1276, Loss 0.545864462852478\n",
      "[Training Epoch 0] Batch 1277, Loss 0.5434492826461792\n",
      "[Training Epoch 0] Batch 1278, Loss 0.536795437335968\n",
      "[Training Epoch 0] Batch 1279, Loss 0.5515320301055908\n",
      "[Training Epoch 0] Batch 1280, Loss 0.5455325245857239\n",
      "[Training Epoch 0] Batch 1281, Loss 0.546205997467041\n",
      "[Training Epoch 0] Batch 1282, Loss 0.537611722946167\n",
      "[Training Epoch 0] Batch 1283, Loss 0.5227925777435303\n",
      "[Training Epoch 0] Batch 1284, Loss 0.5420184135437012\n",
      "[Training Epoch 0] Batch 1285, Loss 0.5330131649971008\n",
      "[Training Epoch 0] Batch 1286, Loss 0.5526646375656128\n",
      "[Training Epoch 0] Batch 1287, Loss 0.5410480499267578\n",
      "[Training Epoch 0] Batch 1288, Loss 0.5311694741249084\n",
      "[Training Epoch 0] Batch 1289, Loss 0.5476456880569458\n",
      "[Training Epoch 0] Batch 1290, Loss 0.5456293225288391\n",
      "[Training Epoch 0] Batch 1291, Loss 0.5366577506065369\n",
      "[Training Epoch 0] Batch 1292, Loss 0.5503789782524109\n",
      "[Training Epoch 0] Batch 1293, Loss 0.5251191258430481\n",
      "[Training Epoch 0] Batch 1294, Loss 0.5348301529884338\n",
      "[Training Epoch 0] Batch 1295, Loss 0.5540837645530701\n",
      "[Training Epoch 0] Batch 1296, Loss 0.549260139465332\n",
      "[Training Epoch 0] Batch 1297, Loss 0.5367077589035034\n",
      "[Training Epoch 0] Batch 1298, Loss 0.5274108052253723\n",
      "[Training Epoch 0] Batch 1299, Loss 0.5476686358451843\n",
      "[Training Epoch 0] Batch 1300, Loss 0.5310764908790588\n",
      "[Training Epoch 0] Batch 1301, Loss 0.5301494598388672\n",
      "[Training Epoch 0] Batch 1302, Loss 0.5265768766403198\n",
      "[Training Epoch 0] Batch 1303, Loss 0.5424416065216064\n",
      "[Training Epoch 0] Batch 1304, Loss 0.5349148511886597\n",
      "[Training Epoch 0] Batch 1305, Loss 0.5480461120605469\n",
      "[Training Epoch 0] Batch 1306, Loss 0.5433634519577026\n",
      "[Training Epoch 0] Batch 1307, Loss 0.5526963472366333\n",
      "[Training Epoch 0] Batch 1308, Loss 0.528877317905426\n",
      "[Training Epoch 0] Batch 1309, Loss 0.5350906252861023\n",
      "[Training Epoch 0] Batch 1310, Loss 0.539320707321167\n",
      "[Training Epoch 0] Batch 1311, Loss 0.5417433977127075\n",
      "[Training Epoch 0] Batch 1312, Loss 0.533223569393158\n",
      "[Training Epoch 0] Batch 1313, Loss 0.539573609828949\n",
      "[Training Epoch 0] Batch 1314, Loss 0.5497584342956543\n",
      "[Training Epoch 0] Batch 1315, Loss 0.5437452793121338\n",
      "[Training Epoch 0] Batch 1316, Loss 0.5432966947555542\n",
      "[Training Epoch 0] Batch 1317, Loss 0.5339315533638\n",
      "[Training Epoch 0] Batch 1318, Loss 0.5371794104576111\n",
      "[Training Epoch 0] Batch 1319, Loss 0.5454029440879822\n",
      "[Training Epoch 0] Batch 1320, Loss 0.5334726572036743\n",
      "[Training Epoch 0] Batch 1321, Loss 0.5451210737228394\n",
      "[Training Epoch 0] Batch 1322, Loss 0.5399957895278931\n",
      "[Training Epoch 0] Batch 1323, Loss 0.532325267791748\n",
      "[Training Epoch 0] Batch 1324, Loss 0.5476850867271423\n",
      "[Training Epoch 0] Batch 1325, Loss 0.549678385257721\n",
      "[Training Epoch 0] Batch 1326, Loss 0.5483890175819397\n",
      "[Training Epoch 0] Batch 1327, Loss 0.5441000461578369\n",
      "[Training Epoch 0] Batch 1328, Loss 0.5334888696670532\n",
      "[Training Epoch 0] Batch 1329, Loss 0.5404637455940247\n",
      "[Training Epoch 0] Batch 1330, Loss 0.5340918302536011\n",
      "[Training Epoch 0] Batch 1331, Loss 0.5269177556037903\n",
      "[Training Epoch 0] Batch 1332, Loss 0.5312483310699463\n",
      "[Training Epoch 0] Batch 1333, Loss 0.5345072150230408\n",
      "[Training Epoch 0] Batch 1334, Loss 0.5478715896606445\n",
      "[Training Epoch 0] Batch 1335, Loss 0.523686408996582\n",
      "[Training Epoch 0] Batch 1336, Loss 0.5335718989372253\n",
      "[Training Epoch 0] Batch 1337, Loss 0.5460379719734192\n",
      "[Training Epoch 0] Batch 1338, Loss 0.540489673614502\n",
      "[Training Epoch 0] Batch 1339, Loss 0.5385890603065491\n",
      "[Training Epoch 0] Batch 1340, Loss 0.5297636985778809\n",
      "[Training Epoch 0] Batch 1341, Loss 0.5285909175872803\n",
      "[Training Epoch 0] Batch 1342, Loss 0.5441730618476868\n",
      "[Training Epoch 0] Batch 1343, Loss 0.5350292921066284\n",
      "[Training Epoch 0] Batch 1344, Loss 0.5244994759559631\n",
      "[Training Epoch 0] Batch 1345, Loss 0.5293962955474854\n",
      "[Training Epoch 0] Batch 1346, Loss 0.5267108082771301\n",
      "[Training Epoch 0] Batch 1347, Loss 0.5387458801269531\n",
      "[Training Epoch 0] Batch 1348, Loss 0.539417028427124\n",
      "[Training Epoch 0] Batch 1349, Loss 0.5393984317779541\n",
      "[Training Epoch 0] Batch 1350, Loss 0.527701735496521\n",
      "[Training Epoch 0] Batch 1351, Loss 0.5593888163566589\n",
      "[Training Epoch 0] Batch 1352, Loss 0.5296621322631836\n",
      "[Training Epoch 0] Batch 1353, Loss 0.5348067879676819\n",
      "[Training Epoch 0] Batch 1354, Loss 0.5542259216308594\n",
      "[Training Epoch 0] Batch 1355, Loss 0.5411749482154846\n",
      "[Training Epoch 0] Batch 1356, Loss 0.5373018980026245\n",
      "[Training Epoch 0] Batch 1357, Loss 0.5530937910079956\n",
      "[Training Epoch 0] Batch 1358, Loss 0.5569685697555542\n",
      "[Training Epoch 0] Batch 1359, Loss 0.5333300828933716\n",
      "[Training Epoch 0] Batch 1360, Loss 0.5494893789291382\n",
      "[Training Epoch 0] Batch 1361, Loss 0.5334069728851318\n",
      "[Training Epoch 0] Batch 1362, Loss 0.5348628759384155\n",
      "[Training Epoch 0] Batch 1363, Loss 0.5300693511962891\n",
      "[Training Epoch 0] Batch 1364, Loss 0.5375850200653076\n",
      "[Training Epoch 0] Batch 1365, Loss 0.5366548895835876\n",
      "[Training Epoch 0] Batch 1366, Loss 0.5234204530715942\n",
      "[Training Epoch 0] Batch 1367, Loss 0.5329539775848389\n",
      "[Training Epoch 0] Batch 1368, Loss 0.5474774241447449\n",
      "[Training Epoch 0] Batch 1369, Loss 0.5267584323883057\n",
      "[Training Epoch 0] Batch 1370, Loss 0.545463502407074\n",
      "[Training Epoch 0] Batch 1371, Loss 0.5179890394210815\n",
      "[Training Epoch 0] Batch 1372, Loss 0.5365741848945618\n",
      "[Training Epoch 0] Batch 1373, Loss 0.5414795875549316\n",
      "[Training Epoch 0] Batch 1374, Loss 0.5184065103530884\n",
      "[Training Epoch 0] Batch 1375, Loss 0.5464630126953125\n",
      "[Training Epoch 0] Batch 1376, Loss 0.5264850854873657\n",
      "[Training Epoch 0] Batch 1377, Loss 0.5329694151878357\n",
      "[Training Epoch 0] Batch 1378, Loss 0.5260136723518372\n",
      "[Training Epoch 0] Batch 1379, Loss 0.5225658416748047\n",
      "[Training Epoch 0] Batch 1380, Loss 0.53952956199646\n",
      "[Training Epoch 0] Batch 1381, Loss 0.5341654419898987\n",
      "[Training Epoch 0] Batch 1382, Loss 0.5370935797691345\n",
      "[Training Epoch 0] Batch 1383, Loss 0.5332837700843811\n",
      "[Training Epoch 0] Batch 1384, Loss 0.5397976636886597\n",
      "[Training Epoch 0] Batch 1385, Loss 0.541370689868927\n",
      "[Training Epoch 0] Batch 1386, Loss 0.5511515140533447\n",
      "[Training Epoch 0] Batch 1387, Loss 0.5381054878234863\n",
      "[Training Epoch 0] Batch 1388, Loss 0.5506983399391174\n",
      "[Training Epoch 0] Batch 1389, Loss 0.524683952331543\n",
      "[Training Epoch 0] Batch 1390, Loss 0.5289894938468933\n",
      "[Training Epoch 0] Batch 1391, Loss 0.5222218036651611\n",
      "[Training Epoch 0] Batch 1392, Loss 0.5273848176002502\n",
      "[Training Epoch 0] Batch 1393, Loss 0.5506555438041687\n",
      "[Training Epoch 0] Batch 1394, Loss 0.5417612791061401\n",
      "[Training Epoch 0] Batch 1395, Loss 0.5468894839286804\n",
      "[Training Epoch 0] Batch 1396, Loss 0.5444364547729492\n",
      "[Training Epoch 0] Batch 1397, Loss 0.5419933199882507\n",
      "[Training Epoch 0] Batch 1398, Loss 0.5179033875465393\n",
      "[Training Epoch 0] Batch 1399, Loss 0.5323050022125244\n",
      "[Training Epoch 0] Batch 1400, Loss 0.5200610160827637\n",
      "[Training Epoch 0] Batch 1401, Loss 0.5252050161361694\n",
      "[Training Epoch 0] Batch 1402, Loss 0.5329360961914062\n",
      "[Training Epoch 0] Batch 1403, Loss 0.5462584495544434\n",
      "[Training Epoch 0] Batch 1404, Loss 0.524375319480896\n",
      "[Training Epoch 0] Batch 1405, Loss 0.5393974184989929\n",
      "[Training Epoch 0] Batch 1406, Loss 0.5217746496200562\n",
      "[Training Epoch 0] Batch 1407, Loss 0.5390661954879761\n",
      "[Training Epoch 0] Batch 1408, Loss 0.5425063371658325\n",
      "[Training Epoch 0] Batch 1409, Loss 0.5459935665130615\n",
      "[Training Epoch 0] Batch 1410, Loss 0.5330364108085632\n",
      "[Training Epoch 0] Batch 1411, Loss 0.5149109363555908\n",
      "[Training Epoch 0] Batch 1412, Loss 0.5175415277481079\n",
      "[Training Epoch 0] Batch 1413, Loss 0.5446901321411133\n",
      "[Training Epoch 0] Batch 1414, Loss 0.5177642107009888\n",
      "[Training Epoch 0] Batch 1415, Loss 0.5192413330078125\n",
      "[Training Epoch 0] Batch 1416, Loss 0.5280885100364685\n",
      "[Training Epoch 0] Batch 1417, Loss 0.5243158340454102\n",
      "[Training Epoch 0] Batch 1418, Loss 0.5447914004325867\n",
      "[Training Epoch 0] Batch 1419, Loss 0.5368601083755493\n",
      "[Training Epoch 0] Batch 1420, Loss 0.534238874912262\n",
      "[Training Epoch 0] Batch 1421, Loss 0.5301240682601929\n",
      "[Training Epoch 0] Batch 1422, Loss 0.5369516611099243\n",
      "[Training Epoch 0] Batch 1423, Loss 0.5412454605102539\n",
      "[Training Epoch 0] Batch 1424, Loss 0.5261237621307373\n",
      "[Training Epoch 0] Batch 1425, Loss 0.5490290522575378\n",
      "[Training Epoch 0] Batch 1426, Loss 0.5322277545928955\n",
      "[Training Epoch 0] Batch 1427, Loss 0.5190550684928894\n",
      "[Training Epoch 0] Batch 1428, Loss 0.533780574798584\n",
      "[Training Epoch 0] Batch 1429, Loss 0.5354371666908264\n",
      "[Training Epoch 0] Batch 1430, Loss 0.5096202492713928\n",
      "[Training Epoch 0] Batch 1431, Loss 0.5455092191696167\n",
      "[Training Epoch 0] Batch 1432, Loss 0.5446908473968506\n",
      "[Training Epoch 0] Batch 1433, Loss 0.5392106175422668\n",
      "[Training Epoch 0] Batch 1434, Loss 0.5386861562728882\n",
      "[Training Epoch 0] Batch 1435, Loss 0.540002703666687\n",
      "[Training Epoch 0] Batch 1436, Loss 0.5291538834571838\n",
      "[Training Epoch 0] Batch 1437, Loss 0.5220890045166016\n",
      "[Training Epoch 0] Batch 1438, Loss 0.5274182558059692\n",
      "[Training Epoch 0] Batch 1439, Loss 0.5400106310844421\n",
      "[Training Epoch 0] Batch 1440, Loss 0.5125219225883484\n",
      "[Training Epoch 0] Batch 1441, Loss 0.5223475098609924\n",
      "[Training Epoch 0] Batch 1442, Loss 0.5417876839637756\n",
      "[Training Epoch 0] Batch 1443, Loss 0.5440303087234497\n",
      "[Training Epoch 0] Batch 1444, Loss 0.5418056845664978\n",
      "[Training Epoch 0] Batch 1445, Loss 0.534794807434082\n",
      "[Training Epoch 0] Batch 1446, Loss 0.5324113965034485\n",
      "[Training Epoch 0] Batch 1447, Loss 0.5315420031547546\n",
      "[Training Epoch 0] Batch 1448, Loss 0.5408539175987244\n",
      "[Training Epoch 0] Batch 1449, Loss 0.5283817648887634\n",
      "[Training Epoch 0] Batch 1450, Loss 0.5313706994056702\n",
      "[Training Epoch 0] Batch 1451, Loss 0.5283657908439636\n",
      "[Training Epoch 0] Batch 1452, Loss 0.5102363228797913\n",
      "[Training Epoch 0] Batch 1453, Loss 0.5492905378341675\n",
      "[Training Epoch 0] Batch 1454, Loss 0.5365501642227173\n",
      "[Training Epoch 0] Batch 1455, Loss 0.5437602996826172\n",
      "[Training Epoch 0] Batch 1456, Loss 0.5459015369415283\n",
      "[Training Epoch 0] Batch 1457, Loss 0.5269964337348938\n",
      "[Training Epoch 0] Batch 1458, Loss 0.5260289907455444\n",
      "[Training Epoch 0] Batch 1459, Loss 0.5385905504226685\n",
      "[Training Epoch 0] Batch 1460, Loss 0.5460947751998901\n",
      "[Training Epoch 0] Batch 1461, Loss 0.5151861310005188\n",
      "[Training Epoch 0] Batch 1462, Loss 0.5356415510177612\n",
      "[Training Epoch 0] Batch 1463, Loss 0.5273868441581726\n",
      "[Training Epoch 0] Batch 1464, Loss 0.5236732959747314\n",
      "[Training Epoch 0] Batch 1465, Loss 0.5423601269721985\n",
      "[Training Epoch 0] Batch 1466, Loss 0.5239642262458801\n",
      "[Training Epoch 0] Batch 1467, Loss 0.5398628115653992\n",
      "[Training Epoch 0] Batch 1468, Loss 0.5271259546279907\n",
      "[Training Epoch 0] Batch 1469, Loss 0.5104157328605652\n",
      "[Training Epoch 0] Batch 1470, Loss 0.527875542640686\n",
      "[Training Epoch 0] Batch 1471, Loss 0.5467949509620667\n",
      "[Training Epoch 0] Batch 1472, Loss 0.5301629900932312\n",
      "[Training Epoch 0] Batch 1473, Loss 0.5332859754562378\n",
      "[Training Epoch 0] Batch 1474, Loss 0.5325134992599487\n",
      "[Training Epoch 0] Batch 1475, Loss 0.5179610848426819\n",
      "[Training Epoch 0] Batch 1476, Loss 0.5225780606269836\n",
      "[Training Epoch 0] Batch 1477, Loss 0.5315834283828735\n",
      "[Training Epoch 0] Batch 1478, Loss 0.5308361649513245\n",
      "[Training Epoch 0] Batch 1479, Loss 0.5290398001670837\n",
      "[Training Epoch 0] Batch 1480, Loss 0.5529059767723083\n",
      "[Training Epoch 0] Batch 1481, Loss 0.5232217311859131\n",
      "[Training Epoch 0] Batch 1482, Loss 0.5328661799430847\n",
      "[Training Epoch 0] Batch 1483, Loss 0.5472266674041748\n",
      "[Training Epoch 0] Batch 1484, Loss 0.5295419692993164\n",
      "[Training Epoch 0] Batch 1485, Loss 0.5121399760246277\n",
      "[Training Epoch 0] Batch 1486, Loss 0.540673553943634\n",
      "[Training Epoch 0] Batch 1487, Loss 0.5243254899978638\n",
      "[Training Epoch 0] Batch 1488, Loss 0.5222071409225464\n",
      "[Training Epoch 0] Batch 1489, Loss 0.5243691205978394\n",
      "[Training Epoch 0] Batch 1490, Loss 0.5299656987190247\n",
      "[Training Epoch 0] Batch 1491, Loss 0.53387850522995\n",
      "[Training Epoch 0] Batch 1492, Loss 0.5266733765602112\n",
      "[Training Epoch 0] Batch 1493, Loss 0.5418872833251953\n",
      "[Training Epoch 0] Batch 1494, Loss 0.5152069926261902\n",
      "[Training Epoch 0] Batch 1495, Loss 0.508653998374939\n",
      "[Training Epoch 0] Batch 1496, Loss 0.5367094278335571\n",
      "[Training Epoch 0] Batch 1497, Loss 0.5456279516220093\n",
      "[Training Epoch 0] Batch 1498, Loss 0.515039324760437\n",
      "[Training Epoch 0] Batch 1499, Loss 0.5046305060386658\n",
      "[Training Epoch 0] Batch 1500, Loss 0.526066243648529\n",
      "[Training Epoch 0] Batch 1501, Loss 0.548553466796875\n",
      "[Training Epoch 0] Batch 1502, Loss 0.5244877934455872\n",
      "[Training Epoch 0] Batch 1503, Loss 0.4938638210296631\n",
      "[Training Epoch 0] Batch 1504, Loss 0.52189040184021\n",
      "[Training Epoch 0] Batch 1505, Loss 0.5162026286125183\n",
      "[Training Epoch 0] Batch 1506, Loss 0.5356242656707764\n",
      "[Training Epoch 0] Batch 1507, Loss 0.5354638695716858\n",
      "[Training Epoch 0] Batch 1508, Loss 0.5234343409538269\n",
      "[Training Epoch 0] Batch 1509, Loss 0.5264805555343628\n",
      "[Training Epoch 0] Batch 1510, Loss 0.5208250880241394\n",
      "[Training Epoch 0] Batch 1511, Loss 0.5304710865020752\n",
      "[Training Epoch 0] Batch 1512, Loss 0.5344244837760925\n",
      "[Training Epoch 0] Batch 1513, Loss 0.5353109836578369\n",
      "[Training Epoch 0] Batch 1514, Loss 0.5432806611061096\n",
      "[Training Epoch 0] Batch 1515, Loss 0.5422699451446533\n",
      "[Training Epoch 0] Batch 1516, Loss 0.5188546180725098\n",
      "[Training Epoch 0] Batch 1517, Loss 0.5300599932670593\n",
      "[Training Epoch 0] Batch 1518, Loss 0.5397922992706299\n",
      "[Training Epoch 0] Batch 1519, Loss 0.5121223330497742\n",
      "[Training Epoch 0] Batch 1520, Loss 0.5265539288520813\n",
      "[Training Epoch 0] Batch 1521, Loss 0.52491694688797\n",
      "[Training Epoch 0] Batch 1522, Loss 0.5329952836036682\n",
      "[Training Epoch 0] Batch 1523, Loss 0.5535711050033569\n",
      "[Training Epoch 0] Batch 1524, Loss 0.536300539970398\n",
      "[Training Epoch 0] Batch 1525, Loss 0.5328913331031799\n",
      "[Training Epoch 0] Batch 1526, Loss 0.5370057225227356\n",
      "[Training Epoch 0] Batch 1527, Loss 0.5278364419937134\n",
      "[Training Epoch 0] Batch 1528, Loss 0.5345410108566284\n",
      "[Training Epoch 0] Batch 1529, Loss 0.5237036347389221\n",
      "[Training Epoch 0] Batch 1530, Loss 0.527129590511322\n",
      "[Training Epoch 0] Batch 1531, Loss 0.5285406708717346\n",
      "[Training Epoch 0] Batch 1532, Loss 0.5130347013473511\n",
      "[Training Epoch 0] Batch 1533, Loss 0.5263099074363708\n",
      "[Training Epoch 0] Batch 1534, Loss 0.5332785844802856\n",
      "[Training Epoch 0] Batch 1535, Loss 0.5392881631851196\n",
      "[Training Epoch 0] Batch 1536, Loss 0.517535924911499\n",
      "[Training Epoch 0] Batch 1537, Loss 0.526745617389679\n",
      "[Training Epoch 0] Batch 1538, Loss 0.5158289074897766\n",
      "[Training Epoch 0] Batch 1539, Loss 0.5300023555755615\n",
      "[Training Epoch 0] Batch 1540, Loss 0.5278517603874207\n",
      "[Training Epoch 0] Batch 1541, Loss 0.5431995391845703\n",
      "[Training Epoch 0] Batch 1542, Loss 0.5369116067886353\n",
      "[Training Epoch 0] Batch 1543, Loss 0.5273037552833557\n",
      "[Training Epoch 0] Batch 1544, Loss 0.5197252631187439\n",
      "[Training Epoch 0] Batch 1545, Loss 0.5165264010429382\n",
      "[Training Epoch 0] Batch 1546, Loss 0.5278504490852356\n",
      "[Training Epoch 0] Batch 1547, Loss 0.5261009931564331\n",
      "[Training Epoch 0] Batch 1548, Loss 0.5278570652008057\n",
      "[Training Epoch 0] Batch 1549, Loss 0.5253951549530029\n",
      "[Training Epoch 0] Batch 1550, Loss 0.5076678991317749\n",
      "[Training Epoch 0] Batch 1551, Loss 0.5192987322807312\n",
      "[Training Epoch 0] Batch 1552, Loss 0.5450992584228516\n",
      "[Training Epoch 0] Batch 1553, Loss 0.5215960144996643\n",
      "[Training Epoch 0] Batch 1554, Loss 0.5257941484451294\n",
      "[Training Epoch 0] Batch 1555, Loss 0.5391678810119629\n",
      "[Training Epoch 0] Batch 1556, Loss 0.5382075905799866\n",
      "[Training Epoch 0] Batch 1557, Loss 0.5224957466125488\n",
      "[Training Epoch 0] Batch 1558, Loss 0.5258055925369263\n",
      "[Training Epoch 0] Batch 1559, Loss 0.5363214015960693\n",
      "[Training Epoch 0] Batch 1560, Loss 0.5323828458786011\n",
      "[Training Epoch 0] Batch 1561, Loss 0.5170856714248657\n",
      "[Training Epoch 0] Batch 1562, Loss 0.5280545949935913\n",
      "[Training Epoch 0] Batch 1563, Loss 0.5144731998443604\n",
      "[Training Epoch 0] Batch 1564, Loss 0.5379931926727295\n",
      "[Training Epoch 0] Batch 1565, Loss 0.5228976607322693\n",
      "[Training Epoch 0] Batch 1566, Loss 0.5125402212142944\n",
      "[Training Epoch 0] Batch 1567, Loss 0.54368656873703\n",
      "[Training Epoch 0] Batch 1568, Loss 0.5341046452522278\n",
      "[Training Epoch 0] Batch 1569, Loss 0.5284886956214905\n",
      "[Training Epoch 0] Batch 1570, Loss 0.5462626814842224\n",
      "[Training Epoch 0] Batch 1571, Loss 0.524211049079895\n",
      "[Training Epoch 0] Batch 1572, Loss 0.5215392112731934\n",
      "[Training Epoch 0] Batch 1573, Loss 0.508831262588501\n",
      "[Training Epoch 0] Batch 1574, Loss 0.5254666805267334\n",
      "[Training Epoch 0] Batch 1575, Loss 0.5255952477455139\n",
      "[Training Epoch 0] Batch 1576, Loss 0.5271306037902832\n",
      "[Training Epoch 0] Batch 1577, Loss 0.5298311710357666\n",
      "[Training Epoch 0] Batch 1578, Loss 0.5332291722297668\n",
      "[Training Epoch 0] Batch 1579, Loss 0.5192069411277771\n",
      "[Training Epoch 0] Batch 1580, Loss 0.5144321322441101\n",
      "[Training Epoch 0] Batch 1581, Loss 0.5306293368339539\n",
      "[Training Epoch 0] Batch 1582, Loss 0.5224773287773132\n",
      "[Training Epoch 0] Batch 1583, Loss 0.5176236629486084\n",
      "[Training Epoch 0] Batch 1584, Loss 0.5345137119293213\n",
      "[Training Epoch 0] Batch 1585, Loss 0.5447179675102234\n",
      "[Training Epoch 0] Batch 1586, Loss 0.528653621673584\n",
      "[Training Epoch 0] Batch 1587, Loss 0.5343952178955078\n",
      "[Training Epoch 0] Batch 1588, Loss 0.532828688621521\n",
      "[Training Epoch 0] Batch 1589, Loss 0.5173605680465698\n",
      "[Training Epoch 0] Batch 1590, Loss 0.5311079621315002\n",
      "[Training Epoch 0] Batch 1591, Loss 0.5419229865074158\n",
      "[Training Epoch 0] Batch 1592, Loss 0.5137197971343994\n",
      "[Training Epoch 0] Batch 1593, Loss 0.5314156413078308\n",
      "[Training Epoch 0] Batch 1594, Loss 0.5143016576766968\n",
      "[Training Epoch 0] Batch 1595, Loss 0.520179033279419\n",
      "[Training Epoch 0] Batch 1596, Loss 0.5264051556587219\n",
      "[Training Epoch 0] Batch 1597, Loss 0.536949872970581\n",
      "[Training Epoch 0] Batch 1598, Loss 0.5442695617675781\n",
      "[Training Epoch 0] Batch 1599, Loss 0.528843879699707\n",
      "[Training Epoch 0] Batch 1600, Loss 0.5087166428565979\n",
      "[Training Epoch 0] Batch 1601, Loss 0.5386686325073242\n",
      "[Training Epoch 0] Batch 1602, Loss 0.5228930115699768\n",
      "[Training Epoch 0] Batch 1603, Loss 0.522095799446106\n",
      "[Training Epoch 0] Batch 1604, Loss 0.5272805094718933\n",
      "[Training Epoch 0] Batch 1605, Loss 0.5338156223297119\n",
      "[Training Epoch 0] Batch 1606, Loss 0.5343655347824097\n",
      "[Training Epoch 0] Batch 1607, Loss 0.5183765888214111\n",
      "[Training Epoch 0] Batch 1608, Loss 0.5234209299087524\n",
      "[Training Epoch 0] Batch 1609, Loss 0.5396360754966736\n",
      "[Training Epoch 0] Batch 1610, Loss 0.5326265692710876\n",
      "[Training Epoch 0] Batch 1611, Loss 0.5087448358535767\n",
      "[Training Epoch 0] Batch 1612, Loss 0.5225030183792114\n",
      "[Training Epoch 0] Batch 1613, Loss 0.5166736245155334\n",
      "[Training Epoch 0] Batch 1614, Loss 0.526313066482544\n",
      "[Training Epoch 0] Batch 1615, Loss 0.5338371992111206\n",
      "[Training Epoch 0] Batch 1616, Loss 0.5376242399215698\n",
      "[Training Epoch 0] Batch 1617, Loss 0.49724438786506653\n",
      "[Training Epoch 0] Batch 1618, Loss 0.5413497686386108\n",
      "[Training Epoch 0] Batch 1619, Loss 0.5238428115844727\n",
      "[Training Epoch 0] Batch 1620, Loss 0.5216624140739441\n",
      "[Training Epoch 0] Batch 1621, Loss 0.5202358961105347\n",
      "[Training Epoch 0] Batch 1622, Loss 0.5252557396888733\n",
      "[Training Epoch 0] Batch 1623, Loss 0.5275489687919617\n",
      "[Training Epoch 0] Batch 1624, Loss 0.5158502459526062\n",
      "[Training Epoch 0] Batch 1625, Loss 0.5256709456443787\n",
      "[Training Epoch 0] Batch 1626, Loss 0.5399070978164673\n",
      "[Training Epoch 0] Batch 1627, Loss 0.5156561732292175\n",
      "[Training Epoch 0] Batch 1628, Loss 0.5149957537651062\n",
      "[Training Epoch 0] Batch 1629, Loss 0.5328619480133057\n",
      "[Training Epoch 0] Batch 1630, Loss 0.5336816310882568\n",
      "[Training Epoch 0] Batch 1631, Loss 0.5315659642219543\n",
      "[Training Epoch 0] Batch 1632, Loss 0.5328112840652466\n",
      "[Training Epoch 0] Batch 1633, Loss 0.5302585959434509\n",
      "[Training Epoch 0] Batch 1634, Loss 0.5188512802124023\n",
      "[Training Epoch 0] Batch 1635, Loss 0.5244733691215515\n",
      "[Training Epoch 0] Batch 1636, Loss 0.5355873107910156\n",
      "[Training Epoch 0] Batch 1637, Loss 0.5359567403793335\n",
      "[Training Epoch 0] Batch 1638, Loss 0.5070528984069824\n",
      "[Training Epoch 0] Batch 1639, Loss 0.5159415602684021\n",
      "[Training Epoch 0] Batch 1640, Loss 0.519690990447998\n",
      "[Training Epoch 0] Batch 1641, Loss 0.5262874960899353\n",
      "[Training Epoch 0] Batch 1642, Loss 0.5020420551300049\n",
      "[Training Epoch 0] Batch 1643, Loss 0.5270329117774963\n",
      "[Training Epoch 0] Batch 1644, Loss 0.5402408838272095\n",
      "[Training Epoch 0] Batch 1645, Loss 0.5130045413970947\n",
      "[Training Epoch 0] Batch 1646, Loss 0.548006534576416\n",
      "[Training Epoch 0] Batch 1647, Loss 0.5275559425354004\n",
      "[Training Epoch 0] Batch 1648, Loss 0.5355144143104553\n",
      "[Training Epoch 0] Batch 1649, Loss 0.5207844972610474\n",
      "[Training Epoch 0] Batch 1650, Loss 0.5244383811950684\n",
      "[Training Epoch 0] Batch 1651, Loss 0.5042297840118408\n",
      "[Training Epoch 0] Batch 1652, Loss 0.4996088743209839\n",
      "[Training Epoch 0] Batch 1653, Loss 0.5319267511367798\n",
      "[Training Epoch 0] Batch 1654, Loss 0.5257521271705627\n",
      "[Training Epoch 0] Batch 1655, Loss 0.537319004535675\n",
      "[Training Epoch 0] Batch 1656, Loss 0.5136701464653015\n",
      "[Training Epoch 0] Batch 1657, Loss 0.5247132182121277\n",
      "[Training Epoch 0] Batch 1658, Loss 0.5240963697433472\n",
      "[Training Epoch 0] Batch 1659, Loss 0.491969496011734\n",
      "[Training Epoch 0] Batch 1660, Loss 0.5054236650466919\n",
      "[Training Epoch 0] Batch 1661, Loss 0.5176038146018982\n",
      "[Training Epoch 0] Batch 1662, Loss 0.5071468353271484\n",
      "[Training Epoch 0] Batch 1663, Loss 0.5331204533576965\n",
      "[Training Epoch 0] Batch 1664, Loss 0.5324174761772156\n",
      "[Training Epoch 0] Batch 1665, Loss 0.5305909514427185\n",
      "[Training Epoch 0] Batch 1666, Loss 0.5146104097366333\n",
      "[Training Epoch 0] Batch 1667, Loss 0.5226054787635803\n",
      "[Training Epoch 0] Batch 1668, Loss 0.5323709845542908\n",
      "[Training Epoch 0] Batch 1669, Loss 0.5186659693717957\n",
      "[Training Epoch 0] Batch 1670, Loss 0.4986066520214081\n",
      "[Training Epoch 0] Batch 1671, Loss 0.5261573195457458\n",
      "[Training Epoch 0] Batch 1672, Loss 0.5010109543800354\n",
      "[Training Epoch 0] Batch 1673, Loss 0.5011798739433289\n",
      "[Training Epoch 0] Batch 1674, Loss 0.5285688042640686\n",
      "[Training Epoch 0] Batch 1675, Loss 0.5339255332946777\n",
      "[Training Epoch 0] Batch 1676, Loss 0.5027647018432617\n",
      "[Training Epoch 0] Batch 1677, Loss 0.5327481627464294\n",
      "[Training Epoch 0] Batch 1678, Loss 0.5203965902328491\n",
      "[Training Epoch 0] Batch 1679, Loss 0.5140804052352905\n",
      "[Training Epoch 0] Batch 1680, Loss 0.5275561809539795\n",
      "[Training Epoch 0] Batch 1681, Loss 0.5202792882919312\n",
      "[Training Epoch 0] Batch 1682, Loss 0.5166943669319153\n",
      "[Training Epoch 0] Batch 1683, Loss 0.5030408501625061\n",
      "[Training Epoch 0] Batch 1684, Loss 0.49746182560920715\n",
      "[Training Epoch 0] Batch 1685, Loss 0.5010221600532532\n",
      "[Training Epoch 0] Batch 1686, Loss 0.5224938988685608\n",
      "[Training Epoch 0] Batch 1687, Loss 0.5140231847763062\n",
      "[Training Epoch 0] Batch 1688, Loss 0.5229302644729614\n",
      "[Training Epoch 0] Batch 1689, Loss 0.5163447856903076\n",
      "[Training Epoch 0] Batch 1690, Loss 0.5183786153793335\n",
      "[Training Epoch 0] Batch 1691, Loss 0.5234823822975159\n",
      "[Training Epoch 0] Batch 1692, Loss 0.525127112865448\n",
      "[Training Epoch 0] Batch 1693, Loss 0.5107496976852417\n",
      "[Training Epoch 0] Batch 1694, Loss 0.5195304155349731\n",
      "[Training Epoch 0] Batch 1695, Loss 0.5188866853713989\n",
      "[Training Epoch 0] Batch 1696, Loss 0.5095951557159424\n",
      "[Training Epoch 0] Batch 1697, Loss 0.5168992280960083\n",
      "[Training Epoch 0] Batch 1698, Loss 0.5617345571517944\n",
      "[Training Epoch 0] Batch 1699, Loss 0.5070830583572388\n",
      "[Training Epoch 0] Batch 1700, Loss 0.4924088716506958\n",
      "[Training Epoch 0] Batch 1701, Loss 0.504282534122467\n",
      "[Training Epoch 0] Batch 1702, Loss 0.5122956037521362\n",
      "[Training Epoch 0] Batch 1703, Loss 0.5195748209953308\n",
      "[Training Epoch 0] Batch 1704, Loss 0.5238068699836731\n",
      "[Training Epoch 0] Batch 1705, Loss 0.5273742079734802\n",
      "[Training Epoch 0] Batch 1706, Loss 0.5112239718437195\n",
      "[Training Epoch 0] Batch 1707, Loss 0.5131902098655701\n",
      "[Training Epoch 0] Batch 1708, Loss 0.5097943544387817\n",
      "[Training Epoch 0] Batch 1709, Loss 0.4967108964920044\n",
      "[Training Epoch 0] Batch 1710, Loss 0.519056499004364\n",
      "[Training Epoch 0] Batch 1711, Loss 0.5220215320587158\n",
      "[Training Epoch 0] Batch 1712, Loss 0.5272083282470703\n",
      "[Training Epoch 0] Batch 1713, Loss 0.5222359895706177\n",
      "[Training Epoch 0] Batch 1714, Loss 0.5117579698562622\n",
      "[Training Epoch 0] Batch 1715, Loss 0.54616379737854\n",
      "[Training Epoch 0] Batch 1716, Loss 0.5271644592285156\n",
      "[Training Epoch 0] Batch 1717, Loss 0.5148925185203552\n",
      "[Training Epoch 0] Batch 1718, Loss 0.48971322178840637\n",
      "[Training Epoch 0] Batch 1719, Loss 0.5125621557235718\n",
      "[Training Epoch 0] Batch 1720, Loss 0.5116768479347229\n",
      "[Training Epoch 0] Batch 1721, Loss 0.5095739960670471\n",
      "[Training Epoch 0] Batch 1722, Loss 0.5086658596992493\n",
      "[Training Epoch 0] Batch 1723, Loss 0.5204641819000244\n",
      "[Training Epoch 0] Batch 1724, Loss 0.5294195413589478\n",
      "[Training Epoch 0] Batch 1725, Loss 0.5332458019256592\n",
      "[Training Epoch 0] Batch 1726, Loss 0.5190032720565796\n",
      "[Training Epoch 0] Batch 1727, Loss 0.5266398787498474\n",
      "[Training Epoch 0] Batch 1728, Loss 0.519243597984314\n",
      "[Training Epoch 0] Batch 1729, Loss 0.5029226541519165\n",
      "[Training Epoch 0] Batch 1730, Loss 0.5186576247215271\n",
      "[Training Epoch 0] Batch 1731, Loss 0.5099160075187683\n",
      "[Training Epoch 0] Batch 1732, Loss 0.5211930274963379\n",
      "[Training Epoch 0] Batch 1733, Loss 0.5260893106460571\n",
      "[Training Epoch 0] Batch 1734, Loss 0.5272568464279175\n",
      "[Training Epoch 0] Batch 1735, Loss 0.5024702548980713\n",
      "[Training Epoch 0] Batch 1736, Loss 0.5271415114402771\n",
      "[Training Epoch 0] Batch 1737, Loss 0.5089063048362732\n",
      "[Training Epoch 0] Batch 1738, Loss 0.5069127678871155\n",
      "[Training Epoch 0] Batch 1739, Loss 0.5279844999313354\n",
      "[Training Epoch 0] Batch 1740, Loss 0.5079128742218018\n",
      "[Training Epoch 0] Batch 1741, Loss 0.5340344309806824\n",
      "[Training Epoch 0] Batch 1742, Loss 0.4978189170360565\n",
      "[Training Epoch 0] Batch 1743, Loss 0.5212670564651489\n",
      "[Training Epoch 0] Batch 1744, Loss 0.5058152079582214\n",
      "[Training Epoch 0] Batch 1745, Loss 0.5047956109046936\n",
      "[Training Epoch 0] Batch 1746, Loss 0.5122507214546204\n",
      "[Training Epoch 0] Batch 1747, Loss 0.5250164270401001\n",
      "[Training Epoch 0] Batch 1748, Loss 0.5258591771125793\n",
      "[Training Epoch 0] Batch 1749, Loss 0.5009233951568604\n",
      "[Training Epoch 0] Batch 1750, Loss 0.5066121816635132\n",
      "[Training Epoch 0] Batch 1751, Loss 0.5219873785972595\n",
      "[Training Epoch 0] Batch 1752, Loss 0.5009644031524658\n",
      "[Training Epoch 0] Batch 1753, Loss 0.5147705078125\n",
      "[Training Epoch 0] Batch 1754, Loss 0.5072461366653442\n",
      "[Training Epoch 0] Batch 1755, Loss 0.5215637683868408\n",
      "[Training Epoch 0] Batch 1756, Loss 0.5312026739120483\n",
      "[Training Epoch 0] Batch 1757, Loss 0.50943922996521\n",
      "[Training Epoch 0] Batch 1758, Loss 0.5308927893638611\n",
      "[Training Epoch 0] Batch 1759, Loss 0.4980071485042572\n",
      "[Training Epoch 0] Batch 1760, Loss 0.4983595013618469\n",
      "[Training Epoch 0] Batch 1761, Loss 0.50224769115448\n",
      "[Training Epoch 0] Batch 1762, Loss 0.51920086145401\n",
      "[Training Epoch 0] Batch 1763, Loss 0.5329511165618896\n",
      "[Training Epoch 0] Batch 1764, Loss 0.5210563540458679\n",
      "[Training Epoch 0] Batch 1765, Loss 0.5252393484115601\n",
      "[Training Epoch 0] Batch 1766, Loss 0.5067999362945557\n",
      "[Training Epoch 0] Batch 1767, Loss 0.5149617791175842\n",
      "[Training Epoch 0] Batch 1768, Loss 0.5027591586112976\n",
      "[Training Epoch 0] Batch 1769, Loss 0.5186891555786133\n",
      "[Training Epoch 0] Batch 1770, Loss 0.5308478474617004\n",
      "[Training Epoch 0] Batch 1771, Loss 0.5140393376350403\n",
      "[Training Epoch 0] Batch 1772, Loss 0.5262095928192139\n",
      "[Training Epoch 0] Batch 1773, Loss 0.5308825969696045\n",
      "[Training Epoch 0] Batch 1774, Loss 0.501051664352417\n",
      "[Training Epoch 0] Batch 1775, Loss 0.5017813444137573\n",
      "[Training Epoch 0] Batch 1776, Loss 0.5109448432922363\n",
      "[Training Epoch 0] Batch 1777, Loss 0.5005795359611511\n",
      "[Training Epoch 0] Batch 1778, Loss 0.506334662437439\n",
      "[Training Epoch 0] Batch 1779, Loss 0.5036506652832031\n",
      "[Training Epoch 0] Batch 1780, Loss 0.5052187442779541\n",
      "[Training Epoch 0] Batch 1781, Loss 0.5183136463165283\n",
      "[Training Epoch 0] Batch 1782, Loss 0.5050458908081055\n",
      "[Training Epoch 0] Batch 1783, Loss 0.5202369093894958\n",
      "[Training Epoch 0] Batch 1784, Loss 0.5155695080757141\n",
      "[Training Epoch 0] Batch 1785, Loss 0.49950313568115234\n",
      "[Training Epoch 0] Batch 1786, Loss 0.5098695755004883\n",
      "[Training Epoch 0] Batch 1787, Loss 0.5320871472358704\n",
      "[Training Epoch 0] Batch 1788, Loss 0.5207883715629578\n",
      "[Training Epoch 0] Batch 1789, Loss 0.5123775601387024\n",
      "[Training Epoch 0] Batch 1790, Loss 0.5114527940750122\n",
      "[Training Epoch 0] Batch 1791, Loss 0.4943906366825104\n",
      "[Training Epoch 0] Batch 1792, Loss 0.5236767530441284\n",
      "[Training Epoch 0] Batch 1793, Loss 0.5140848755836487\n",
      "[Training Epoch 0] Batch 1794, Loss 0.5026698112487793\n",
      "[Training Epoch 0] Batch 1795, Loss 0.5189663171768188\n",
      "[Training Epoch 0] Batch 1796, Loss 0.4969518780708313\n",
      "[Training Epoch 0] Batch 1797, Loss 0.5310081243515015\n",
      "[Training Epoch 0] Batch 1798, Loss 0.504324197769165\n",
      "[Training Epoch 0] Batch 1799, Loss 0.5204973816871643\n",
      "[Training Epoch 0] Batch 1800, Loss 0.5015789866447449\n",
      "[Training Epoch 0] Batch 1801, Loss 0.5280975699424744\n",
      "[Training Epoch 0] Batch 1802, Loss 0.508056640625\n",
      "[Training Epoch 0] Batch 1803, Loss 0.5168009400367737\n",
      "[Training Epoch 0] Batch 1804, Loss 0.5299901962280273\n",
      "[Training Epoch 0] Batch 1805, Loss 0.516668438911438\n",
      "[Training Epoch 0] Batch 1806, Loss 0.5231373906135559\n",
      "[Training Epoch 0] Batch 1807, Loss 0.5057518482208252\n",
      "[Training Epoch 0] Batch 1808, Loss 0.5042936205863953\n",
      "[Training Epoch 0] Batch 1809, Loss 0.5176316499710083\n",
      "[Training Epoch 0] Batch 1810, Loss 0.5194335579872131\n",
      "[Training Epoch 0] Batch 1811, Loss 0.5258408188819885\n",
      "[Training Epoch 0] Batch 1812, Loss 0.530666708946228\n",
      "[Training Epoch 0] Batch 1813, Loss 0.5171347260475159\n",
      "[Training Epoch 0] Batch 1814, Loss 0.5093874931335449\n",
      "[Training Epoch 0] Batch 1815, Loss 0.5018459558486938\n",
      "[Training Epoch 0] Batch 1816, Loss 0.5018578767776489\n",
      "[Training Epoch 0] Batch 1817, Loss 0.5283070206642151\n",
      "[Training Epoch 0] Batch 1818, Loss 0.5181099772453308\n",
      "[Training Epoch 0] Batch 1819, Loss 0.5130676031112671\n",
      "[Training Epoch 0] Batch 1820, Loss 0.5082728862762451\n",
      "[Training Epoch 0] Batch 1821, Loss 0.5130064487457275\n",
      "[Training Epoch 0] Batch 1822, Loss 0.49601441621780396\n",
      "[Training Epoch 0] Batch 1823, Loss 0.5148604512214661\n",
      "[Training Epoch 0] Batch 1824, Loss 0.5149215459823608\n",
      "[Training Epoch 0] Batch 1825, Loss 0.5273666977882385\n",
      "[Training Epoch 0] Batch 1826, Loss 0.5225200057029724\n",
      "[Training Epoch 0] Batch 1827, Loss 0.5120611190795898\n",
      "[Training Epoch 0] Batch 1828, Loss 0.509706974029541\n",
      "[Training Epoch 0] Batch 1829, Loss 0.5126787424087524\n",
      "[Training Epoch 0] Batch 1830, Loss 0.5092062950134277\n",
      "[Training Epoch 0] Batch 1831, Loss 0.5244945287704468\n",
      "[Training Epoch 0] Batch 1832, Loss 0.508862316608429\n",
      "[Training Epoch 0] Batch 1833, Loss 0.5157094597816467\n",
      "[Training Epoch 0] Batch 1834, Loss 0.5214482545852661\n",
      "[Training Epoch 0] Batch 1835, Loss 0.512667715549469\n",
      "[Training Epoch 0] Batch 1836, Loss 0.5354548692703247\n",
      "[Training Epoch 0] Batch 1837, Loss 0.5220921635627747\n",
      "[Training Epoch 0] Batch 1838, Loss 0.5225218534469604\n",
      "[Training Epoch 0] Batch 1839, Loss 0.5273164510726929\n",
      "[Training Epoch 0] Batch 1840, Loss 0.5182750225067139\n",
      "[Training Epoch 0] Batch 1841, Loss 0.5212599635124207\n",
      "[Training Epoch 0] Batch 1842, Loss 0.5362561345100403\n",
      "[Training Epoch 0] Batch 1843, Loss 0.5153234004974365\n",
      "[Training Epoch 0] Batch 1844, Loss 0.48697903752326965\n",
      "[Training Epoch 0] Batch 1845, Loss 0.5157108306884766\n",
      "[Training Epoch 0] Batch 1846, Loss 0.5086607933044434\n",
      "[Training Epoch 0] Batch 1847, Loss 0.5181213021278381\n",
      "[Training Epoch 0] Batch 1848, Loss 0.523076057434082\n",
      "[Training Epoch 0] Batch 1849, Loss 0.5300096273422241\n",
      "[Training Epoch 0] Batch 1850, Loss 0.5285142660140991\n",
      "[Training Epoch 0] Batch 1851, Loss 0.5111744999885559\n",
      "[Training Epoch 0] Batch 1852, Loss 0.5151841640472412\n",
      "[Training Epoch 0] Batch 1853, Loss 0.5113313794136047\n",
      "[Training Epoch 0] Batch 1854, Loss 0.514937162399292\n",
      "[Training Epoch 0] Batch 1855, Loss 0.5231612920761108\n",
      "[Training Epoch 0] Batch 1856, Loss 0.5082959532737732\n",
      "[Training Epoch 0] Batch 1857, Loss 0.5034379363059998\n",
      "[Training Epoch 0] Batch 1858, Loss 0.5322023034095764\n",
      "[Training Epoch 0] Batch 1859, Loss 0.5240390300750732\n",
      "[Training Epoch 0] Batch 1860, Loss 0.5392295122146606\n",
      "[Training Epoch 0] Batch 1861, Loss 0.5079790949821472\n",
      "[Training Epoch 0] Batch 1862, Loss 0.5051407814025879\n",
      "[Training Epoch 0] Batch 1863, Loss 0.5150192975997925\n",
      "[Training Epoch 0] Batch 1864, Loss 0.5392316579818726\n",
      "[Training Epoch 0] Batch 1865, Loss 0.5489490032196045\n",
      "[Training Epoch 0] Batch 1866, Loss 0.517159640789032\n",
      "[Training Epoch 0] Batch 1867, Loss 0.5168358683586121\n",
      "[Training Epoch 0] Batch 1868, Loss 0.5070649981498718\n",
      "[Training Epoch 0] Batch 1869, Loss 0.5215123891830444\n",
      "[Training Epoch 0] Batch 1870, Loss 0.5086949467658997\n",
      "[Training Epoch 0] Batch 1871, Loss 0.5059928297996521\n",
      "[Training Epoch 0] Batch 1872, Loss 0.5186173915863037\n",
      "[Training Epoch 0] Batch 1873, Loss 0.4959401488304138\n",
      "[Training Epoch 0] Batch 1874, Loss 0.5322347283363342\n",
      "[Training Epoch 0] Batch 1875, Loss 0.5213354229927063\n",
      "[Training Epoch 0] Batch 1876, Loss 0.5026121139526367\n",
      "[Training Epoch 0] Batch 1877, Loss 0.5322616100311279\n",
      "[Training Epoch 0] Batch 1878, Loss 0.5154815316200256\n",
      "[Training Epoch 0] Batch 1879, Loss 0.525296688079834\n",
      "[Training Epoch 0] Batch 1880, Loss 0.5170555114746094\n",
      "[Training Epoch 0] Batch 1881, Loss 0.5306575298309326\n",
      "[Training Epoch 0] Batch 1882, Loss 0.504606306552887\n",
      "[Training Epoch 0] Batch 1883, Loss 0.523925244808197\n",
      "[Training Epoch 0] Batch 1884, Loss 0.5202233195304871\n",
      "[Training Epoch 0] Batch 1885, Loss 0.5443457365036011\n",
      "[Training Epoch 0] Batch 1886, Loss 0.5211172103881836\n",
      "[Training Epoch 0] Batch 1887, Loss 0.5056143999099731\n",
      "[Training Epoch 0] Batch 1888, Loss 0.5044584274291992\n",
      "[Training Epoch 0] Batch 1889, Loss 0.514082133769989\n",
      "[Training Epoch 0] Batch 1890, Loss 0.4773246645927429\n",
      "[Training Epoch 0] Batch 1891, Loss 0.49542802572250366\n",
      "[Training Epoch 0] Batch 1892, Loss 0.4944879710674286\n",
      "[Training Epoch 0] Batch 1893, Loss 0.4964403510093689\n",
      "[Training Epoch 0] Batch 1894, Loss 0.5261070728302002\n",
      "[Training Epoch 0] Batch 1895, Loss 0.4956035614013672\n",
      "[Training Epoch 0] Batch 1896, Loss 0.5167636871337891\n",
      "[Training Epoch 0] Batch 1897, Loss 0.5131474733352661\n",
      "[Training Epoch 0] Batch 1898, Loss 0.5071499943733215\n",
      "[Training Epoch 0] Batch 1899, Loss 0.5013154149055481\n",
      "[Training Epoch 0] Batch 1900, Loss 0.5204098224639893\n",
      "[Training Epoch 0] Batch 1901, Loss 0.5236620306968689\n",
      "[Training Epoch 0] Batch 1902, Loss 0.5139702558517456\n",
      "[Training Epoch 0] Batch 1903, Loss 0.521365761756897\n",
      "[Training Epoch 0] Batch 1904, Loss 0.5242044925689697\n",
      "[Training Epoch 0] Batch 1905, Loss 0.5128447413444519\n",
      "[Training Epoch 0] Batch 1906, Loss 0.5091644525527954\n",
      "[Training Epoch 0] Batch 1907, Loss 0.5097591876983643\n",
      "[Training Epoch 0] Batch 1908, Loss 0.5190850496292114\n",
      "[Training Epoch 0] Batch 1909, Loss 0.49937617778778076\n",
      "[Training Epoch 0] Batch 1910, Loss 0.5134742259979248\n",
      "[Training Epoch 0] Batch 1911, Loss 0.5036044716835022\n",
      "[Training Epoch 0] Batch 1912, Loss 0.5036457777023315\n",
      "[Training Epoch 0] Batch 1913, Loss 0.509880781173706\n",
      "[Training Epoch 0] Batch 1914, Loss 0.5188095569610596\n",
      "[Training Epoch 0] Batch 1915, Loss 0.507775068283081\n",
      "[Training Epoch 0] Batch 1916, Loss 0.5064650774002075\n",
      "[Training Epoch 0] Batch 1917, Loss 0.5096335411071777\n",
      "[Training Epoch 0] Batch 1918, Loss 0.49940624833106995\n",
      "[Training Epoch 0] Batch 1919, Loss 0.5040423274040222\n",
      "[Training Epoch 0] Batch 1920, Loss 0.5480725169181824\n",
      "[Training Epoch 0] Batch 1921, Loss 0.5200302600860596\n",
      "[Training Epoch 0] Batch 1922, Loss 0.5215337872505188\n",
      "[Training Epoch 0] Batch 1923, Loss 0.5154240727424622\n",
      "[Training Epoch 0] Batch 1924, Loss 0.524757981300354\n",
      "[Training Epoch 0] Batch 1925, Loss 0.5193233489990234\n",
      "[Training Epoch 0] Batch 1926, Loss 0.49712586402893066\n",
      "[Training Epoch 0] Batch 1927, Loss 0.5110156536102295\n",
      "[Training Epoch 0] Batch 1928, Loss 0.5121362209320068\n",
      "[Training Epoch 0] Batch 1929, Loss 0.5073016881942749\n",
      "[Training Epoch 0] Batch 1930, Loss 0.5276613235473633\n",
      "[Training Epoch 0] Batch 1931, Loss 0.49210622906684875\n",
      "[Training Epoch 0] Batch 1932, Loss 0.5134128928184509\n",
      "[Training Epoch 0] Batch 1933, Loss 0.5049324035644531\n",
      "[Training Epoch 0] Batch 1934, Loss 0.5139543414115906\n",
      "[Training Epoch 0] Batch 1935, Loss 0.5072813630104065\n",
      "[Training Epoch 0] Batch 1936, Loss 0.508730411529541\n",
      "[Training Epoch 0] Batch 1937, Loss 0.49577242136001587\n",
      "[Training Epoch 0] Batch 1938, Loss 0.5210444927215576\n",
      "[Training Epoch 0] Batch 1939, Loss 0.5078076720237732\n",
      "[Training Epoch 0] Batch 1940, Loss 0.5078706741333008\n",
      "[Training Epoch 0] Batch 1941, Loss 0.5311342477798462\n",
      "[Training Epoch 0] Batch 1942, Loss 0.519058883190155\n",
      "[Training Epoch 0] Batch 1943, Loss 0.5342892408370972\n",
      "[Training Epoch 0] Batch 1944, Loss 0.4989806115627289\n",
      "[Training Epoch 0] Batch 1945, Loss 0.4980303645133972\n",
      "[Training Epoch 0] Batch 1946, Loss 0.5108505487442017\n",
      "[Training Epoch 0] Batch 1947, Loss 0.5298140048980713\n",
      "[Training Epoch 0] Batch 1948, Loss 0.5217890739440918\n",
      "[Training Epoch 0] Batch 1949, Loss 0.528174877166748\n",
      "[Training Epoch 0] Batch 1950, Loss 0.5077215433120728\n",
      "[Training Epoch 0] Batch 1951, Loss 0.48671719431877136\n",
      "[Training Epoch 0] Batch 1952, Loss 0.5073719024658203\n",
      "[Training Epoch 0] Batch 1953, Loss 0.5085406303405762\n",
      "[Training Epoch 0] Batch 1954, Loss 0.5084267258644104\n",
      "[Training Epoch 0] Batch 1955, Loss 0.5153898596763611\n",
      "[Training Epoch 0] Batch 1956, Loss 0.5180922150611877\n",
      "[Training Epoch 0] Batch 1957, Loss 0.5134758353233337\n",
      "[Training Epoch 0] Batch 1958, Loss 0.5122970342636108\n",
      "[Training Epoch 0] Batch 1959, Loss 0.49387091398239136\n",
      "[Training Epoch 0] Batch 1960, Loss 0.5142003297805786\n",
      "[Training Epoch 0] Batch 1961, Loss 0.513998806476593\n",
      "[Training Epoch 0] Batch 1962, Loss 0.5065332055091858\n",
      "[Training Epoch 0] Batch 1963, Loss 0.508837878704071\n",
      "[Training Epoch 0] Batch 1964, Loss 0.507220983505249\n",
      "[Training Epoch 0] Batch 1965, Loss 0.5012692809104919\n",
      "[Training Epoch 0] Batch 1966, Loss 0.5170358419418335\n",
      "[Training Epoch 0] Batch 1967, Loss 0.5053700804710388\n",
      "[Training Epoch 0] Batch 1968, Loss 0.5156288146972656\n",
      "[Training Epoch 0] Batch 1969, Loss 0.517133891582489\n",
      "[Training Epoch 0] Batch 1970, Loss 0.5027070045471191\n",
      "[Training Epoch 0] Batch 1971, Loss 0.5231846570968628\n",
      "[Training Epoch 0] Batch 1972, Loss 0.5018072128295898\n",
      "[Training Epoch 0] Batch 1973, Loss 0.5091574192047119\n",
      "[Training Epoch 0] Batch 1974, Loss 0.4977697730064392\n",
      "[Training Epoch 0] Batch 1975, Loss 0.5227747559547424\n",
      "[Training Epoch 0] Batch 1976, Loss 0.4977370798587799\n",
      "[Training Epoch 0] Batch 1977, Loss 0.5118467807769775\n",
      "[Training Epoch 0] Batch 1978, Loss 0.49245551228523254\n",
      "[Training Epoch 0] Batch 1979, Loss 0.5182640552520752\n",
      "[Training Epoch 0] Batch 1980, Loss 0.502597987651825\n",
      "[Training Epoch 0] Batch 1981, Loss 0.5210979580879211\n",
      "[Training Epoch 0] Batch 1982, Loss 0.5075401067733765\n",
      "[Training Epoch 0] Batch 1983, Loss 0.5230903625488281\n",
      "[Training Epoch 0] Batch 1984, Loss 0.5231561064720154\n",
      "[Training Epoch 0] Batch 1985, Loss 0.5074313879013062\n",
      "[Training Epoch 0] Batch 1986, Loss 0.5144355893135071\n",
      "[Training Epoch 0] Batch 1987, Loss 0.5176158547401428\n",
      "[Training Epoch 0] Batch 1988, Loss 0.5245382785797119\n",
      "[Training Epoch 0] Batch 1989, Loss 0.5301870703697205\n",
      "[Training Epoch 0] Batch 1990, Loss 0.5219043493270874\n",
      "[Training Epoch 0] Batch 1991, Loss 0.5125815868377686\n",
      "[Training Epoch 0] Batch 1992, Loss 0.5206756591796875\n",
      "[Training Epoch 0] Batch 1993, Loss 0.5009117722511292\n",
      "[Training Epoch 0] Batch 1994, Loss 0.5248275995254517\n",
      "[Training Epoch 0] Batch 1995, Loss 0.5170978903770447\n",
      "[Training Epoch 0] Batch 1996, Loss 0.48830679059028625\n",
      "[Training Epoch 0] Batch 1997, Loss 0.5132888555526733\n",
      "[Training Epoch 0] Batch 1998, Loss 0.5226137042045593\n",
      "[Training Epoch 0] Batch 1999, Loss 0.5194257497787476\n",
      "[Training Epoch 0] Batch 2000, Loss 0.49844691157341003\n",
      "[Training Epoch 0] Batch 2001, Loss 0.5376002788543701\n",
      "[Training Epoch 0] Batch 2002, Loss 0.5337050557136536\n",
      "[Training Epoch 0] Batch 2003, Loss 0.5000941753387451\n",
      "[Training Epoch 0] Batch 2004, Loss 0.5086404085159302\n",
      "[Training Epoch 0] Batch 2005, Loss 0.5179818272590637\n",
      "[Training Epoch 0] Batch 2006, Loss 0.4966120719909668\n",
      "[Training Epoch 0] Batch 2007, Loss 0.5087378621101379\n",
      "[Training Epoch 0] Batch 2008, Loss 0.5168386697769165\n",
      "[Training Epoch 0] Batch 2009, Loss 0.5290645360946655\n",
      "[Training Epoch 0] Batch 2010, Loss 0.5112630128860474\n",
      "[Training Epoch 0] Batch 2011, Loss 0.5043622255325317\n",
      "[Training Epoch 0] Batch 2012, Loss 0.49714887142181396\n",
      "[Training Epoch 0] Batch 2013, Loss 0.5097218751907349\n",
      "[Training Epoch 0] Batch 2014, Loss 0.4855514466762543\n",
      "[Training Epoch 0] Batch 2015, Loss 0.5013244152069092\n",
      "[Training Epoch 0] Batch 2016, Loss 0.517677903175354\n",
      "[Training Epoch 0] Batch 2017, Loss 0.5268821120262146\n",
      "[Training Epoch 0] Batch 2018, Loss 0.5220388174057007\n",
      "[Training Epoch 0] Batch 2019, Loss 0.5008301138877869\n",
      "[Training Epoch 0] Batch 2020, Loss 0.5064043998718262\n",
      "[Training Epoch 0] Batch 2021, Loss 0.5152881145477295\n",
      "[Training Epoch 0] Batch 2022, Loss 0.4863974452018738\n",
      "[Training Epoch 0] Batch 2023, Loss 0.5024747848510742\n",
      "[Training Epoch 0] Batch 2024, Loss 0.5035767555236816\n",
      "[Training Epoch 0] Batch 2025, Loss 0.5091127157211304\n",
      "[Training Epoch 0] Batch 2026, Loss 0.5139027833938599\n",
      "[Training Epoch 0] Batch 2027, Loss 0.5035074949264526\n",
      "[Training Epoch 0] Batch 2028, Loss 0.4994008541107178\n",
      "[Training Epoch 0] Batch 2029, Loss 0.4995151162147522\n",
      "[Training Epoch 0] Batch 2030, Loss 0.5029447078704834\n",
      "[Training Epoch 0] Batch 2031, Loss 0.4950316846370697\n",
      "[Training Epoch 0] Batch 2032, Loss 0.5077380537986755\n",
      "[Training Epoch 0] Batch 2033, Loss 0.5169539451599121\n",
      "[Training Epoch 0] Batch 2034, Loss 0.5149691700935364\n",
      "[Training Epoch 0] Batch 2035, Loss 0.49500900506973267\n",
      "[Training Epoch 0] Batch 2036, Loss 0.5056261420249939\n",
      "[Training Epoch 0] Batch 2037, Loss 0.515093207359314\n",
      "[Training Epoch 0] Batch 2038, Loss 0.5027009844779968\n",
      "[Training Epoch 0] Batch 2039, Loss 0.5415299534797668\n",
      "[Training Epoch 0] Batch 2040, Loss 0.5096249580383301\n",
      "[Training Epoch 0] Batch 2041, Loss 0.5104190707206726\n",
      "[Training Epoch 0] Batch 2042, Loss 0.5074663162231445\n",
      "[Training Epoch 0] Batch 2043, Loss 0.49888771772384644\n",
      "[Training Epoch 0] Batch 2044, Loss 0.5209034085273743\n",
      "[Training Epoch 0] Batch 2045, Loss 0.5052177309989929\n",
      "[Training Epoch 0] Batch 2046, Loss 0.48883241415023804\n",
      "[Training Epoch 0] Batch 2047, Loss 0.5021634697914124\n",
      "[Training Epoch 0] Batch 2048, Loss 0.50014328956604\n",
      "[Training Epoch 0] Batch 2049, Loss 0.4843960106372833\n",
      "[Training Epoch 0] Batch 2050, Loss 0.5000534653663635\n",
      "[Training Epoch 0] Batch 2051, Loss 0.49070435762405396\n",
      "[Training Epoch 0] Batch 2052, Loss 0.5019142031669617\n",
      "[Training Epoch 0] Batch 2053, Loss 0.5112491846084595\n",
      "[Training Epoch 0] Batch 2054, Loss 0.5029221773147583\n",
      "[Training Epoch 0] Batch 2055, Loss 0.49485182762145996\n",
      "[Training Epoch 0] Batch 2056, Loss 0.5186681747436523\n",
      "[Training Epoch 0] Batch 2057, Loss 0.49041348695755005\n",
      "[Training Epoch 0] Batch 2058, Loss 0.5166316628456116\n",
      "[Training Epoch 0] Batch 2059, Loss 0.5239013433456421\n",
      "[Training Epoch 0] Batch 2060, Loss 0.5209406614303589\n",
      "[Training Epoch 0] Batch 2061, Loss 0.5288041234016418\n",
      "[Training Epoch 0] Batch 2062, Loss 0.5029625296592712\n",
      "[Training Epoch 0] Batch 2063, Loss 0.5050586462020874\n",
      "[Training Epoch 0] Batch 2064, Loss 0.5047613382339478\n",
      "[Training Epoch 0] Batch 2065, Loss 0.5183301568031311\n",
      "[Training Epoch 0] Batch 2066, Loss 0.5079554915428162\n",
      "[Training Epoch 0] Batch 2067, Loss 0.5162857174873352\n",
      "[Training Epoch 0] Batch 2068, Loss 0.5280231237411499\n",
      "[Training Epoch 0] Batch 2069, Loss 0.5130642056465149\n",
      "[Training Epoch 0] Batch 2070, Loss 0.5089103579521179\n",
      "[Training Epoch 0] Batch 2071, Loss 0.522467315196991\n",
      "[Training Epoch 0] Batch 2072, Loss 0.5148923993110657\n",
      "[Training Epoch 0] Batch 2073, Loss 0.5056064128875732\n",
      "[Training Epoch 0] Batch 2074, Loss 0.49842458963394165\n",
      "[Training Epoch 0] Batch 2075, Loss 0.5234820246696472\n",
      "[Training Epoch 0] Batch 2076, Loss 0.4889376759529114\n",
      "[Training Epoch 0] Batch 2077, Loss 0.5371363759040833\n",
      "[Training Epoch 0] Batch 2078, Loss 0.5182735323905945\n",
      "[Training Epoch 0] Batch 2079, Loss 0.484637588262558\n",
      "[Training Epoch 0] Batch 2080, Loss 0.5161958336830139\n",
      "[Training Epoch 0] Batch 2081, Loss 0.4888688623905182\n",
      "[Training Epoch 0] Batch 2082, Loss 0.5044797658920288\n",
      "[Training Epoch 0] Batch 2083, Loss 0.5064547061920166\n",
      "[Training Epoch 0] Batch 2084, Loss 0.5064361095428467\n",
      "[Training Epoch 0] Batch 2085, Loss 0.5180104970932007\n",
      "[Training Epoch 0] Batch 2086, Loss 0.528756320476532\n",
      "[Training Epoch 0] Batch 2087, Loss 0.5137887001037598\n",
      "[Training Epoch 0] Batch 2088, Loss 0.4949175715446472\n",
      "[Training Epoch 0] Batch 2089, Loss 0.4992806017398834\n",
      "[Training Epoch 0] Batch 2090, Loss 0.5266376733779907\n",
      "[Training Epoch 0] Batch 2091, Loss 0.5179827213287354\n",
      "[Training Epoch 0] Batch 2092, Loss 0.5125974416732788\n",
      "[Training Epoch 0] Batch 2093, Loss 0.5109274387359619\n",
      "[Training Epoch 0] Batch 2094, Loss 0.5096020102500916\n",
      "[Training Epoch 0] Batch 2095, Loss 0.4799404740333557\n",
      "[Training Epoch 0] Batch 2096, Loss 0.5094627141952515\n",
      "[Training Epoch 0] Batch 2097, Loss 0.5240296125411987\n",
      "[Training Epoch 0] Batch 2098, Loss 0.5126190781593323\n",
      "[Training Epoch 0] Batch 2099, Loss 0.5107517242431641\n",
      "[Training Epoch 0] Batch 2100, Loss 0.4981122314929962\n",
      "[Training Epoch 0] Batch 2101, Loss 0.5169986486434937\n",
      "[Training Epoch 0] Batch 2102, Loss 0.4902459681034088\n",
      "[Training Epoch 0] Batch 2103, Loss 0.5032570362091064\n",
      "[Training Epoch 0] Batch 2104, Loss 0.5072174668312073\n",
      "[Training Epoch 0] Batch 2105, Loss 0.5398020148277283\n",
      "[Training Epoch 0] Batch 2106, Loss 0.5157830119132996\n",
      "[Training Epoch 0] Batch 2107, Loss 0.49879610538482666\n",
      "[Training Epoch 0] Batch 2108, Loss 0.4827688932418823\n",
      "[Training Epoch 0] Batch 2109, Loss 0.5273866057395935\n",
      "[Training Epoch 0] Batch 2110, Loss 0.46869319677352905\n",
      "[Training Epoch 0] Batch 2111, Loss 0.5079513192176819\n",
      "[Training Epoch 0] Batch 2112, Loss 0.5178747773170471\n",
      "[Training Epoch 0] Batch 2113, Loss 0.5093629956245422\n",
      "[Training Epoch 0] Batch 2114, Loss 0.5050733685493469\n",
      "[Training Epoch 0] Batch 2115, Loss 0.520136833190918\n",
      "[Training Epoch 0] Batch 2116, Loss 0.49927422404289246\n",
      "[Training Epoch 0] Batch 2117, Loss 0.5070676803588867\n",
      "[Training Epoch 0] Batch 2118, Loss 0.49635788798332214\n",
      "[Training Epoch 0] Batch 2119, Loss 0.5071831941604614\n",
      "[Training Epoch 0] Batch 2120, Loss 0.48261669278144836\n",
      "[Training Epoch 0] Batch 2121, Loss 0.4959963262081146\n",
      "[Training Epoch 0] Batch 2122, Loss 0.509172797203064\n",
      "[Training Epoch 0] Batch 2123, Loss 0.5077378749847412\n",
      "[Training Epoch 0] Batch 2124, Loss 0.5069789886474609\n",
      "[Training Epoch 0] Batch 2125, Loss 0.5088747143745422\n",
      "[Training Epoch 0] Batch 2126, Loss 0.49414849281311035\n",
      "[Training Epoch 0] Batch 2127, Loss 0.5240016579627991\n",
      "[Training Epoch 0] Batch 2128, Loss 0.4896404445171356\n",
      "[Training Epoch 0] Batch 2129, Loss 0.48864635825157166\n",
      "[Training Epoch 0] Batch 2130, Loss 0.508023202419281\n",
      "[Training Epoch 0] Batch 2131, Loss 0.4853082001209259\n",
      "[Training Epoch 0] Batch 2132, Loss 0.5207332968711853\n",
      "[Training Epoch 0] Batch 2133, Loss 0.502139151096344\n",
      "[Training Epoch 0] Batch 2134, Loss 0.5096952319145203\n",
      "[Training Epoch 0] Batch 2135, Loss 0.5078110694885254\n",
      "[Training Epoch 0] Batch 2136, Loss 0.505755603313446\n",
      "[Training Epoch 0] Batch 2137, Loss 0.5237410068511963\n",
      "[Training Epoch 0] Batch 2138, Loss 0.5055750012397766\n",
      "[Training Epoch 0] Batch 2139, Loss 0.508863627910614\n",
      "[Training Epoch 0] Batch 2140, Loss 0.508828341960907\n",
      "[Training Epoch 0] Batch 2141, Loss 0.5184369683265686\n",
      "[Training Epoch 0] Batch 2142, Loss 0.5010887384414673\n",
      "[Training Epoch 0] Batch 2143, Loss 0.5130401849746704\n",
      "[Training Epoch 0] Batch 2144, Loss 0.5194275975227356\n",
      "[Training Epoch 0] Batch 2145, Loss 0.4989413321018219\n",
      "[Training Epoch 0] Batch 2146, Loss 0.51285320520401\n",
      "[Training Epoch 0] Batch 2147, Loss 0.5029844045639038\n",
      "[Training Epoch 0] Batch 2148, Loss 0.48292702436447144\n",
      "[Training Epoch 0] Batch 2149, Loss 0.5407736301422119\n",
      "[Training Epoch 0] Batch 2150, Loss 0.5181775689125061\n",
      "[Training Epoch 0] Batch 2151, Loss 0.5169207453727722\n",
      "[Training Epoch 0] Batch 2152, Loss 0.5198089480400085\n",
      "[Training Epoch 0] Batch 2153, Loss 0.5075270533561707\n",
      "[Training Epoch 0] Batch 2154, Loss 0.500805675983429\n",
      "[Training Epoch 0] Batch 2155, Loss 0.4943420886993408\n",
      "[Training Epoch 0] Batch 2156, Loss 0.5149256587028503\n",
      "[Training Epoch 0] Batch 2157, Loss 0.4910918176174164\n",
      "[Training Epoch 0] Batch 2158, Loss 0.5125609636306763\n",
      "[Training Epoch 0] Batch 2159, Loss 0.5320484042167664\n",
      "[Training Epoch 0] Batch 2160, Loss 0.5204362273216248\n",
      "[Training Epoch 0] Batch 2161, Loss 0.5286295413970947\n",
      "[Training Epoch 0] Batch 2162, Loss 0.5191866755485535\n",
      "[Training Epoch 0] Batch 2163, Loss 0.5147310495376587\n",
      "[Training Epoch 0] Batch 2164, Loss 0.5225216150283813\n",
      "[Training Epoch 0] Batch 2165, Loss 0.49902719259262085\n",
      "[Training Epoch 0] Batch 2166, Loss 0.48778200149536133\n",
      "[Training Epoch 0] Batch 2167, Loss 0.5106669068336487\n",
      "[Training Epoch 0] Batch 2168, Loss 0.4983777403831482\n",
      "[Training Epoch 0] Batch 2169, Loss 0.48580068349838257\n",
      "[Training Epoch 0] Batch 2170, Loss 0.51819908618927\n",
      "[Training Epoch 0] Batch 2171, Loss 0.49195706844329834\n",
      "[Training Epoch 0] Batch 2172, Loss 0.4950389564037323\n",
      "[Training Epoch 0] Batch 2173, Loss 0.5407872200012207\n",
      "[Training Epoch 0] Batch 2174, Loss 0.49924933910369873\n",
      "[Training Epoch 0] Batch 2175, Loss 0.5082521438598633\n",
      "[Training Epoch 0] Batch 2176, Loss 0.5264747738838196\n",
      "[Training Epoch 0] Batch 2177, Loss 0.5301100015640259\n",
      "[Training Epoch 0] Batch 2178, Loss 0.4833059012889862\n",
      "[Training Epoch 0] Batch 2179, Loss 0.5262500643730164\n",
      "[Training Epoch 0] Batch 2180, Loss 0.48979344964027405\n",
      "[Training Epoch 0] Batch 2181, Loss 0.5023298263549805\n",
      "[Training Epoch 0] Batch 2182, Loss 0.5101373791694641\n",
      "[Training Epoch 0] Batch 2183, Loss 0.5105258226394653\n",
      "[Training Epoch 0] Batch 2184, Loss 0.5003101229667664\n",
      "[Training Epoch 0] Batch 2185, Loss 0.5037785768508911\n",
      "[Training Epoch 0] Batch 2186, Loss 0.5113799571990967\n",
      "[Training Epoch 0] Batch 2187, Loss 0.5059416890144348\n",
      "[Training Epoch 0] Batch 2188, Loss 0.5171035528182983\n",
      "[Training Epoch 0] Batch 2189, Loss 0.5318602919578552\n",
      "[Training Epoch 0] Batch 2190, Loss 0.5077822208404541\n",
      "[Training Epoch 0] Batch 2191, Loss 0.49365127086639404\n",
      "[Training Epoch 0] Batch 2192, Loss 0.5142626762390137\n",
      "[Training Epoch 0] Batch 2193, Loss 0.524345338344574\n",
      "[Training Epoch 0] Batch 2194, Loss 0.511411190032959\n",
      "[Training Epoch 0] Batch 2195, Loss 0.5307261347770691\n",
      "[Training Epoch 0] Batch 2196, Loss 0.543937087059021\n",
      "[Training Epoch 0] Batch 2197, Loss 0.5154092907905579\n",
      "[Training Epoch 0] Batch 2198, Loss 0.4999748170375824\n",
      "[Training Epoch 0] Batch 2199, Loss 0.5037885904312134\n",
      "[Training Epoch 0] Batch 2200, Loss 0.4869306981563568\n",
      "[Training Epoch 0] Batch 2201, Loss 0.4849220812320709\n",
      "[Training Epoch 0] Batch 2202, Loss 0.4780685305595398\n",
      "[Training Epoch 0] Batch 2203, Loss 0.51875901222229\n",
      "[Training Epoch 0] Batch 2204, Loss 0.5107285976409912\n",
      "[Training Epoch 0] Batch 2205, Loss 0.4989270567893982\n",
      "[Training Epoch 0] Batch 2206, Loss 0.5274966955184937\n",
      "[Training Epoch 0] Batch 2207, Loss 0.4881390631198883\n",
      "[Training Epoch 0] Batch 2208, Loss 0.5165411233901978\n",
      "[Training Epoch 0] Batch 2209, Loss 0.49471402168273926\n",
      "[Training Epoch 0] Batch 2210, Loss 0.4845888316631317\n",
      "[Training Epoch 0] Batch 2211, Loss 0.5176099538803101\n",
      "[Training Epoch 0] Batch 2212, Loss 0.5055233836174011\n",
      "[Training Epoch 0] Batch 2213, Loss 0.5218995213508606\n",
      "[Training Epoch 0] Batch 2214, Loss 0.511835515499115\n",
      "[Training Epoch 0] Batch 2215, Loss 0.502231776714325\n",
      "[Training Epoch 0] Batch 2216, Loss 0.5181464552879333\n",
      "[Training Epoch 0] Batch 2217, Loss 0.5185741186141968\n",
      "[Training Epoch 0] Batch 2218, Loss 0.49788200855255127\n",
      "[Training Epoch 0] Batch 2219, Loss 0.5141424536705017\n",
      "[Training Epoch 0] Batch 2220, Loss 0.5036389827728271\n",
      "[Training Epoch 0] Batch 2221, Loss 0.5098042488098145\n",
      "[Training Epoch 0] Batch 2222, Loss 0.5336429476737976\n",
      "[Training Epoch 0] Batch 2223, Loss 0.5085121989250183\n",
      "[Training Epoch 0] Batch 2224, Loss 0.5019562840461731\n",
      "[Training Epoch 0] Batch 2225, Loss 0.5086491703987122\n",
      "[Training Epoch 0] Batch 2226, Loss 0.5019488334655762\n",
      "[Training Epoch 0] Batch 2227, Loss 0.512147843837738\n",
      "[Training Epoch 0] Batch 2228, Loss 0.5083513855934143\n",
      "[Training Epoch 0] Batch 2229, Loss 0.5010052919387817\n",
      "[Training Epoch 0] Batch 2230, Loss 0.5165015459060669\n",
      "[Training Epoch 0] Batch 2231, Loss 0.5153382420539856\n",
      "[Training Epoch 0] Batch 2232, Loss 0.4941757619380951\n",
      "[Training Epoch 0] Batch 2233, Loss 0.48509594798088074\n",
      "[Training Epoch 0] Batch 2234, Loss 0.5140960812568665\n",
      "[Training Epoch 0] Batch 2235, Loss 0.5028128027915955\n",
      "[Training Epoch 0] Batch 2236, Loss 0.47513681650161743\n",
      "[Training Epoch 0] Batch 2237, Loss 0.49654436111450195\n",
      "[Training Epoch 0] Batch 2238, Loss 0.5026018023490906\n",
      "[Training Epoch 0] Batch 2239, Loss 0.5314486026763916\n",
      "[Training Epoch 0] Batch 2240, Loss 0.48278260231018066\n",
      "[Training Epoch 0] Batch 2241, Loss 0.47871437668800354\n",
      "[Training Epoch 0] Batch 2242, Loss 0.4882183074951172\n",
      "[Training Epoch 0] Batch 2243, Loss 0.5206695795059204\n",
      "[Training Epoch 0] Batch 2244, Loss 0.5052015781402588\n",
      "[Training Epoch 0] Batch 2245, Loss 0.509074866771698\n",
      "[Training Epoch 0] Batch 2246, Loss 0.5159554481506348\n",
      "[Training Epoch 0] Batch 2247, Loss 0.4985738694667816\n",
      "[Training Epoch 0] Batch 2248, Loss 0.49967920780181885\n",
      "[Training Epoch 0] Batch 2249, Loss 0.507372260093689\n",
      "[Training Epoch 0] Batch 2250, Loss 0.5198272466659546\n",
      "[Training Epoch 0] Batch 2251, Loss 0.47489941120147705\n",
      "[Training Epoch 0] Batch 2252, Loss 0.495045930147171\n",
      "[Training Epoch 0] Batch 2253, Loss 0.5138591527938843\n",
      "[Training Epoch 0] Batch 2254, Loss 0.5351896286010742\n",
      "[Training Epoch 0] Batch 2255, Loss 0.5168300867080688\n",
      "[Training Epoch 0] Batch 2256, Loss 0.4826444983482361\n",
      "[Training Epoch 0] Batch 2257, Loss 0.5302585959434509\n",
      "[Training Epoch 0] Batch 2258, Loss 0.48361849784851074\n",
      "[Training Epoch 0] Batch 2259, Loss 0.5003086924552917\n",
      "[Training Epoch 0] Batch 2260, Loss 0.5064414739608765\n",
      "[Training Epoch 0] Batch 2261, Loss 0.5108422040939331\n",
      "[Training Epoch 0] Batch 2262, Loss 0.5221825838088989\n",
      "[Training Epoch 0] Batch 2263, Loss 0.506942093372345\n",
      "[Training Epoch 0] Batch 2264, Loss 0.5093926191329956\n",
      "[Training Epoch 0] Batch 2265, Loss 0.5031372308731079\n",
      "[Training Epoch 0] Batch 2266, Loss 0.5278781056404114\n",
      "[Training Epoch 0] Batch 2267, Loss 0.505940854549408\n",
      "[Training Epoch 0] Batch 2268, Loss 0.5051370859146118\n",
      "[Training Epoch 0] Batch 2269, Loss 0.5065473914146423\n",
      "[Training Epoch 0] Batch 2270, Loss 0.5022661685943604\n",
      "[Training Epoch 0] Batch 2271, Loss 0.4978500008583069\n",
      "[Training Epoch 0] Batch 2272, Loss 0.5034927129745483\n",
      "[Training Epoch 0] Batch 2273, Loss 0.5025779008865356\n",
      "[Training Epoch 0] Batch 2274, Loss 0.5076197385787964\n",
      "[Training Epoch 0] Batch 2275, Loss 0.5391279458999634\n",
      "[Training Epoch 0] Batch 2276, Loss 0.4758496582508087\n",
      "[Training Epoch 0] Batch 2277, Loss 0.5133876800537109\n",
      "[Training Epoch 0] Batch 2278, Loss 0.47575652599334717\n",
      "[Training Epoch 0] Batch 2279, Loss 0.47124820947647095\n",
      "[Training Epoch 0] Batch 2280, Loss 0.4741784930229187\n",
      "[Training Epoch 0] Batch 2281, Loss 0.4907642900943756\n",
      "[Training Epoch 0] Batch 2282, Loss 0.5054451823234558\n",
      "[Training Epoch 0] Batch 2283, Loss 0.49005624651908875\n",
      "[Training Epoch 0] Batch 2284, Loss 0.5178225040435791\n",
      "[Training Epoch 0] Batch 2285, Loss 0.5187669992446899\n",
      "[Training Epoch 0] Batch 2286, Loss 0.4807140529155731\n",
      "[Training Epoch 0] Batch 2287, Loss 0.4732687473297119\n",
      "[Training Epoch 0] Batch 2288, Loss 0.5224161148071289\n",
      "[Training Epoch 0] Batch 2289, Loss 0.518001139163971\n",
      "[Training Epoch 0] Batch 2290, Loss 0.4871748089790344\n",
      "[Training Epoch 0] Batch 2291, Loss 0.5201820135116577\n",
      "[Training Epoch 0] Batch 2292, Loss 0.5303546786308289\n",
      "[Training Epoch 0] Batch 2293, Loss 0.5211912393569946\n",
      "[Training Epoch 0] Batch 2294, Loss 0.5064764618873596\n",
      "[Training Epoch 0] Batch 2295, Loss 0.5032213926315308\n",
      "[Training Epoch 0] Batch 2296, Loss 0.4918423891067505\n",
      "[Training Epoch 0] Batch 2297, Loss 0.5053978562355042\n",
      "[Training Epoch 0] Batch 2298, Loss 0.502133846282959\n",
      "[Training Epoch 0] Batch 2299, Loss 0.5099973082542419\n",
      "[Training Epoch 0] Batch 2300, Loss 0.5323926210403442\n",
      "[Training Epoch 0] Batch 2301, Loss 0.4898614287376404\n",
      "[Training Epoch 0] Batch 2302, Loss 0.5010639429092407\n",
      "[Training Epoch 0] Batch 2303, Loss 0.49737289547920227\n",
      "[Training Epoch 0] Batch 2304, Loss 0.4897027611732483\n",
      "[Training Epoch 0] Batch 2305, Loss 0.49653318524360657\n",
      "[Training Epoch 0] Batch 2306, Loss 0.5043330788612366\n",
      "[Training Epoch 0] Batch 2307, Loss 0.4931725263595581\n",
      "[Training Epoch 0] Batch 2308, Loss 0.5054017305374146\n",
      "[Training Epoch 0] Batch 2309, Loss 0.49942949414253235\n",
      "[Training Epoch 0] Batch 2310, Loss 0.5134470462799072\n",
      "[Training Epoch 0] Batch 2311, Loss 0.4873003363609314\n",
      "[Training Epoch 0] Batch 2312, Loss 0.48404747247695923\n",
      "[Training Epoch 0] Batch 2313, Loss 0.5109955072402954\n",
      "[Training Epoch 0] Batch 2314, Loss 0.4702204167842865\n",
      "[Training Epoch 0] Batch 2315, Loss 0.5019779205322266\n",
      "[Training Epoch 0] Batch 2316, Loss 0.4961334466934204\n",
      "[Training Epoch 0] Batch 2317, Loss 0.5042828917503357\n",
      "[Training Epoch 0] Batch 2318, Loss 0.48919394612312317\n",
      "[Training Epoch 0] Batch 2319, Loss 0.49362504482269287\n",
      "[Training Epoch 0] Batch 2320, Loss 0.5469291806221008\n",
      "[Training Epoch 0] Batch 2321, Loss 0.49047380685806274\n",
      "[Training Epoch 0] Batch 2322, Loss 0.5252107381820679\n",
      "[Training Epoch 0] Batch 2323, Loss 0.49134039878845215\n",
      "[Training Epoch 0] Batch 2324, Loss 0.5094940066337585\n",
      "[Training Epoch 0] Batch 2325, Loss 0.5029070377349854\n",
      "[Training Epoch 0] Batch 2326, Loss 0.4983980655670166\n",
      "[Training Epoch 0] Batch 2327, Loss 0.489124596118927\n",
      "[Training Epoch 0] Batch 2328, Loss 0.5241276025772095\n",
      "[Training Epoch 0] Batch 2329, Loss 0.5092031955718994\n",
      "[Training Epoch 0] Batch 2330, Loss 0.47209614515304565\n",
      "[Training Epoch 0] Batch 2331, Loss 0.5274338126182556\n",
      "[Training Epoch 0] Batch 2332, Loss 0.5369126796722412\n",
      "[Training Epoch 0] Batch 2333, Loss 0.5037301778793335\n",
      "[Training Epoch 0] Batch 2334, Loss 0.5048902034759521\n",
      "[Training Epoch 0] Batch 2335, Loss 0.4944315552711487\n",
      "[Training Epoch 0] Batch 2336, Loss 0.48560360074043274\n",
      "[Training Epoch 0] Batch 2337, Loss 0.5029376149177551\n",
      "[Training Epoch 0] Batch 2338, Loss 0.5153279900550842\n",
      "[Training Epoch 0] Batch 2339, Loss 0.4925481677055359\n",
      "[Training Epoch 0] Batch 2340, Loss 0.48676300048828125\n",
      "[Training Epoch 0] Batch 2341, Loss 0.5256907343864441\n",
      "[Training Epoch 0] Batch 2342, Loss 0.4947560727596283\n",
      "[Training Epoch 0] Batch 2343, Loss 0.48897281289100647\n",
      "[Training Epoch 0] Batch 2344, Loss 0.5287530422210693\n",
      "[Training Epoch 0] Batch 2345, Loss 0.4901220202445984\n",
      "[Training Epoch 0] Batch 2346, Loss 0.4982590675354004\n",
      "[Training Epoch 0] Batch 2347, Loss 0.5138794183731079\n",
      "[Training Epoch 0] Batch 2348, Loss 0.5091260671615601\n",
      "[Training Epoch 0] Batch 2349, Loss 0.5251767635345459\n",
      "[Training Epoch 0] Batch 2350, Loss 0.4934583604335785\n",
      "[Training Epoch 0] Batch 2351, Loss 0.49932071566581726\n",
      "[Training Epoch 0] Batch 2352, Loss 0.5114212036132812\n",
      "[Training Epoch 0] Batch 2353, Loss 0.5003390312194824\n",
      "[Training Epoch 0] Batch 2354, Loss 0.47524145245552063\n",
      "[Training Epoch 0] Batch 2355, Loss 0.5195237994194031\n",
      "[Training Epoch 0] Batch 2356, Loss 0.525294303894043\n",
      "[Training Epoch 0] Batch 2357, Loss 0.4989892244338989\n",
      "[Training Epoch 0] Batch 2358, Loss 0.5057356357574463\n",
      "[Training Epoch 0] Batch 2359, Loss 0.5322349071502686\n",
      "[Training Epoch 0] Batch 2360, Loss 0.48884308338165283\n",
      "[Training Epoch 0] Batch 2361, Loss 0.5126736164093018\n",
      "[Training Epoch 0] Batch 2362, Loss 0.5012423396110535\n",
      "[Training Epoch 0] Batch 2363, Loss 0.49589812755584717\n",
      "[Training Epoch 0] Batch 2364, Loss 0.49439457058906555\n",
      "[Training Epoch 0] Batch 2365, Loss 0.5217604637145996\n",
      "[Training Epoch 0] Batch 2366, Loss 0.5422230362892151\n",
      "[Training Epoch 0] Batch 2367, Loss 0.4955514967441559\n",
      "[Training Epoch 0] Batch 2368, Loss 0.5043666362762451\n",
      "[Training Epoch 0] Batch 2369, Loss 0.5034707188606262\n",
      "[Training Epoch 0] Batch 2370, Loss 0.4793909192085266\n",
      "[Training Epoch 0] Batch 2371, Loss 0.5089712738990784\n",
      "[Training Epoch 0] Batch 2372, Loss 0.4911002814769745\n",
      "[Training Epoch 0] Batch 2373, Loss 0.4744831323623657\n",
      "[Training Epoch 0] Batch 2374, Loss 0.5003191828727722\n",
      "[Training Epoch 0] Batch 2375, Loss 0.5215165019035339\n",
      "[Training Epoch 0] Batch 2376, Loss 0.5045298933982849\n",
      "[Training Epoch 0] Batch 2377, Loss 0.5034103393554688\n",
      "[Training Epoch 0] Batch 2378, Loss 0.5021952986717224\n",
      "[Training Epoch 0] Batch 2379, Loss 0.5161656141281128\n",
      "[Training Epoch 0] Batch 2380, Loss 0.4907117187976837\n",
      "[Training Epoch 0] Batch 2381, Loss 0.5088230967521667\n",
      "[Training Epoch 0] Batch 2382, Loss 0.4927689731121063\n",
      "[Training Epoch 0] Batch 2383, Loss 0.4921153783798218\n",
      "[Training Epoch 0] Batch 2384, Loss 0.5181435942649841\n",
      "[Training Epoch 0] Batch 2385, Loss 0.4953339397907257\n",
      "[Training Epoch 0] Batch 2386, Loss 0.48839128017425537\n",
      "[Training Epoch 0] Batch 2387, Loss 0.5108554363250732\n",
      "[Training Epoch 0] Batch 2388, Loss 0.4815395176410675\n",
      "[Training Epoch 0] Batch 2389, Loss 0.4963425397872925\n",
      "[Training Epoch 0] Batch 2390, Loss 0.5006881952285767\n",
      "[Training Epoch 0] Batch 2391, Loss 0.5046905875205994\n",
      "[Training Epoch 0] Batch 2392, Loss 0.5133415460586548\n",
      "[Training Epoch 0] Batch 2393, Loss 0.5088436603546143\n",
      "[Training Epoch 0] Batch 2394, Loss 0.5090718865394592\n",
      "[Training Epoch 0] Batch 2395, Loss 0.4949733018875122\n",
      "[Training Epoch 0] Batch 2396, Loss 0.4903910160064697\n",
      "[Training Epoch 0] Batch 2397, Loss 0.5308359861373901\n",
      "[Training Epoch 0] Batch 2398, Loss 0.47642284631729126\n",
      "[Training Epoch 0] Batch 2399, Loss 0.4881676137447357\n",
      "[Training Epoch 0] Batch 2400, Loss 0.5146477818489075\n",
      "[Training Epoch 0] Batch 2401, Loss 0.4938182830810547\n",
      "[Training Epoch 0] Batch 2402, Loss 0.5053816437721252\n",
      "[Training Epoch 0] Batch 2403, Loss 0.48579201102256775\n",
      "[Training Epoch 0] Batch 2404, Loss 0.4902980923652649\n",
      "[Training Epoch 0] Batch 2405, Loss 0.5020456314086914\n",
      "[Training Epoch 0] Batch 2406, Loss 0.4856662154197693\n",
      "[Training Epoch 0] Batch 2407, Loss 0.4778652787208557\n",
      "[Training Epoch 0] Batch 2408, Loss 0.5065898895263672\n",
      "[Training Epoch 0] Batch 2409, Loss 0.5110964179039001\n",
      "[Training Epoch 0] Batch 2410, Loss 0.5145939588546753\n",
      "[Training Epoch 0] Batch 2411, Loss 0.48334449529647827\n",
      "[Training Epoch 0] Batch 2412, Loss 0.4913940727710724\n",
      "[Training Epoch 0] Batch 2413, Loss 0.5041142702102661\n",
      "[Training Epoch 0] Batch 2414, Loss 0.5251771211624146\n",
      "[Training Epoch 0] Batch 2415, Loss 0.4903135299682617\n",
      "[Training Epoch 0] Batch 2416, Loss 0.49121659994125366\n",
      "[Training Epoch 0] Batch 2417, Loss 0.5041911602020264\n",
      "[Training Epoch 0] Batch 2418, Loss 0.5049189925193787\n",
      "[Training Epoch 0] Batch 2419, Loss 0.5040419101715088\n",
      "[Training Epoch 0] Batch 2420, Loss 0.4947354793548584\n",
      "[Training Epoch 0] Batch 2421, Loss 0.5317583084106445\n",
      "[Training Epoch 0] Batch 2422, Loss 0.5002016425132751\n",
      "[Training Epoch 0] Batch 2423, Loss 0.5015689134597778\n",
      "[Training Epoch 0] Batch 2424, Loss 0.5144950747489929\n",
      "[Training Epoch 0] Batch 2425, Loss 0.5258141756057739\n",
      "[Training Epoch 0] Batch 2426, Loss 0.48286211490631104\n",
      "[Training Epoch 0] Batch 2427, Loss 0.49566736817359924\n",
      "[Training Epoch 0] Batch 2428, Loss 0.5209915041923523\n",
      "[Training Epoch 0] Batch 2429, Loss 0.4700566232204437\n",
      "[Training Epoch 0] Batch 2430, Loss 0.49917030334472656\n",
      "[Training Epoch 0] Batch 2431, Loss 0.5179995894432068\n",
      "[Training Epoch 0] Batch 2432, Loss 0.5143151879310608\n",
      "[Training Epoch 0] Batch 2433, Loss 0.5238821506500244\n",
      "[Training Epoch 0] Batch 2434, Loss 0.502682089805603\n",
      "[Training Epoch 0] Batch 2435, Loss 0.5080676078796387\n",
      "[Training Epoch 0] Batch 2436, Loss 0.5170013904571533\n",
      "[Training Epoch 0] Batch 2437, Loss 0.4974108338356018\n",
      "[Training Epoch 0] Batch 2438, Loss 0.48382118344306946\n",
      "[Training Epoch 0] Batch 2439, Loss 0.5063280463218689\n",
      "[Training Epoch 0] Batch 2440, Loss 0.5143338441848755\n",
      "[Training Epoch 0] Batch 2441, Loss 0.5135172605514526\n",
      "[Training Epoch 0] Batch 2442, Loss 0.5282464027404785\n",
      "[Training Epoch 0] Batch 2443, Loss 0.5095230340957642\n",
      "[Training Epoch 0] Batch 2444, Loss 0.5306820869445801\n",
      "[Training Epoch 0] Batch 2445, Loss 0.509318470954895\n",
      "[Training Epoch 0] Batch 2446, Loss 0.48327070474624634\n",
      "[Training Epoch 0] Batch 2447, Loss 0.5279383659362793\n",
      "[Training Epoch 0] Batch 2448, Loss 0.5141416192054749\n",
      "[Training Epoch 0] Batch 2449, Loss 0.5018032193183899\n",
      "[Training Epoch 0] Batch 2450, Loss 0.5127513408660889\n",
      "[Training Epoch 0] Batch 2451, Loss 0.49432095885276794\n",
      "[Training Epoch 0] Batch 2452, Loss 0.5279585123062134\n",
      "[Training Epoch 0] Batch 2453, Loss 0.5102847814559937\n",
      "[Training Epoch 0] Batch 2454, Loss 0.4862699806690216\n",
      "[Training Epoch 0] Batch 2455, Loss 0.5143868923187256\n",
      "[Training Epoch 0] Batch 2456, Loss 0.4872836768627167\n",
      "[Training Epoch 0] Batch 2457, Loss 0.48591968417167664\n",
      "[Training Epoch 0] Batch 2458, Loss 0.5040251612663269\n",
      "[Training Epoch 0] Batch 2459, Loss 0.49817198514938354\n",
      "[Training Epoch 0] Batch 2460, Loss 0.5069315433502197\n",
      "[Training Epoch 0] Batch 2461, Loss 0.5169573426246643\n",
      "[Training Epoch 0] Batch 2462, Loss 0.5246075987815857\n",
      "[Training Epoch 0] Batch 2463, Loss 0.5003396272659302\n",
      "[Training Epoch 0] Batch 2464, Loss 0.5131815671920776\n",
      "[Training Epoch 0] Batch 2465, Loss 0.5144367814064026\n",
      "[Training Epoch 0] Batch 2466, Loss 0.49873149394989014\n",
      "[Training Epoch 0] Batch 2467, Loss 0.5179933905601501\n",
      "[Training Epoch 0] Batch 2468, Loss 0.49703359603881836\n",
      "[Training Epoch 0] Batch 2469, Loss 0.4959103763103485\n",
      "[Training Epoch 0] Batch 2470, Loss 0.4978235363960266\n",
      "[Training Epoch 0] Batch 2471, Loss 0.48513659834861755\n",
      "[Training Epoch 0] Batch 2472, Loss 0.5201894044876099\n",
      "[Training Epoch 0] Batch 2473, Loss 0.5005293488502502\n",
      "[Training Epoch 0] Batch 2474, Loss 0.49420860409736633\n",
      "[Training Epoch 0] Batch 2475, Loss 0.5313921570777893\n",
      "[Training Epoch 0] Batch 2476, Loss 0.4826142489910126\n",
      "[Training Epoch 0] Batch 2477, Loss 0.5048443675041199\n",
      "[Training Epoch 0] Batch 2478, Loss 0.5107046961784363\n",
      "[Training Epoch 0] Batch 2479, Loss 0.5142890214920044\n",
      "[Training Epoch 0] Batch 2480, Loss 0.48857542872428894\n",
      "[Training Epoch 0] Batch 2481, Loss 0.5059927701950073\n",
      "[Training Epoch 0] Batch 2482, Loss 0.5116265416145325\n",
      "[Training Epoch 0] Batch 2483, Loss 0.5058897733688354\n",
      "[Training Epoch 0] Batch 2484, Loss 0.4824732542037964\n",
      "[Training Epoch 0] Batch 2485, Loss 0.49655288457870483\n",
      "[Training Epoch 0] Batch 2486, Loss 0.4907974600791931\n",
      "[Training Epoch 0] Batch 2487, Loss 0.4844016432762146\n",
      "[Training Epoch 0] Batch 2488, Loss 0.5115957260131836\n",
      "[Training Epoch 0] Batch 2489, Loss 0.48237574100494385\n",
      "[Training Epoch 0] Batch 2490, Loss 0.4987143278121948\n",
      "[Training Epoch 0] Batch 2491, Loss 0.515048086643219\n",
      "[Training Epoch 0] Batch 2492, Loss 0.517480731010437\n",
      "[Training Epoch 0] Batch 2493, Loss 0.47734972834587097\n",
      "[Training Epoch 0] Batch 2494, Loss 0.47763004899024963\n",
      "[Training Epoch 0] Batch 2495, Loss 0.5048024654388428\n",
      "[Training Epoch 0] Batch 2496, Loss 0.5315635204315186\n",
      "[Training Epoch 0] Batch 2497, Loss 0.5290747880935669\n",
      "[Training Epoch 0] Batch 2498, Loss 0.5272083282470703\n",
      "[Training Epoch 0] Batch 2499, Loss 0.5104877948760986\n",
      "[Training Epoch 0] Batch 2500, Loss 0.5139706134796143\n",
      "[Training Epoch 0] Batch 2501, Loss 0.4895886778831482\n",
      "[Training Epoch 0] Batch 2502, Loss 0.5083342790603638\n",
      "[Training Epoch 0] Batch 2503, Loss 0.5128340721130371\n",
      "[Training Epoch 0] Batch 2504, Loss 0.504508912563324\n",
      "[Training Epoch 0] Batch 2505, Loss 0.49631473422050476\n",
      "[Training Epoch 0] Batch 2506, Loss 0.49742674827575684\n",
      "[Training Epoch 0] Batch 2507, Loss 0.5186916589736938\n",
      "[Training Epoch 0] Batch 2508, Loss 0.5079585909843445\n",
      "[Training Epoch 0] Batch 2509, Loss 0.4819997251033783\n",
      "[Training Epoch 0] Batch 2510, Loss 0.49066877365112305\n",
      "[Training Epoch 0] Batch 2511, Loss 0.48682111501693726\n",
      "[Training Epoch 0] Batch 2512, Loss 0.5068767070770264\n",
      "[Training Epoch 0] Batch 2513, Loss 0.49416694045066833\n",
      "[Training Epoch 0] Batch 2514, Loss 0.49964919686317444\n",
      "[Training Epoch 0] Batch 2515, Loss 0.49852705001831055\n",
      "[Training Epoch 0] Batch 2516, Loss 0.5317184925079346\n",
      "[Training Epoch 0] Batch 2517, Loss 0.5140243768692017\n",
      "[Training Epoch 0] Batch 2518, Loss 0.5031187534332275\n",
      "[Training Epoch 0] Batch 2519, Loss 0.5093842148780823\n",
      "[Training Epoch 0] Batch 2520, Loss 0.477342426776886\n",
      "[Training Epoch 0] Batch 2521, Loss 0.49711719155311584\n",
      "[Training Epoch 0] Batch 2522, Loss 0.5362508296966553\n",
      "[Training Epoch 0] Batch 2523, Loss 0.4876934289932251\n",
      "[Training Epoch 0] Batch 2524, Loss 0.4877798557281494\n",
      "[Training Epoch 0] Batch 2525, Loss 0.5066978335380554\n",
      "[Training Epoch 0] Batch 2526, Loss 0.48308807611465454\n",
      "[Training Epoch 0] Batch 2527, Loss 0.5116569399833679\n",
      "[Training Epoch 0] Batch 2528, Loss 0.4939633905887604\n",
      "[Training Epoch 0] Batch 2529, Loss 0.5164250135421753\n",
      "[Training Epoch 0] Batch 2530, Loss 0.5115689039230347\n",
      "[Training Epoch 0] Batch 2531, Loss 0.5090779662132263\n",
      "[Training Epoch 0] Batch 2532, Loss 0.49478045105934143\n",
      "[Training Epoch 0] Batch 2533, Loss 0.5256065130233765\n",
      "[Training Epoch 0] Batch 2534, Loss 0.5423722267150879\n",
      "[Training Epoch 0] Batch 2535, Loss 0.49263691902160645\n",
      "[Training Epoch 0] Batch 2536, Loss 0.4900858700275421\n",
      "[Training Epoch 0] Batch 2537, Loss 0.5044112801551819\n",
      "[Training Epoch 0] Batch 2538, Loss 0.5196698904037476\n",
      "[Training Epoch 0] Batch 2539, Loss 0.5126742720603943\n",
      "[Training Epoch 0] Batch 2540, Loss 0.505307674407959\n",
      "[Training Epoch 0] Batch 2541, Loss 0.5411182641983032\n",
      "[Training Epoch 0] Batch 2542, Loss 0.5114454030990601\n",
      "[Training Epoch 0] Batch 2543, Loss 0.5006950497627258\n",
      "[Training Epoch 0] Batch 2544, Loss 0.49715572595596313\n",
      "[Training Epoch 0] Batch 2545, Loss 0.5041688680648804\n",
      "[Training Epoch 0] Batch 2546, Loss 0.5017715096473694\n",
      "[Training Epoch 0] Batch 2547, Loss 0.5055224895477295\n",
      "[Training Epoch 0] Batch 2548, Loss 0.5114216208457947\n",
      "[Training Epoch 0] Batch 2549, Loss 0.532394528388977\n",
      "[Training Epoch 0] Batch 2550, Loss 0.4874217212200165\n",
      "[Training Epoch 0] Batch 2551, Loss 0.5290917158126831\n",
      "[Training Epoch 0] Batch 2552, Loss 0.5054726600646973\n",
      "[Training Epoch 0] Batch 2553, Loss 0.48553937673568726\n",
      "[Training Epoch 0] Batch 2554, Loss 0.4769792854785919\n",
      "[Training Epoch 0] Batch 2555, Loss 0.4852958023548126\n",
      "[Training Epoch 0] Batch 2556, Loss 0.5077484846115112\n",
      "[Training Epoch 0] Batch 2557, Loss 0.49566328525543213\n",
      "[Training Epoch 0] Batch 2558, Loss 0.4947665333747864\n",
      "[Training Epoch 0] Batch 2559, Loss 0.5303122997283936\n",
      "[Training Epoch 0] Batch 2560, Loss 0.5017215013504028\n",
      "[Training Epoch 0] Batch 2561, Loss 0.5055144429206848\n",
      "[Training Epoch 0] Batch 2562, Loss 0.49915939569473267\n",
      "[Training Epoch 0] Batch 2563, Loss 0.49815136194229126\n",
      "[Training Epoch 0] Batch 2564, Loss 0.5114105939865112\n",
      "[Training Epoch 0] Batch 2565, Loss 0.5125274658203125\n",
      "[Training Epoch 0] Batch 2566, Loss 0.5089505314826965\n",
      "[Training Epoch 0] Batch 2567, Loss 0.50760817527771\n",
      "[Training Epoch 0] Batch 2568, Loss 0.48509514331817627\n",
      "[Training Epoch 0] Batch 2569, Loss 0.47210371494293213\n",
      "[Training Epoch 0] Batch 2570, Loss 0.5013772249221802\n",
      "[Training Epoch 0] Batch 2571, Loss 0.5036737322807312\n",
      "[Training Epoch 0] Batch 2572, Loss 0.5230318903923035\n",
      "[Training Epoch 0] Batch 2573, Loss 0.49332988262176514\n",
      "[Training Epoch 0] Batch 2574, Loss 0.49838051199913025\n",
      "[Training Epoch 0] Batch 2575, Loss 0.4945152997970581\n",
      "[Training Epoch 0] Batch 2576, Loss 0.5038062334060669\n",
      "[Training Epoch 0] Batch 2577, Loss 0.4934781491756439\n",
      "[Training Epoch 0] Batch 2578, Loss 0.47916775941848755\n",
      "[Training Epoch 0] Batch 2579, Loss 0.4860093593597412\n",
      "[Training Epoch 0] Batch 2580, Loss 0.5136834383010864\n",
      "[Training Epoch 0] Batch 2581, Loss 0.5149003267288208\n",
      "[Training Epoch 0] Batch 2582, Loss 0.49693557620048523\n",
      "[Training Epoch 0] Batch 2583, Loss 0.5214120745658875\n",
      "[Training Epoch 0] Batch 2584, Loss 0.48740002512931824\n",
      "[Training Epoch 0] Batch 2585, Loss 0.5003190636634827\n",
      "[Training Epoch 0] Batch 2586, Loss 0.48623716831207275\n",
      "[Training Epoch 0] Batch 2587, Loss 0.48229116201400757\n",
      "[Training Epoch 0] Batch 2588, Loss 0.5254709124565125\n",
      "[Training Epoch 0] Batch 2589, Loss 0.5018348097801208\n",
      "[Training Epoch 0] Batch 2590, Loss 0.5181684494018555\n",
      "[Training Epoch 0] Batch 2591, Loss 0.49073028564453125\n",
      "[Training Epoch 0] Batch 2592, Loss 0.5157427787780762\n",
      "[Training Epoch 0] Batch 2593, Loss 0.49191004037857056\n",
      "[Training Epoch 0] Batch 2594, Loss 0.5084239840507507\n",
      "[Training Epoch 0] Batch 2595, Loss 0.520787239074707\n",
      "[Training Epoch 0] Batch 2596, Loss 0.5011796951293945\n",
      "[Training Epoch 0] Batch 2597, Loss 0.49705004692077637\n",
      "[Training Epoch 0] Batch 2598, Loss 0.47523605823516846\n",
      "[Training Epoch 0] Batch 2599, Loss 0.5041832327842712\n",
      "[Training Epoch 0] Batch 2600, Loss 0.510022759437561\n",
      "[Training Epoch 0] Batch 2601, Loss 0.5126002430915833\n",
      "[Training Epoch 0] Batch 2602, Loss 0.4977094531059265\n",
      "[Training Epoch 0] Batch 2603, Loss 0.5254407525062561\n",
      "[Training Epoch 0] Batch 2604, Loss 0.49502357840538025\n",
      "[Training Epoch 0] Batch 2605, Loss 0.4868214726448059\n",
      "[Training Epoch 0] Batch 2606, Loss 0.5048503875732422\n",
      "[Training Epoch 0] Batch 2607, Loss 0.5324749946594238\n",
      "[Training Epoch 0] Batch 2608, Loss 0.4940022826194763\n",
      "[Training Epoch 0] Batch 2609, Loss 0.49598562717437744\n",
      "[Training Epoch 0] Batch 2610, Loss 0.48049595952033997\n",
      "[Training Epoch 0] Batch 2611, Loss 0.5003257989883423\n",
      "[Training Epoch 0] Batch 2612, Loss 0.5100434422492981\n",
      "[Training Epoch 0] Batch 2613, Loss 0.519895076751709\n",
      "[Training Epoch 0] Batch 2614, Loss 0.49348294734954834\n",
      "[Training Epoch 0] Batch 2615, Loss 0.49800360202789307\n",
      "[Training Epoch 0] Batch 2616, Loss 0.4944992661476135\n",
      "[Training Epoch 0] Batch 2617, Loss 0.47078025341033936\n",
      "[Training Epoch 0] Batch 2618, Loss 0.5028907060623169\n",
      "[Training Epoch 0] Batch 2619, Loss 0.5211735963821411\n",
      "[Training Epoch 0] Batch 2620, Loss 0.511377215385437\n",
      "[Training Epoch 0] Batch 2621, Loss 0.5098877549171448\n",
      "[Training Epoch 0] Batch 2622, Loss 0.5005713105201721\n",
      "[Training Epoch 0] Batch 2623, Loss 0.5014100074768066\n",
      "[Training Epoch 0] Batch 2624, Loss 0.49570244550704956\n",
      "[Training Epoch 0] Batch 2625, Loss 0.5217816829681396\n",
      "[Training Epoch 0] Batch 2626, Loss 0.49979910254478455\n",
      "[Training Epoch 0] Batch 2627, Loss 0.4727158844470978\n",
      "[Training Epoch 0] Batch 2628, Loss 0.4918985962867737\n",
      "[Training Epoch 0] Batch 2629, Loss 0.5135418772697449\n",
      "[Training Epoch 0] Batch 2630, Loss 0.5111514329910278\n",
      "[Training Epoch 0] Batch 2631, Loss 0.47715044021606445\n",
      "[Training Epoch 0] Batch 2632, Loss 0.47142624855041504\n",
      "[Training Epoch 0] Batch 2633, Loss 0.5240485072135925\n",
      "[Training Epoch 0] Batch 2634, Loss 0.4959699213504791\n",
      "[Training Epoch 0] Batch 2635, Loss 0.5197810530662537\n",
      "[Training Epoch 0] Batch 2636, Loss 0.49192512035369873\n",
      "[Training Epoch 0] Batch 2637, Loss 0.518039882183075\n",
      "[Training Epoch 0] Batch 2638, Loss 0.5035244822502136\n",
      "[Training Epoch 0] Batch 2639, Loss 0.5086643695831299\n",
      "[Training Epoch 0] Batch 2640, Loss 0.4964905083179474\n",
      "[Training Epoch 0] Batch 2641, Loss 0.517984926700592\n",
      "[Training Epoch 0] Batch 2642, Loss 0.5026077032089233\n",
      "[Training Epoch 0] Batch 2643, Loss 0.4783638119697571\n",
      "[Training Epoch 0] Batch 2644, Loss 0.49771854281425476\n",
      "[Training Epoch 0] Batch 2645, Loss 0.4797053635120392\n",
      "[Training Epoch 0] Batch 2646, Loss 0.4888998866081238\n",
      "[Training Epoch 0] Batch 2647, Loss 0.49535754323005676\n",
      "[Training Epoch 0] Batch 2648, Loss 0.5131334066390991\n",
      "[Training Epoch 0] Batch 2649, Loss 0.5024725794792175\n",
      "[Training Epoch 0] Batch 2650, Loss 0.5087411999702454\n",
      "[Training Epoch 0] Batch 2651, Loss 0.47570639848709106\n",
      "[Training Epoch 0] Batch 2652, Loss 0.466342568397522\n",
      "[Training Epoch 0] Batch 2653, Loss 0.49889296293258667\n",
      "[Training Epoch 0] Batch 2654, Loss 0.5243843197822571\n",
      "[Training Epoch 0] Batch 2655, Loss 0.4928542971611023\n",
      "[Training Epoch 0] Batch 2656, Loss 0.5400201678276062\n",
      "[Training Epoch 0] Batch 2657, Loss 0.5011789202690125\n",
      "[Training Epoch 0] Batch 2658, Loss 0.4697599709033966\n",
      "[Training Epoch 0] Batch 2659, Loss 0.5110966563224792\n",
      "[Training Epoch 0] Batch 2660, Loss 0.5000832676887512\n",
      "[Training Epoch 0] Batch 2661, Loss 0.4999273717403412\n",
      "[Training Epoch 0] Batch 2662, Loss 0.502453088760376\n",
      "[Training Epoch 0] Batch 2663, Loss 0.4829597473144531\n",
      "[Training Epoch 0] Batch 2664, Loss 0.46471500396728516\n",
      "[Training Epoch 0] Batch 2665, Loss 0.5038962960243225\n",
      "[Training Epoch 0] Batch 2666, Loss 0.5048340559005737\n",
      "[Training Epoch 0] Batch 2667, Loss 0.48903095722198486\n",
      "[Training Epoch 0] Batch 2668, Loss 0.5229675769805908\n",
      "[Training Epoch 0] Batch 2669, Loss 0.5482000708580017\n",
      "[Training Epoch 0] Batch 2670, Loss 0.48916110396385193\n",
      "[Training Epoch 0] Batch 2671, Loss 0.5497429370880127\n",
      "[Training Epoch 0] Batch 2672, Loss 0.5133383870124817\n",
      "[Training Epoch 0] Batch 2673, Loss 0.49379727244377136\n",
      "[Training Epoch 0] Batch 2674, Loss 0.4984881579875946\n",
      "[Training Epoch 0] Batch 2675, Loss 0.5157207250595093\n",
      "[Training Epoch 0] Batch 2676, Loss 0.4949583411216736\n",
      "[Training Epoch 0] Batch 2677, Loss 0.4840732514858246\n",
      "[Training Epoch 0] Batch 2678, Loss 0.4890005588531494\n",
      "[Training Epoch 0] Batch 2679, Loss 0.5120249390602112\n",
      "[Training Epoch 0] Batch 2680, Loss 0.48747944831848145\n",
      "[Training Epoch 0] Batch 2681, Loss 0.4899599552154541\n",
      "[Training Epoch 0] Batch 2682, Loss 0.46946844458580017\n",
      "[Training Epoch 0] Batch 2683, Loss 0.504787266254425\n",
      "[Training Epoch 0] Batch 2684, Loss 0.5046354532241821\n",
      "[Training Epoch 0] Batch 2685, Loss 0.5059241652488708\n",
      "[Training Epoch 0] Batch 2686, Loss 0.50934898853302\n",
      "[Training Epoch 0] Batch 2687, Loss 0.49239426851272583\n",
      "[Training Epoch 0] Batch 2688, Loss 0.497593492269516\n",
      "[Training Epoch 0] Batch 2689, Loss 0.5081492066383362\n",
      "[Training Epoch 0] Batch 2690, Loss 0.49501872062683105\n",
      "[Training Epoch 0] Batch 2691, Loss 0.4706454873085022\n",
      "[Training Epoch 0] Batch 2692, Loss 0.4925694167613983\n",
      "[Training Epoch 0] Batch 2693, Loss 0.5326791405677795\n",
      "[Training Epoch 0] Batch 2694, Loss 0.4645366668701172\n",
      "[Training Epoch 0] Batch 2695, Loss 0.5019747018814087\n",
      "[Training Epoch 0] Batch 2696, Loss 0.5105130672454834\n",
      "[Training Epoch 0] Batch 2697, Loss 0.4998169243335724\n",
      "[Training Epoch 0] Batch 2698, Loss 0.49229782819747925\n",
      "[Training Epoch 0] Batch 2699, Loss 0.5253139734268188\n",
      "[Training Epoch 0] Batch 2700, Loss 0.5194876194000244\n",
      "[Training Epoch 0] Batch 2701, Loss 0.49852365255355835\n",
      "[Training Epoch 0] Batch 2702, Loss 0.49976956844329834\n",
      "[Training Epoch 0] Batch 2703, Loss 0.5282021760940552\n",
      "[Training Epoch 0] Batch 2704, Loss 0.4996030330657959\n",
      "[Training Epoch 0] Batch 2705, Loss 0.5068017244338989\n",
      "[Training Epoch 0] Batch 2706, Loss 0.49968066811561584\n",
      "[Training Epoch 0] Batch 2707, Loss 0.5180326700210571\n",
      "[Training Epoch 0] Batch 2708, Loss 0.4775354564189911\n",
      "[Training Epoch 0] Batch 2709, Loss 0.5220034718513489\n",
      "[Training Epoch 0] Batch 2710, Loss 0.49242323637008667\n",
      "[Training Epoch 0] Batch 2711, Loss 0.4873846769332886\n",
      "[Training Epoch 0] Batch 2712, Loss 0.5375899076461792\n",
      "[Training Epoch 0] Batch 2713, Loss 0.5167996883392334\n",
      "[Training Epoch 0] Batch 2714, Loss 0.49873432517051697\n",
      "[Training Epoch 0] Batch 2715, Loss 0.4861723482608795\n",
      "[Training Epoch 0] Batch 2716, Loss 0.4838096499443054\n",
      "[Training Epoch 0] Batch 2717, Loss 0.5425263047218323\n",
      "[Training Epoch 0] Batch 2718, Loss 0.4973070025444031\n",
      "[Training Epoch 0] Batch 2719, Loss 0.4996596872806549\n",
      "[Training Epoch 0] Batch 2720, Loss 0.504615306854248\n",
      "[Training Epoch 0] Batch 2721, Loss 0.49082261323928833\n",
      "[Training Epoch 0] Batch 2722, Loss 0.5083634853363037\n",
      "[Training Epoch 0] Batch 2723, Loss 0.4654187560081482\n",
      "[Training Epoch 0] Batch 2724, Loss 0.5131068229675293\n",
      "[Training Epoch 0] Batch 2725, Loss 0.504554808139801\n",
      "[Training Epoch 0] Batch 2726, Loss 0.5118785500526428\n",
      "[Training Epoch 0] Batch 2727, Loss 0.4945949614048004\n",
      "[Training Epoch 0] Batch 2728, Loss 0.49575451016426086\n",
      "[Training Epoch 0] Batch 2729, Loss 0.49476534128189087\n",
      "[Training Epoch 0] Batch 2730, Loss 0.5107501745223999\n",
      "[Training Epoch 0] Batch 2731, Loss 0.5093099474906921\n",
      "[Training Epoch 0] Batch 2732, Loss 0.4848315715789795\n",
      "[Training Epoch 0] Batch 2733, Loss 0.4840273857116699\n",
      "[Training Epoch 0] Batch 2734, Loss 0.4935043752193451\n",
      "[Training Epoch 0] Batch 2735, Loss 0.5143263339996338\n",
      "[Training Epoch 0] Batch 2736, Loss 0.5180644989013672\n",
      "[Training Epoch 0] Batch 2737, Loss 0.4960072338581085\n",
      "[Training Epoch 0] Batch 2738, Loss 0.4822061359882355\n",
      "[Training Epoch 0] Batch 2739, Loss 0.489793062210083\n",
      "[Training Epoch 0] Batch 2740, Loss 0.49577605724334717\n",
      "[Training Epoch 0] Batch 2741, Loss 0.49346259236335754\n",
      "[Training Epoch 0] Batch 2742, Loss 0.4907470643520355\n",
      "[Training Epoch 0] Batch 2743, Loss 0.5055209398269653\n",
      "[Training Epoch 0] Batch 2744, Loss 0.4994799494743347\n",
      "[Training Epoch 0] Batch 2745, Loss 0.5018841028213501\n",
      "[Training Epoch 0] Batch 2746, Loss 0.5007239580154419\n",
      "[Training Epoch 0] Batch 2747, Loss 0.5252375602722168\n",
      "[Training Epoch 0] Batch 2748, Loss 0.5376468896865845\n",
      "[Training Epoch 0] Batch 2749, Loss 0.5117930769920349\n",
      "[Training Epoch 0] Batch 2750, Loss 0.4919378161430359\n",
      "[Training Epoch 0] Batch 2751, Loss 0.4935669004917145\n",
      "[Training Epoch 0] Batch 2752, Loss 0.5190423130989075\n",
      "[Training Epoch 0] Batch 2753, Loss 0.4787970781326294\n",
      "[Training Epoch 0] Batch 2754, Loss 0.5079697370529175\n",
      "[Training Epoch 0] Batch 2755, Loss 0.5009164810180664\n",
      "[Training Epoch 0] Batch 2756, Loss 0.5119143128395081\n",
      "[Training Epoch 0] Batch 2757, Loss 0.5056059956550598\n",
      "[Training Epoch 0] Batch 2758, Loss 0.5251315236091614\n",
      "[Training Epoch 0] Batch 2759, Loss 0.5021420121192932\n",
      "[Training Epoch 0] Batch 2760, Loss 0.49336451292037964\n",
      "[Training Epoch 0] Batch 2761, Loss 0.4983392059803009\n",
      "[Training Epoch 0] Batch 2762, Loss 0.4944619834423065\n",
      "[Training Epoch 0] Batch 2763, Loss 0.514089047908783\n",
      "[Training Epoch 0] Batch 2764, Loss 0.4856613874435425\n",
      "[Training Epoch 0] Batch 2765, Loss 0.4833575189113617\n",
      "[Training Epoch 0] Batch 2766, Loss 0.5007290840148926\n",
      "[Training Epoch 0] Batch 2767, Loss 0.47741448879241943\n",
      "[Training Epoch 0] Batch 2768, Loss 0.5004910826683044\n",
      "[Training Epoch 0] Batch 2769, Loss 0.48223310708999634\n",
      "[Training Epoch 0] Batch 2770, Loss 0.5305496454238892\n",
      "[Training Epoch 0] Batch 2771, Loss 0.4955497980117798\n",
      "[Training Epoch 0] Batch 2772, Loss 0.5276228785514832\n",
      "[Training Epoch 0] Batch 2773, Loss 0.502066433429718\n",
      "[Training Epoch 0] Batch 2774, Loss 0.5251945853233337\n",
      "[Training Epoch 0] Batch 2775, Loss 0.49204346537590027\n",
      "[Training Epoch 0] Batch 2776, Loss 0.477198988199234\n",
      "[Training Epoch 0] Batch 2777, Loss 0.5008215308189392\n",
      "[Training Epoch 0] Batch 2778, Loss 0.4720243811607361\n",
      "[Training Epoch 0] Batch 2779, Loss 0.5118228793144226\n",
      "[Training Epoch 0] Batch 2780, Loss 0.49696072936058044\n",
      "[Training Epoch 0] Batch 2781, Loss 0.5179906487464905\n",
      "[Training Epoch 0] Batch 2782, Loss 0.5243375301361084\n",
      "[Training Epoch 0] Batch 2783, Loss 0.48565223813056946\n",
      "[Training Epoch 0] Batch 2784, Loss 0.5068137645721436\n",
      "[Training Epoch 0] Batch 2785, Loss 0.5105132460594177\n",
      "[Training Epoch 0] Batch 2786, Loss 0.4956424832344055\n",
      "[Training Epoch 0] Batch 2787, Loss 0.49668553471565247\n",
      "[Training Epoch 0] Batch 2788, Loss 0.5255933403968811\n",
      "[Training Epoch 0] Batch 2789, Loss 0.49819937348365784\n",
      "[Training Epoch 0] Batch 2790, Loss 0.518968939781189\n",
      "[Training Epoch 0] Batch 2791, Loss 0.4879108667373657\n",
      "[Training Epoch 0] Batch 2792, Loss 0.48301875591278076\n",
      "[Training Epoch 0] Batch 2793, Loss 0.5104312896728516\n",
      "[Training Epoch 0] Batch 2794, Loss 0.4968249499797821\n",
      "[Training Epoch 0] Batch 2795, Loss 0.4620745778083801\n",
      "[Training Epoch 0] Batch 2796, Loss 0.4671053886413574\n",
      "[Training Epoch 0] Batch 2797, Loss 0.5079057812690735\n",
      "[Training Epoch 0] Batch 2798, Loss 0.507929265499115\n",
      "[Training Epoch 0] Batch 2799, Loss 0.5077012777328491\n",
      "[Training Epoch 0] Batch 2800, Loss 0.4954192042350769\n",
      "[Training Epoch 0] Batch 2801, Loss 0.491913378238678\n",
      "[Training Epoch 0] Batch 2802, Loss 0.5042048096656799\n",
      "[Training Epoch 0] Batch 2803, Loss 0.49561333656311035\n",
      "[Training Epoch 0] Batch 2804, Loss 0.5240671634674072\n",
      "[Training Epoch 0] Batch 2805, Loss 0.5090950727462769\n",
      "[Training Epoch 0] Batch 2806, Loss 0.5017093420028687\n",
      "[Training Epoch 0] Batch 2807, Loss 0.5103219747543335\n",
      "[Training Epoch 0] Batch 2808, Loss 0.5091152787208557\n",
      "[Training Epoch 0] Batch 2809, Loss 0.4931950867176056\n",
      "[Training Epoch 0] Batch 2810, Loss 0.5003188848495483\n",
      "[Training Epoch 0] Batch 2811, Loss 0.4769253134727478\n",
      "[Training Epoch 0] Batch 2812, Loss 0.49328017234802246\n",
      "[Training Epoch 0] Batch 2813, Loss 0.4994969964027405\n",
      "[Training Epoch 0] Batch 2814, Loss 0.4992610216140747\n",
      "[Training Epoch 0] Batch 2815, Loss 0.5030259490013123\n",
      "[Training Epoch 0] Batch 2816, Loss 0.47039440274238586\n",
      "[Training Epoch 0] Batch 2817, Loss 0.48912152647972107\n",
      "[Training Epoch 0] Batch 2818, Loss 0.481776624917984\n",
      "[Training Epoch 0] Batch 2819, Loss 0.4996124804019928\n",
      "[Training Epoch 0] Batch 2820, Loss 0.519654393196106\n",
      "[Training Epoch 0] Batch 2821, Loss 0.4879518151283264\n",
      "[Training Epoch 0] Batch 2822, Loss 0.5064715147018433\n",
      "[Training Epoch 0] Batch 2823, Loss 0.5164033770561218\n",
      "[Training Epoch 0] Batch 2824, Loss 0.4870508909225464\n",
      "[Training Epoch 0] Batch 2825, Loss 0.5031439661979675\n",
      "[Training Epoch 0] Batch 2826, Loss 0.4866894781589508\n",
      "[Training Epoch 0] Batch 2827, Loss 0.5062528848648071\n",
      "[Training Epoch 0] Batch 2828, Loss 0.4681822955608368\n",
      "[Training Epoch 0] Batch 2829, Loss 0.5168255567550659\n",
      "[Training Epoch 0] Batch 2830, Loss 0.5208473205566406\n",
      "[Training Epoch 0] Batch 2831, Loss 0.47053325176239014\n",
      "[Training Epoch 0] Batch 2832, Loss 0.4931497871875763\n",
      "[Training Epoch 0] Batch 2833, Loss 0.4828294813632965\n",
      "[Training Epoch 0] Batch 2834, Loss 0.4794337749481201\n",
      "[Training Epoch 0] Batch 2835, Loss 0.5004081726074219\n",
      "[Training Epoch 0] Batch 2836, Loss 0.48774051666259766\n",
      "[Training Epoch 0] Batch 2837, Loss 0.4890872538089752\n",
      "[Training Epoch 0] Batch 2838, Loss 0.49662959575653076\n",
      "[Training Epoch 0] Batch 2839, Loss 0.4965859055519104\n",
      "[Training Epoch 0] Batch 2840, Loss 0.5253927707672119\n",
      "[Training Epoch 0] Batch 2841, Loss 0.5038381814956665\n",
      "[Training Epoch 0] Batch 2842, Loss 0.5091457366943359\n",
      "[Training Epoch 0] Batch 2843, Loss 0.5056155920028687\n",
      "[Training Epoch 0] Batch 2844, Loss 0.5128896236419678\n",
      "[Training Epoch 0] Batch 2845, Loss 0.5028615593910217\n",
      "[Training Epoch 0] Batch 2846, Loss 0.4679475724697113\n",
      "[Training Epoch 0] Batch 2847, Loss 0.5340853929519653\n",
      "[Training Epoch 0] Batch 2848, Loss 0.5217013359069824\n",
      "[Training Epoch 0] Batch 2849, Loss 0.4942863881587982\n",
      "[Training Epoch 0] Batch 2850, Loss 0.5227569937705994\n",
      "[Training Epoch 0] Batch 2851, Loss 0.4752827286720276\n",
      "[Training Epoch 0] Batch 2852, Loss 0.5202398300170898\n",
      "[Training Epoch 0] Batch 2853, Loss 0.5391141176223755\n",
      "[Training Epoch 0] Batch 2854, Loss 0.49168914556503296\n",
      "[Training Epoch 0] Batch 2855, Loss 0.5163426399230957\n",
      "[Training Epoch 0] Batch 2856, Loss 0.5140732526779175\n",
      "[Training Epoch 0] Batch 2857, Loss 0.5001754760742188\n",
      "[Training Epoch 0] Batch 2858, Loss 0.5153167843818665\n",
      "[Training Epoch 0] Batch 2859, Loss 0.47512882947921753\n",
      "[Training Epoch 0] Batch 2860, Loss 0.5292084217071533\n",
      "[Training Epoch 0] Batch 2861, Loss 0.5093568563461304\n",
      "[Training Epoch 0] Batch 2862, Loss 0.5015426874160767\n",
      "[Training Epoch 0] Batch 2863, Loss 0.5239928960800171\n",
      "[Training Epoch 0] Batch 2864, Loss 0.51397705078125\n",
      "[Training Epoch 0] Batch 2865, Loss 0.4916662573814392\n",
      "[Training Epoch 0] Batch 2866, Loss 0.5317631363868713\n",
      "[Training Epoch 0] Batch 2867, Loss 0.4878576993942261\n",
      "[Training Epoch 0] Batch 2868, Loss 0.5276614427566528\n",
      "[Training Epoch 0] Batch 2869, Loss 0.5341492295265198\n",
      "[Training Epoch 0] Batch 2870, Loss 0.4864959418773651\n",
      "[Training Epoch 0] Batch 2871, Loss 0.4727560877799988\n",
      "[Training Epoch 0] Batch 2872, Loss 0.5213446021080017\n",
      "[Training Epoch 0] Batch 2873, Loss 0.5139395594596863\n",
      "[Training Epoch 0] Batch 2874, Loss 0.4926897883415222\n",
      "[Training Epoch 0] Batch 2875, Loss 0.4789910912513733\n",
      "[Training Epoch 0] Batch 2876, Loss 0.5116405487060547\n",
      "[Training Epoch 0] Batch 2877, Loss 0.5204173922538757\n",
      "[Training Epoch 0] Batch 2878, Loss 0.5000128149986267\n",
      "[Training Epoch 0] Batch 2879, Loss 0.5217914581298828\n",
      "[Training Epoch 0] Batch 2880, Loss 0.48776811361312866\n",
      "[Training Epoch 0] Batch 2881, Loss 0.5066633224487305\n",
      "[Training Epoch 0] Batch 2882, Loss 0.5176993608474731\n",
      "[Training Epoch 0] Batch 2883, Loss 0.5217828750610352\n",
      "[Training Epoch 0] Batch 2884, Loss 0.5115917325019836\n",
      "[Training Epoch 0] Batch 2885, Loss 0.5414415001869202\n",
      "[Training Epoch 0] Batch 2886, Loss 0.48680442571640015\n",
      "[Training Epoch 0] Batch 2887, Loss 0.5080357193946838\n",
      "[Training Epoch 0] Batch 2888, Loss 0.49043312668800354\n",
      "[Training Epoch 0] Batch 2889, Loss 0.5074320435523987\n",
      "[Training Epoch 0] Batch 2890, Loss 0.5066091418266296\n",
      "[Training Epoch 0] Batch 2891, Loss 0.48516562581062317\n",
      "[Training Epoch 0] Batch 2892, Loss 0.48763442039489746\n",
      "[Training Epoch 0] Batch 2893, Loss 0.48786109685897827\n",
      "[Training Epoch 0] Batch 2894, Loss 0.5126573443412781\n",
      "[Training Epoch 0] Batch 2895, Loss 0.47516199946403503\n",
      "[Training Epoch 0] Batch 2896, Loss 0.4928608238697052\n",
      "[Training Epoch 0] Batch 2897, Loss 0.5201514959335327\n",
      "[Training Epoch 0] Batch 2898, Loss 0.5003870725631714\n",
      "[Training Epoch 0] Batch 2899, Loss 0.5187764167785645\n",
      "[Training Epoch 0] Batch 2900, Loss 0.5268067717552185\n",
      "[Training Epoch 0] Batch 2901, Loss 0.5003221035003662\n",
      "[Training Epoch 0] Batch 2902, Loss 0.464806467294693\n",
      "[Training Epoch 0] Batch 2903, Loss 0.5065982937812805\n",
      "[Training Epoch 0] Batch 2904, Loss 0.4890053868293762\n",
      "[Training Epoch 0] Batch 2905, Loss 0.5118100047111511\n",
      "[Training Epoch 0] Batch 2906, Loss 0.4901069104671478\n",
      "[Training Epoch 0] Batch 2907, Loss 0.4850502610206604\n",
      "[Training Epoch 0] Batch 2908, Loss 0.4873146414756775\n",
      "[Training Epoch 0] Batch 2909, Loss 0.5125683546066284\n",
      "[Training Epoch 0] Batch 2910, Loss 0.4911935329437256\n",
      "[Training Epoch 0] Batch 2911, Loss 0.4863458573818207\n",
      "[Training Epoch 0] Batch 2912, Loss 0.5091332793235779\n",
      "[Training Epoch 0] Batch 2913, Loss 0.48869776725769043\n",
      "[Training Epoch 0] Batch 2914, Loss 0.4889151453971863\n",
      "[Training Epoch 0] Batch 2915, Loss 0.4964258670806885\n",
      "[Training Epoch 0] Batch 2916, Loss 0.5114589929580688\n",
      "[Training Epoch 0] Batch 2917, Loss 0.49917036294937134\n",
      "[Training Epoch 0] Batch 2918, Loss 0.4698815941810608\n",
      "[Training Epoch 0] Batch 2919, Loss 0.5112640857696533\n",
      "[Training Epoch 0] Batch 2920, Loss 0.4860536456108093\n",
      "[Training Epoch 0] Batch 2921, Loss 0.4962674379348755\n",
      "[Training Epoch 0] Batch 2922, Loss 0.5203609466552734\n",
      "[Training Epoch 0] Batch 2923, Loss 0.4998551309108734\n",
      "[Training Epoch 0] Batch 2924, Loss 0.47104716300964355\n",
      "[Training Epoch 0] Batch 2925, Loss 0.5027632117271423\n",
      "[Training Epoch 0] Batch 2926, Loss 0.5048184394836426\n",
      "[Training Epoch 0] Batch 2927, Loss 0.5112778544425964\n",
      "[Training Epoch 0] Batch 2928, Loss 0.5223811864852905\n",
      "[Training Epoch 0] Batch 2929, Loss 0.50749272108078\n",
      "[Training Epoch 0] Batch 2930, Loss 0.4913284182548523\n",
      "[Training Epoch 0] Batch 2931, Loss 0.4938881993293762\n",
      "[Training Epoch 0] Batch 2932, Loss 0.47879159450531006\n",
      "[Training Epoch 0] Batch 2933, Loss 0.5239991545677185\n",
      "[Training Epoch 0] Batch 2934, Loss 0.5315698981285095\n",
      "[Training Epoch 0] Batch 2935, Loss 0.4999503493309021\n",
      "[Training Epoch 0] Batch 2936, Loss 0.4947276711463928\n",
      "[Training Epoch 0] Batch 2937, Loss 0.5415729880332947\n",
      "[Training Epoch 0] Batch 2938, Loss 0.5178913474082947\n",
      "[Training Epoch 0] Batch 2939, Loss 0.48918962478637695\n",
      "[Training Epoch 0] Batch 2940, Loss 0.5202968120574951\n",
      "[Training Epoch 0] Batch 2941, Loss 0.5210684537887573\n",
      "[Training Epoch 0] Batch 2942, Loss 0.5031217932701111\n",
      "[Training Epoch 0] Batch 2943, Loss 0.4619176685810089\n",
      "[Training Epoch 0] Batch 2944, Loss 0.4692356586456299\n",
      "[Training Epoch 0] Batch 2945, Loss 0.48853200674057007\n",
      "[Training Epoch 0] Batch 2946, Loss 0.49653860926628113\n",
      "[Training Epoch 0] Batch 2947, Loss 0.5105545520782471\n",
      "[Training Epoch 0] Batch 2948, Loss 0.5207764506340027\n",
      "[Training Epoch 0] Batch 2949, Loss 0.4999590814113617\n",
      "[Training Epoch 0] Batch 2950, Loss 0.5031678080558777\n",
      "[Training Epoch 0] Batch 2951, Loss 0.502981960773468\n",
      "[Training Epoch 0] Batch 2952, Loss 0.5135526657104492\n",
      "[Training Epoch 0] Batch 2953, Loss 0.49994346499443054\n",
      "[Training Epoch 0] Batch 2954, Loss 0.4960690140724182\n",
      "[Training Epoch 0] Batch 2955, Loss 0.4800249934196472\n",
      "[Training Epoch 0] Batch 2956, Loss 0.5040928721427917\n",
      "[Training Epoch 0] Batch 2957, Loss 0.5454603433609009\n",
      "[Training Epoch 0] Batch 2958, Loss 0.5012366771697998\n",
      "[Training Epoch 0] Batch 2959, Loss 0.5051522850990295\n",
      "[Training Epoch 0] Batch 2960, Loss 0.5111863017082214\n",
      "[Training Epoch 0] Batch 2961, Loss 0.5116310119628906\n",
      "[Training Epoch 0] Batch 2962, Loss 0.5114103555679321\n",
      "[Training Epoch 0] Batch 2963, Loss 0.4889381229877472\n",
      "[Training Epoch 0] Batch 2964, Loss 0.5039222240447998\n",
      "[Training Epoch 0] Batch 2965, Loss 0.5202325582504272\n",
      "[Training Epoch 0] Batch 2966, Loss 0.5213565826416016\n",
      "[Training Epoch 0] Batch 2967, Loss 0.4849453568458557\n",
      "[Training Epoch 0] Batch 2968, Loss 0.5164234042167664\n",
      "[Training Epoch 0] Batch 2969, Loss 0.5104297399520874\n",
      "[Training Epoch 0] Batch 2970, Loss 0.49973759055137634\n",
      "[Training Epoch 0] Batch 2971, Loss 0.5089371204376221\n",
      "[Training Epoch 0] Batch 2972, Loss 0.48851898312568665\n",
      "[Training Epoch 0] Batch 2973, Loss 0.4783514142036438\n",
      "[Training Epoch 0] Batch 2974, Loss 0.4986927807331085\n",
      "[Training Epoch 0] Batch 2975, Loss 0.5176231861114502\n",
      "[Training Epoch 0] Batch 2976, Loss 0.47476860880851746\n",
      "[Training Epoch 0] Batch 2977, Loss 0.521508514881134\n",
      "[Training Epoch 0] Batch 2978, Loss 0.5317014455795288\n",
      "[Training Epoch 0] Batch 2979, Loss 0.4937044382095337\n",
      "[Training Epoch 0] Batch 2980, Loss 0.4961620569229126\n",
      "[Training Epoch 0] Batch 2981, Loss 0.5010864734649658\n",
      "[Training Epoch 0] Batch 2982, Loss 0.4909968972206116\n",
      "[Training Epoch 0] Batch 2983, Loss 0.4934536814689636\n",
      "[Training Epoch 0] Batch 2984, Loss 0.4836597442626953\n",
      "[Training Epoch 0] Batch 2985, Loss 0.48871666193008423\n",
      "[Training Epoch 0] Batch 2986, Loss 0.49018457531929016\n",
      "[Training Epoch 0] Batch 2987, Loss 0.5162866115570068\n",
      "[Training Epoch 0] Batch 2988, Loss 0.5060394406318665\n",
      "[Training Epoch 0] Batch 2989, Loss 0.5316530466079712\n",
      "[Training Epoch 0] Batch 2990, Loss 0.4986305236816406\n",
      "[Training Epoch 0] Batch 2991, Loss 0.5139971971511841\n",
      "[Training Epoch 0] Batch 2992, Loss 0.5117532014846802\n",
      "[Training Epoch 0] Batch 2993, Loss 0.49239760637283325\n",
      "[Training Epoch 0] Batch 2994, Loss 0.4761771857738495\n",
      "[Training Epoch 0] Batch 2995, Loss 0.48611509799957275\n",
      "[Training Epoch 0] Batch 2996, Loss 0.4884791672229767\n",
      "[Training Epoch 0] Batch 2997, Loss 0.5027827024459839\n",
      "[Training Epoch 0] Batch 2998, Loss 0.5043137669563293\n",
      "[Training Epoch 0] Batch 2999, Loss 0.4805144667625427\n",
      "[Training Epoch 0] Batch 3000, Loss 0.5366446375846863\n",
      "[Training Epoch 0] Batch 3001, Loss 0.5186100006103516\n",
      "[Training Epoch 0] Batch 3002, Loss 0.5038536190986633\n",
      "[Training Epoch 0] Batch 3003, Loss 0.5114446878433228\n",
      "[Training Epoch 0] Batch 3004, Loss 0.477425217628479\n",
      "[Training Epoch 0] Batch 3005, Loss 0.4794807434082031\n",
      "[Training Epoch 0] Batch 3006, Loss 0.497589647769928\n",
      "[Training Epoch 0] Batch 3007, Loss 0.4683622717857361\n",
      "[Training Epoch 0] Batch 3008, Loss 0.497660756111145\n",
      "[Training Epoch 0] Batch 3009, Loss 0.4746168851852417\n",
      "[Training Epoch 0] Batch 3010, Loss 0.49570131301879883\n",
      "[Training Epoch 0] Batch 3011, Loss 0.517463743686676\n",
      "[Training Epoch 0] Batch 3012, Loss 0.5269101858139038\n",
      "[Training Epoch 0] Batch 3013, Loss 0.5160417556762695\n",
      "[Training Epoch 0] Batch 3014, Loss 0.5252701044082642\n",
      "[Training Epoch 0] Batch 3015, Loss 0.5126587152481079\n",
      "[Training Epoch 0] Batch 3016, Loss 0.4947703182697296\n",
      "[Training Epoch 0] Batch 3017, Loss 0.501144528388977\n",
      "[Training Epoch 0] Batch 3018, Loss 0.5011170506477356\n",
      "[Training Epoch 0] Batch 3019, Loss 0.49449726939201355\n",
      "[Training Epoch 0] Batch 3020, Loss 0.49123477935791016\n",
      "[Training Epoch 0] Batch 3021, Loss 0.4862620234489441\n",
      "[Training Epoch 0] Batch 3022, Loss 0.5318218469619751\n",
      "[Training Epoch 0] Batch 3023, Loss 0.5178160071372986\n",
      "[Training Epoch 0] Batch 3024, Loss 0.49443668127059937\n",
      "[Training Epoch 0] Batch 3025, Loss 0.47334378957748413\n",
      "[Training Epoch 0] Batch 3026, Loss 0.4976975619792938\n",
      "[Training Epoch 0] Batch 3027, Loss 0.49877703189849854\n",
      "[Training Epoch 0] Batch 3028, Loss 0.5090713500976562\n",
      "[Training Epoch 0] Batch 3029, Loss 0.4937179684638977\n",
      "[Training Epoch 0] Batch 3030, Loss 0.5306535959243774\n",
      "[Training Epoch 0] Batch 3031, Loss 0.524146318435669\n",
      "[Training Epoch 0] Batch 3032, Loss 0.5063560009002686\n",
      "[Training Epoch 0] Batch 3033, Loss 0.508737325668335\n",
      "[Training Epoch 0] Batch 3034, Loss 0.5059728622436523\n",
      "[Training Epoch 0] Batch 3035, Loss 0.48475727438926697\n",
      "[Training Epoch 0] Batch 3036, Loss 0.5395970344543457\n",
      "[Training Epoch 0] Batch 3037, Loss 0.4962618350982666\n",
      "[Training Epoch 0] Batch 3038, Loss 0.4893798828125\n",
      "[Training Epoch 0] Batch 3039, Loss 0.5001450181007385\n",
      "[Training Epoch 0] Batch 3040, Loss 0.4844735264778137\n",
      "[Training Epoch 0] Batch 3041, Loss 0.5397278070449829\n",
      "[Training Epoch 0] Batch 3042, Loss 0.5023492574691772\n",
      "[Training Epoch 0] Batch 3043, Loss 0.5192351341247559\n",
      "[Training Epoch 0] Batch 3044, Loss 0.527892529964447\n",
      "[Training Epoch 0] Batch 3045, Loss 0.51160728931427\n",
      "[Training Epoch 0] Batch 3046, Loss 0.49759986996650696\n",
      "[Training Epoch 0] Batch 3047, Loss 0.5333580374717712\n",
      "[Training Epoch 0] Batch 3048, Loss 0.5126241445541382\n",
      "[Training Epoch 0] Batch 3049, Loss 0.4923372268676758\n",
      "[Training Epoch 0] Batch 3050, Loss 0.5009949803352356\n",
      "[Training Epoch 0] Batch 3051, Loss 0.5077497363090515\n",
      "[Training Epoch 0] Batch 3052, Loss 0.5013135075569153\n",
      "[Training Epoch 0] Batch 3053, Loss 0.5219059586524963\n",
      "[Training Epoch 0] Batch 3054, Loss 0.4998493492603302\n",
      "[Training Epoch 0] Batch 3055, Loss 0.49699005484580994\n",
      "[Training Epoch 0] Batch 3056, Loss 0.49477100372314453\n",
      "[Training Epoch 0] Batch 3057, Loss 0.5052120685577393\n",
      "[Training Epoch 0] Batch 3058, Loss 0.4734833240509033\n",
      "[Training Epoch 0] Batch 3059, Loss 0.5113195180892944\n",
      "[Training Epoch 0] Batch 3060, Loss 0.5049753785133362\n",
      "[Training Epoch 0] Batch 3061, Loss 0.49468013644218445\n",
      "[Training Epoch 0] Batch 3062, Loss 0.5084750056266785\n",
      "[Training Epoch 0] Batch 3063, Loss 0.49872446060180664\n",
      "[Training Epoch 0] Batch 3064, Loss 0.5355546474456787\n",
      "[Training Epoch 0] Batch 3065, Loss 0.4793652892112732\n",
      "[Training Epoch 0] Batch 3066, Loss 0.5408210754394531\n",
      "[Training Epoch 0] Batch 3067, Loss 0.49063584208488464\n",
      "[Training Epoch 0] Batch 3068, Loss 0.49494901299476624\n",
      "[Training Epoch 0] Batch 3069, Loss 0.523051381111145\n",
      "[Training Epoch 0] Batch 3070, Loss 0.5072089433670044\n",
      "[Training Epoch 0] Batch 3071, Loss 0.5001735091209412\n",
      "[Training Epoch 0] Batch 3072, Loss 0.49985024333000183\n",
      "[Training Epoch 0] Batch 3073, Loss 0.4795067310333252\n",
      "[Training Epoch 0] Batch 3074, Loss 0.5216076374053955\n",
      "[Training Epoch 0] Batch 3075, Loss 0.48332107067108154\n",
      "[Training Epoch 0] Batch 3076, Loss 0.45490533113479614\n",
      "[Training Epoch 0] Batch 3077, Loss 0.4767882823944092\n",
      "[Training Epoch 0] Batch 3078, Loss 0.5012123584747314\n",
      "[Training Epoch 0] Batch 3079, Loss 0.4934554994106293\n",
      "[Training Epoch 0] Batch 3080, Loss 0.5165348649024963\n",
      "[Training Epoch 0] Batch 3081, Loss 0.46891337633132935\n",
      "[Training Epoch 0] Batch 3082, Loss 0.4883626699447632\n",
      "[Training Epoch 0] Batch 3083, Loss 0.5253586769104004\n",
      "[Training Epoch 0] Batch 3084, Loss 0.47046545147895813\n",
      "[Training Epoch 0] Batch 3085, Loss 0.4871373474597931\n",
      "[Training Epoch 0] Batch 3086, Loss 0.4921148717403412\n",
      "[Training Epoch 0] Batch 3087, Loss 0.49098479747772217\n",
      "[Training Epoch 0] Batch 3088, Loss 0.4959641098976135\n",
      "[Training Epoch 0] Batch 3089, Loss 0.5010256171226501\n",
      "[Training Epoch 0] Batch 3090, Loss 0.5279582142829895\n",
      "[Training Epoch 0] Batch 3091, Loss 0.48848941922187805\n",
      "[Training Epoch 0] Batch 3092, Loss 0.4688885807991028\n",
      "[Training Epoch 0] Batch 3093, Loss 0.4806749224662781\n",
      "[Training Epoch 0] Batch 3094, Loss 0.5241039991378784\n",
      "[Training Epoch 0] Batch 3095, Loss 0.49465519189834595\n",
      "[Training Epoch 0] Batch 3096, Loss 0.48324036598205566\n",
      "[Training Epoch 0] Batch 3097, Loss 0.4988257586956024\n",
      "[Training Epoch 0] Batch 3098, Loss 0.4806416630744934\n",
      "[Training Epoch 0] Batch 3099, Loss 0.46158909797668457\n",
      "[Training Epoch 0] Batch 3100, Loss 0.5103226900100708\n",
      "[Training Epoch 0] Batch 3101, Loss 0.4896585941314697\n",
      "[Training Epoch 0] Batch 3102, Loss 0.49855145812034607\n",
      "[Training Epoch 0] Batch 3103, Loss 0.5023250579833984\n",
      "[Training Epoch 0] Batch 3104, Loss 0.4470844566822052\n",
      "[Training Epoch 0] Batch 3105, Loss 0.4998382329940796\n",
      "[Training Epoch 0] Batch 3106, Loss 0.49181652069091797\n",
      "[Training Epoch 0] Batch 3107, Loss 0.506276547908783\n",
      "[Training Epoch 0] Batch 3108, Loss 0.5088145732879639\n",
      "[Training Epoch 0] Batch 3109, Loss 0.49849623441696167\n",
      "[Training Epoch 0] Batch 3110, Loss 0.4906103312969208\n",
      "[Training Epoch 0] Batch 3111, Loss 0.506234884262085\n",
      "[Training Epoch 0] Batch 3112, Loss 0.4858091175556183\n",
      "[Training Epoch 0] Batch 3113, Loss 0.5091555118560791\n",
      "[Training Epoch 0] Batch 3114, Loss 0.49225401878356934\n",
      "[Training Epoch 0] Batch 3115, Loss 0.4946383833885193\n",
      "[Training Epoch 0] Batch 3116, Loss 0.5010439157485962\n",
      "[Training Epoch 0] Batch 3117, Loss 0.5035527944564819\n",
      "[Training Epoch 0] Batch 3118, Loss 0.47396281361579895\n",
      "[Training Epoch 0] Batch 3119, Loss 0.49339383840560913\n",
      "[Training Epoch 0] Batch 3120, Loss 0.4649791121482849\n",
      "[Training Epoch 0] Batch 3121, Loss 0.4572218656539917\n",
      "[Training Epoch 0] Batch 3122, Loss 0.5307055711746216\n",
      "[Training Epoch 0] Batch 3123, Loss 0.5191664099693298\n",
      "[Training Epoch 0] Batch 3124, Loss 0.4946310222148895\n",
      "[Training Epoch 0] Batch 3125, Loss 0.5162317752838135\n",
      "[Training Epoch 0] Batch 3126, Loss 0.48818814754486084\n",
      "[Training Epoch 0] Batch 3127, Loss 0.49604296684265137\n",
      "[Training Epoch 0] Batch 3128, Loss 0.4662836492061615\n",
      "[Training Epoch 0] Batch 3129, Loss 0.4997221827507019\n",
      "[Training Epoch 0] Batch 3130, Loss 0.4816759526729584\n",
      "[Training Epoch 0] Batch 3131, Loss 0.521641194820404\n",
      "[Training Epoch 0] Batch 3132, Loss 0.507483720779419\n",
      "[Training Epoch 0] Batch 3133, Loss 0.4713195264339447\n",
      "[Training Epoch 0] Batch 3134, Loss 0.5087860822677612\n",
      "[Training Epoch 0] Batch 3135, Loss 0.49078115820884705\n",
      "[Training Epoch 0] Batch 3136, Loss 0.47905170917510986\n",
      "[Training Epoch 0] Batch 3137, Loss 0.5230425000190735\n",
      "[Training Epoch 0] Batch 3138, Loss 0.4907575249671936\n",
      "[Training Epoch 0] Batch 3139, Loss 0.49843838810920715\n",
      "[Training Epoch 0] Batch 3140, Loss 0.48807933926582336\n",
      "[Training Epoch 0] Batch 3141, Loss 0.4958711266517639\n",
      "[Training Epoch 0] Batch 3142, Loss 0.4737897515296936\n",
      "[Training Epoch 0] Batch 3143, Loss 0.5036357641220093\n",
      "[Training Epoch 0] Batch 3144, Loss 0.5089305639266968\n",
      "[Training Epoch 0] Batch 3145, Loss 0.45602768659591675\n",
      "[Training Epoch 0] Batch 3146, Loss 0.48416781425476074\n",
      "[Training Epoch 0] Batch 3147, Loss 0.5113955140113831\n",
      "[Training Epoch 0] Batch 3148, Loss 0.4815983176231384\n",
      "[Training Epoch 0] Batch 3149, Loss 0.4921020269393921\n",
      "[Training Epoch 0] Batch 3150, Loss 0.5257044434547424\n",
      "[Training Epoch 0] Batch 3151, Loss 0.47648483514785767\n",
      "[Training Epoch 0] Batch 3152, Loss 0.5151905417442322\n",
      "[Training Epoch 0] Batch 3153, Loss 0.46751582622528076\n",
      "[Training Epoch 0] Batch 3154, Loss 0.4982203245162964\n",
      "[Training Epoch 0] Batch 3155, Loss 0.49977412819862366\n",
      "[Training Epoch 0] Batch 3156, Loss 0.5176327228546143\n",
      "[Training Epoch 0] Batch 3157, Loss 0.5242528915405273\n",
      "[Training Epoch 0] Batch 3158, Loss 0.5461797118186951\n",
      "[Training Epoch 0] Batch 3159, Loss 0.521709144115448\n",
      "[Training Epoch 0] Batch 3160, Loss 0.4958130121231079\n",
      "[Training Epoch 0] Batch 3161, Loss 0.5166091322898865\n",
      "[Training Epoch 0] Batch 3162, Loss 0.5259168744087219\n",
      "[Training Epoch 0] Batch 3163, Loss 0.51639723777771\n",
      "[Training Epoch 0] Batch 3164, Loss 0.4973141849040985\n",
      "[Training Epoch 0] Batch 3165, Loss 0.5345088839530945\n",
      "[Training Epoch 0] Batch 3166, Loss 0.4882763624191284\n",
      "[Training Epoch 0] Batch 3167, Loss 0.5009447336196899\n",
      "[Training Epoch 0] Batch 3168, Loss 0.4881450831890106\n",
      "[Training Epoch 0] Batch 3169, Loss 0.49445047974586487\n",
      "[Training Epoch 0] Batch 3170, Loss 0.5023062229156494\n",
      "[Training Epoch 0] Batch 3171, Loss 0.48801010847091675\n",
      "[Training Epoch 0] Batch 3172, Loss 0.47247233986854553\n",
      "[Training Epoch 0] Batch 3173, Loss 0.498425155878067\n",
      "[Training Epoch 0] Batch 3174, Loss 0.5049047470092773\n",
      "[Training Epoch 0] Batch 3175, Loss 0.5073633193969727\n",
      "[Training Epoch 0] Batch 3176, Loss 0.5255464315414429\n",
      "[Training Epoch 0] Batch 3177, Loss 0.49848830699920654\n",
      "[Training Epoch 0] Batch 3178, Loss 0.472504198551178\n",
      "[Training Epoch 0] Batch 3179, Loss 0.5164259672164917\n",
      "[Training Epoch 0] Batch 3180, Loss 0.5074973106384277\n",
      "[Training Epoch 0] Batch 3181, Loss 0.4917566180229187\n",
      "[Training Epoch 0] Batch 3182, Loss 0.5203875303268433\n",
      "[Training Epoch 0] Batch 3183, Loss 0.49957165122032166\n",
      "[Training Epoch 0] Batch 3184, Loss 0.5035942196846008\n",
      "[Training Epoch 0] Batch 3185, Loss 0.4891711473464966\n",
      "[Training Epoch 0] Batch 3186, Loss 0.49839016795158386\n",
      "[Training Epoch 0] Batch 3187, Loss 0.5060467720031738\n",
      "[Training Epoch 0] Batch 3188, Loss 0.5164551138877869\n",
      "[Training Epoch 0] Batch 3189, Loss 0.46188458800315857\n",
      "[Training Epoch 0] Batch 3190, Loss 0.5190956592559814\n",
      "[Training Epoch 0] Batch 3191, Loss 0.4956337809562683\n",
      "[Training Epoch 0] Batch 3192, Loss 0.4930781126022339\n",
      "[Training Epoch 0] Batch 3193, Loss 0.48386457562446594\n",
      "[Training Epoch 0] Batch 3194, Loss 0.5007724761962891\n",
      "[Training Epoch 0] Batch 3195, Loss 0.4879245460033417\n",
      "[Training Epoch 0] Batch 3196, Loss 0.49975648522377014\n",
      "[Training Epoch 0] Batch 3197, Loss 0.4984934329986572\n",
      "[Training Epoch 0] Batch 3198, Loss 0.533244252204895\n",
      "[Training Epoch 0] Batch 3199, Loss 0.49958664178848267\n",
      "[Training Epoch 0] Batch 3200, Loss 0.5126217603683472\n",
      "[Training Epoch 0] Batch 3201, Loss 0.49932336807250977\n",
      "[Training Epoch 0] Batch 3202, Loss 0.4894087314605713\n",
      "[Training Epoch 0] Batch 3203, Loss 0.5165005326271057\n",
      "[Training Epoch 0] Batch 3204, Loss 0.5385804176330566\n",
      "[Training Epoch 0] Batch 3205, Loss 0.4984377324581146\n",
      "[Training Epoch 0] Batch 3206, Loss 0.5410839319229126\n",
      "[Training Epoch 0] Batch 3207, Loss 0.48127657175064087\n",
      "[Training Epoch 0] Batch 3208, Loss 0.5047993659973145\n",
      "[Training Epoch 0] Batch 3209, Loss 0.5114184021949768\n",
      "[Training Epoch 0] Batch 3210, Loss 0.5008268356323242\n",
      "[Training Epoch 0] Batch 3211, Loss 0.49179840087890625\n",
      "[Training Epoch 0] Batch 3212, Loss 0.4812556207180023\n",
      "[Training Epoch 0] Batch 3213, Loss 0.48892003297805786\n",
      "[Training Epoch 0] Batch 3214, Loss 0.5254809260368347\n",
      "[Training Epoch 0] Batch 3215, Loss 0.5162590742111206\n",
      "[Training Epoch 0] Batch 3216, Loss 0.5031521320343018\n",
      "[Training Epoch 0] Batch 3217, Loss 0.5033819079399109\n",
      "[Training Epoch 0] Batch 3218, Loss 0.5130630135536194\n",
      "[Training Epoch 0] Batch 3219, Loss 0.5140932202339172\n",
      "[Training Epoch 0] Batch 3220, Loss 0.5006417632102966\n",
      "[Training Epoch 0] Batch 3221, Loss 0.49738946557044983\n",
      "[Training Epoch 0] Batch 3222, Loss 0.4942847788333893\n",
      "[Training Epoch 0] Batch 3223, Loss 0.4853089153766632\n",
      "[Training Epoch 0] Batch 3224, Loss 0.4835735559463501\n",
      "[Training Epoch 0] Batch 3225, Loss 0.5271826386451721\n",
      "[Training Epoch 0] Batch 3226, Loss 0.4863300919532776\n",
      "[Training Epoch 0] Batch 3227, Loss 0.5148070454597473\n",
      "[Training Epoch 0] Batch 3228, Loss 0.47998225688934326\n",
      "[Training Epoch 0] Batch 3229, Loss 0.5058750510215759\n",
      "[Training Epoch 0] Batch 3230, Loss 0.4864589273929596\n",
      "[Training Epoch 0] Batch 3231, Loss 0.4854133129119873\n",
      "[Training Epoch 0] Batch 3232, Loss 0.49317964911460876\n",
      "[Training Epoch 0] Batch 3233, Loss 0.4812028408050537\n",
      "[Training Epoch 0] Batch 3234, Loss 0.4437534213066101\n",
      "[Training Epoch 0] Batch 3235, Loss 0.49213820695877075\n",
      "[Training Epoch 0] Batch 3236, Loss 0.49723827838897705\n",
      "[Training Epoch 0] Batch 3237, Loss 0.4730217158794403\n",
      "[Training Epoch 0] Batch 3238, Loss 0.524747371673584\n",
      "[Training Epoch 0] Batch 3239, Loss 0.49209266901016235\n",
      "[Training Epoch 0] Batch 3240, Loss 0.5475298166275024\n",
      "[Training Epoch 0] Batch 3241, Loss 0.4852144122123718\n",
      "[Training Epoch 0] Batch 3242, Loss 0.471121221780777\n",
      "[Training Epoch 0] Batch 3243, Loss 0.48646819591522217\n",
      "[Training Epoch 0] Batch 3244, Loss 0.4862797260284424\n",
      "[Training Epoch 0] Batch 3245, Loss 0.5191835165023804\n",
      "[Training Epoch 0] Batch 3246, Loss 0.5338253378868103\n",
      "[Training Epoch 0] Batch 3247, Loss 0.5088202953338623\n",
      "[Training Epoch 0] Batch 3248, Loss 0.5045261383056641\n",
      "[Training Epoch 0] Batch 3249, Loss 0.510155439376831\n",
      "[Training Epoch 0] Batch 3250, Loss 0.5012038350105286\n",
      "[Training Epoch 0] Batch 3251, Loss 0.5438695549964905\n",
      "[Training Epoch 0] Batch 3252, Loss 0.49963054060935974\n",
      "[Training Epoch 0] Batch 3253, Loss 0.5112469792366028\n",
      "[Training Epoch 0] Batch 3254, Loss 0.49914753437042236\n",
      "[Training Epoch 0] Batch 3255, Loss 0.4841880798339844\n",
      "[Training Epoch 0] Batch 3256, Loss 0.5416327118873596\n",
      "[Training Epoch 0] Batch 3257, Loss 0.5135494470596313\n",
      "[Training Epoch 0] Batch 3258, Loss 0.47756773233413696\n",
      "[Training Epoch 0] Batch 3259, Loss 0.5073318481445312\n",
      "[Training Epoch 0] Batch 3260, Loss 0.5061337351799011\n",
      "[Training Epoch 0] Batch 3261, Loss 0.5153568983078003\n",
      "[Training Epoch 0] Batch 3262, Loss 0.49672216176986694\n",
      "[Training Epoch 0] Batch 3263, Loss 0.48897504806518555\n",
      "[Training Epoch 0] Batch 3264, Loss 0.5104045867919922\n",
      "[Training Epoch 0] Batch 3265, Loss 0.5011640787124634\n",
      "[Training Epoch 0] Batch 3266, Loss 0.5082800388336182\n",
      "[Training Epoch 0] Batch 3267, Loss 0.4929519593715668\n",
      "[Training Epoch 0] Batch 3268, Loss 0.5022472739219666\n",
      "[Training Epoch 0] Batch 3269, Loss 0.5167255401611328\n",
      "[Training Epoch 0] Batch 3270, Loss 0.46045780181884766\n",
      "[Training Epoch 0] Batch 3271, Loss 0.5063555240631104\n",
      "[Training Epoch 0] Batch 3272, Loss 0.4880300760269165\n",
      "[Training Epoch 0] Batch 3273, Loss 0.5114260911941528\n",
      "[Training Epoch 0] Batch 3274, Loss 0.5011041164398193\n",
      "[Training Epoch 0] Batch 3275, Loss 0.5061509013175964\n",
      "[Training Epoch 0] Batch 3276, Loss 0.5045768618583679\n",
      "[Training Epoch 0] Batch 3277, Loss 0.4800944924354553\n",
      "[Training Epoch 0] Batch 3278, Loss 0.496969997882843\n",
      "[Training Epoch 0] Batch 3279, Loss 0.4849659502506256\n",
      "[Training Epoch 0] Batch 3280, Loss 0.4997096061706543\n",
      "[Training Epoch 0] Batch 3281, Loss 0.5163207054138184\n",
      "[Training Epoch 0] Batch 3282, Loss 0.5085691809654236\n",
      "[Training Epoch 0] Batch 3283, Loss 0.4723435938358307\n",
      "[Training Epoch 0] Batch 3284, Loss 0.486880898475647\n",
      "[Training Epoch 0] Batch 3285, Loss 0.49136972427368164\n",
      "[Training Epoch 0] Batch 3286, Loss 0.4785771369934082\n",
      "[Training Epoch 0] Batch 3287, Loss 0.5058619976043701\n",
      "[Training Epoch 0] Batch 3288, Loss 0.48529231548309326\n",
      "[Training Epoch 0] Batch 3289, Loss 0.49055641889572144\n",
      "[Training Epoch 0] Batch 3290, Loss 0.5100774765014648\n",
      "[Training Epoch 0] Batch 3291, Loss 0.5129743814468384\n",
      "[Training Epoch 0] Batch 3292, Loss 0.5127983093261719\n",
      "[Training Epoch 0] Batch 3293, Loss 0.5176219344139099\n",
      "[Training Epoch 0] Batch 3294, Loss 0.5034909248352051\n",
      "[Training Epoch 0] Batch 3295, Loss 0.5021374821662903\n",
      "[Training Epoch 0] Batch 3296, Loss 0.5071482062339783\n",
      "[Training Epoch 0] Batch 3297, Loss 0.5353515148162842\n",
      "[Training Epoch 0] Batch 3298, Loss 0.5334834456443787\n",
      "[Training Epoch 0] Batch 3299, Loss 0.5007112622261047\n",
      "[Training Epoch 0] Batch 3300, Loss 0.4826570749282837\n",
      "[Training Epoch 0] Batch 3301, Loss 0.5206858515739441\n",
      "[Training Epoch 0] Batch 3302, Loss 0.5007696151733398\n",
      "[Training Epoch 0] Batch 3303, Loss 0.5296348333358765\n",
      "[Training Epoch 0] Batch 3304, Loss 0.5048547387123108\n",
      "[Training Epoch 0] Batch 3305, Loss 0.4914510250091553\n",
      "[Training Epoch 0] Batch 3306, Loss 0.510158360004425\n",
      "[Training Epoch 0] Batch 3307, Loss 0.4876740574836731\n",
      "[Training Epoch 0] Batch 3308, Loss 0.5048727989196777\n",
      "[Training Epoch 0] Batch 3309, Loss 0.4720200300216675\n",
      "[Training Epoch 0] Batch 3310, Loss 0.5183671712875366\n",
      "[Training Epoch 0] Batch 3311, Loss 0.5137784481048584\n",
      "[Training Epoch 0] Batch 3312, Loss 0.5122987031936646\n",
      "[Training Epoch 0] Batch 3313, Loss 0.5021687150001526\n",
      "[Training Epoch 0] Batch 3314, Loss 0.5193811655044556\n",
      "[Training Epoch 0] Batch 3315, Loss 0.48640623688697815\n",
      "[Training Epoch 0] Batch 3316, Loss 0.5060071349143982\n",
      "[Training Epoch 0] Batch 3317, Loss 0.5272152423858643\n",
      "[Training Epoch 0] Batch 3318, Loss 0.4865814745426178\n",
      "[Training Epoch 0] Batch 3319, Loss 0.5164299011230469\n",
      "[Training Epoch 0] Batch 3320, Loss 0.5022866129875183\n",
      "[Training Epoch 0] Batch 3321, Loss 0.500832736492157\n",
      "[Training Epoch 0] Batch 3322, Loss 0.508711576461792\n",
      "[Training Epoch 0] Batch 3323, Loss 0.5010828375816345\n",
      "[Training Epoch 0] Batch 3324, Loss 0.5085108876228333\n",
      "[Training Epoch 0] Batch 3325, Loss 0.4901133179664612\n",
      "[Training Epoch 0] Batch 3326, Loss 0.4941905438899994\n",
      "[Training Epoch 0] Batch 3327, Loss 0.5150315165519714\n",
      "[Training Epoch 0] Batch 3328, Loss 0.4888177514076233\n",
      "[Training Epoch 0] Batch 3329, Loss 0.5140949487686157\n",
      "[Training Epoch 0] Batch 3330, Loss 0.5232418775558472\n",
      "[Training Epoch 0] Batch 3331, Loss 0.5100243091583252\n",
      "[Training Epoch 0] Batch 3332, Loss 0.4944002032279968\n",
      "[Training Epoch 0] Batch 3333, Loss 0.5102145075798035\n",
      "[Training Epoch 0] Batch 3334, Loss 0.49827924370765686\n",
      "[Training Epoch 0] Batch 3335, Loss 0.4855421483516693\n",
      "[Training Epoch 0] Batch 3336, Loss 0.48492592573165894\n",
      "[Training Epoch 0] Batch 3337, Loss 0.47608861327171326\n",
      "[Training Epoch 0] Batch 3338, Loss 0.5113784074783325\n",
      "[Training Epoch 0] Batch 3339, Loss 0.4654746651649475\n",
      "[Training Epoch 0] Batch 3340, Loss 0.5100797414779663\n",
      "[Training Epoch 0] Batch 3341, Loss 0.5178148746490479\n",
      "[Training Epoch 0] Batch 3342, Loss 0.4810335040092468\n",
      "[Training Epoch 0] Batch 3343, Loss 0.5307552814483643\n",
      "[Training Epoch 0] Batch 3344, Loss 0.5205921530723572\n",
      "[Training Epoch 0] Batch 3345, Loss 0.5058908462524414\n",
      "[Training Epoch 0] Batch 3346, Loss 0.5100337266921997\n",
      "[Training Epoch 0] Batch 3347, Loss 0.49571463465690613\n",
      "[Training Epoch 0] Batch 3348, Loss 0.48741015791893005\n",
      "[Training Epoch 0] Batch 3349, Loss 0.5141309499740601\n",
      "[Training Epoch 0] Batch 3350, Loss 0.4982728064060211\n",
      "[Training Epoch 0] Batch 3351, Loss 0.5152021050453186\n",
      "[Training Epoch 0] Batch 3352, Loss 0.4994136095046997\n",
      "[Training Epoch 0] Batch 3353, Loss 0.48884567618370056\n",
      "[Training Epoch 0] Batch 3354, Loss 0.5022638440132141\n",
      "[Training Epoch 0] Batch 3355, Loss 0.4968075156211853\n",
      "[Training Epoch 0] Batch 3356, Loss 0.49033281207084656\n",
      "[Training Epoch 0] Batch 3357, Loss 0.5018489360809326\n",
      "[Training Epoch 0] Batch 3358, Loss 0.5059389472007751\n",
      "[Training Epoch 0] Batch 3359, Loss 0.4957258701324463\n",
      "[Training Epoch 0] Batch 3360, Loss 0.5046818256378174\n",
      "[Training Epoch 0] Batch 3361, Loss 0.5208289623260498\n",
      "[Training Epoch 0] Batch 3362, Loss 0.4810176193714142\n",
      "[Training Epoch 0] Batch 3363, Loss 0.48775461316108704\n",
      "[Training Epoch 0] Batch 3364, Loss 0.5006137490272522\n",
      "[Training Epoch 0] Batch 3365, Loss 0.5065258741378784\n",
      "[Training Epoch 0] Batch 3366, Loss 0.5035222172737122\n",
      "[Training Epoch 0] Batch 3367, Loss 0.5010426044464111\n",
      "[Training Epoch 0] Batch 3368, Loss 0.47483158111572266\n",
      "[Training Epoch 0] Batch 3369, Loss 0.4995596706867218\n",
      "[Training Epoch 0] Batch 3370, Loss 0.5008184313774109\n",
      "[Training Epoch 0] Batch 3371, Loss 0.49827975034713745\n",
      "[Training Epoch 0] Batch 3372, Loss 0.5086470246315002\n",
      "[Training Epoch 0] Batch 3373, Loss 0.4928469955921173\n",
      "[Training Epoch 0] Batch 3374, Loss 0.4969030022621155\n",
      "[Training Epoch 0] Batch 3375, Loss 0.4838876724243164\n",
      "[Training Epoch 0] Batch 3376, Loss 0.4970746636390686\n",
      "[Training Epoch 0] Batch 3377, Loss 0.498238205909729\n",
      "[Training Epoch 0] Batch 3378, Loss 0.5126128792762756\n",
      "[Training Epoch 0] Batch 3379, Loss 0.5321313738822937\n",
      "[Training Epoch 0] Batch 3380, Loss 0.5176002383232117\n",
      "[Training Epoch 0] Batch 3381, Loss 0.5164438486099243\n",
      "[Training Epoch 0] Batch 3382, Loss 0.48232319951057434\n",
      "[Training Epoch 0] Batch 3383, Loss 0.502050518989563\n",
      "[Training Epoch 0] Batch 3384, Loss 0.4809763431549072\n",
      "[Training Epoch 0] Batch 3385, Loss 0.5061641931533813\n",
      "[Training Epoch 0] Batch 3386, Loss 0.4733952283859253\n",
      "[Training Epoch 0] Batch 3387, Loss 0.5007511377334595\n",
      "[Training Epoch 0] Batch 3388, Loss 0.4980456829071045\n",
      "[Training Epoch 0] Batch 3389, Loss 0.4833499789237976\n",
      "[Training Epoch 0] Batch 3390, Loss 0.5230289101600647\n",
      "[Training Epoch 0] Batch 3391, Loss 0.5321549773216248\n",
      "[Training Epoch 0] Batch 3392, Loss 0.4484628438949585\n",
      "[Training Epoch 0] Batch 3393, Loss 0.4836718738079071\n",
      "[Training Epoch 0] Batch 3394, Loss 0.5047565698623657\n",
      "[Training Epoch 0] Batch 3395, Loss 0.5273591876029968\n",
      "[Training Epoch 0] Batch 3396, Loss 0.4879058301448822\n",
      "[Training Epoch 0] Batch 3397, Loss 0.49660205841064453\n",
      "[Training Epoch 0] Batch 3398, Loss 0.4834507703781128\n",
      "[Training Epoch 0] Batch 3399, Loss 0.47193706035614014\n",
      "[Training Epoch 0] Batch 3400, Loss 0.5063324570655823\n",
      "[Training Epoch 0] Batch 3401, Loss 0.5005771517753601\n",
      "[Training Epoch 0] Batch 3402, Loss 0.498002290725708\n",
      "[Training Epoch 0] Batch 3403, Loss 0.5203902721405029\n",
      "[Training Epoch 0] Batch 3404, Loss 0.49968913197517395\n",
      "[Training Epoch 0] Batch 3405, Loss 0.4890403747558594\n",
      "[Training Epoch 0] Batch 3406, Loss 0.5189785957336426\n",
      "[Training Epoch 0] Batch 3407, Loss 0.4798281490802765\n",
      "[Training Epoch 0] Batch 3408, Loss 0.4836215376853943\n",
      "[Training Epoch 0] Batch 3409, Loss 0.4741578698158264\n",
      "[Training Epoch 0] Batch 3410, Loss 0.4970760941505432\n",
      "[Training Epoch 0] Batch 3411, Loss 0.5259594321250916\n",
      "[Training Epoch 0] Batch 3412, Loss 0.5229331254959106\n",
      "[Training Epoch 0] Batch 3413, Loss 0.48155879974365234\n",
      "[Training Epoch 0] Batch 3414, Loss 0.48517608642578125\n",
      "[Training Epoch 0] Batch 3415, Loss 0.5206904411315918\n",
      "[Training Epoch 0] Batch 3416, Loss 0.47735315561294556\n",
      "[Training Epoch 0] Batch 3417, Loss 0.507904052734375\n",
      "[Training Epoch 0] Batch 3418, Loss 0.5035215020179749\n",
      "[Training Epoch 0] Batch 3419, Loss 0.4954475164413452\n",
      "[Training Epoch 0] Batch 3420, Loss 0.5059980750083923\n",
      "[Training Epoch 0] Batch 3421, Loss 0.49057063460350037\n",
      "[Training Epoch 0] Batch 3422, Loss 0.4969141185283661\n",
      "[Training Epoch 0] Batch 3423, Loss 0.4903368651866913\n",
      "[Training Epoch 0] Batch 3424, Loss 0.4940628707408905\n",
      "[Training Epoch 0] Batch 3425, Loss 0.5259621143341064\n",
      "[Training Epoch 0] Batch 3426, Loss 0.4901605546474457\n",
      "[Training Epoch 0] Batch 3427, Loss 0.4905155301094055\n",
      "[Training Epoch 0] Batch 3428, Loss 0.49829763174057007\n",
      "[Training Epoch 0] Batch 3429, Loss 0.4848753809928894\n",
      "[Training Epoch 0] Batch 3430, Loss 0.508862316608429\n",
      "[Training Epoch 0] Batch 3431, Loss 0.5021007657051086\n",
      "[Training Epoch 0] Batch 3432, Loss 0.48111599683761597\n",
      "[Training Epoch 0] Batch 3433, Loss 0.47458067536354065\n",
      "[Training Epoch 0] Batch 3434, Loss 0.48870089650154114\n",
      "[Training Epoch 0] Batch 3435, Loss 0.49824097752571106\n",
      "[Training Epoch 0] Batch 3436, Loss 0.49695611000061035\n",
      "[Training Epoch 0] Batch 3437, Loss 0.48774805665016174\n",
      "[Training Epoch 0] Batch 3438, Loss 0.5153468251228333\n",
      "[Training Epoch 0] Batch 3439, Loss 0.507293164730072\n",
      "[Training Epoch 0] Batch 3440, Loss 0.4835881292819977\n",
      "[Training Epoch 0] Batch 3441, Loss 0.507091224193573\n",
      "[Training Epoch 0] Batch 3442, Loss 0.4994962513446808\n",
      "[Training Epoch 0] Batch 3443, Loss 0.50228351354599\n",
      "[Training Epoch 0] Batch 3444, Loss 0.5114192962646484\n",
      "[Training Epoch 0] Batch 3445, Loss 0.4943287968635559\n",
      "[Training Epoch 0] Batch 3446, Loss 0.4889366626739502\n",
      "[Training Epoch 0] Batch 3447, Loss 0.4890486001968384\n",
      "[Training Epoch 0] Batch 3448, Loss 0.49549245834350586\n",
      "[Training Epoch 0] Batch 3449, Loss 0.507276713848114\n",
      "[Training Epoch 0] Batch 3450, Loss 0.4559050500392914\n",
      "[Training Epoch 0] Batch 3451, Loss 0.47972604632377625\n",
      "[Training Epoch 0] Batch 3452, Loss 0.4768448770046234\n",
      "[Training Epoch 0] Batch 3453, Loss 0.4557293951511383\n",
      "[Training Epoch 0] Batch 3454, Loss 0.48230206966400146\n",
      "[Training Epoch 0] Batch 3455, Loss 0.4914683401584625\n",
      "[Training Epoch 0] Batch 3456, Loss 0.5115026831626892\n",
      "[Training Epoch 0] Batch 3457, Loss 0.5167871713638306\n",
      "[Training Epoch 0] Batch 3458, Loss 0.5061625838279724\n",
      "[Training Epoch 0] Batch 3459, Loss 0.48752182722091675\n",
      "[Training Epoch 0] Batch 3460, Loss 0.5088145732879639\n",
      "[Training Epoch 0] Batch 3461, Loss 0.4914604723453522\n",
      "[Training Epoch 0] Batch 3462, Loss 0.4993472695350647\n",
      "[Training Epoch 0] Batch 3463, Loss 0.49574655294418335\n",
      "[Training Epoch 0] Batch 3464, Loss 0.49023550748825073\n",
      "[Training Epoch 0] Batch 3465, Loss 0.5246692895889282\n",
      "[Training Epoch 0] Batch 3466, Loss 0.5154489278793335\n",
      "[Training Epoch 0] Batch 3467, Loss 0.5153054594993591\n",
      "[Training Epoch 0] Batch 3468, Loss 0.49813520908355713\n",
      "[Training Epoch 0] Batch 3469, Loss 0.4741041660308838\n",
      "[Training Epoch 0] Batch 3470, Loss 0.513966977596283\n",
      "[Training Epoch 0] Batch 3471, Loss 0.5020841956138611\n",
      "[Training Epoch 0] Batch 3472, Loss 0.5009480714797974\n",
      "[Training Epoch 0] Batch 3473, Loss 0.5046119689941406\n",
      "[Training Epoch 0] Batch 3474, Loss 0.5179147720336914\n",
      "[Training Epoch 0] Batch 3475, Loss 0.49950942397117615\n",
      "[Training Epoch 0] Batch 3476, Loss 0.5218966603279114\n",
      "[Training Epoch 0] Batch 3477, Loss 0.5300427675247192\n",
      "[Training Epoch 0] Batch 3478, Loss 0.4756486117839813\n",
      "[Training Epoch 0] Batch 3479, Loss 0.5205153226852417\n",
      "[Training Epoch 0] Batch 3480, Loss 0.4993697702884674\n",
      "[Training Epoch 0] Batch 3481, Loss 0.5073256492614746\n",
      "[Training Epoch 0] Batch 3482, Loss 0.4927990138530731\n",
      "[Training Epoch 0] Batch 3483, Loss 0.5204415321350098\n",
      "[Training Epoch 0] Batch 3484, Loss 0.5377916693687439\n",
      "[Training Epoch 0] Batch 3485, Loss 0.4846893846988678\n",
      "[Training Epoch 0] Batch 3486, Loss 0.5232309103012085\n",
      "[Training Epoch 0] Batch 3487, Loss 0.48615238070487976\n",
      "[Training Epoch 0] Batch 3488, Loss 0.48613160848617554\n",
      "[Training Epoch 0] Batch 3489, Loss 0.5007725954055786\n",
      "[Training Epoch 0] Batch 3490, Loss 0.4742656946182251\n",
      "[Training Epoch 0] Batch 3491, Loss 0.4993474781513214\n",
      "[Training Epoch 0] Batch 3492, Loss 0.48880118131637573\n",
      "[Training Epoch 0] Batch 3493, Loss 0.4822165071964264\n",
      "[Training Epoch 0] Batch 3494, Loss 0.4663761556148529\n",
      "[Training Epoch 0] Batch 3495, Loss 0.5273829102516174\n",
      "[Training Epoch 0] Batch 3496, Loss 0.5445066094398499\n",
      "[Training Epoch 0] Batch 3497, Loss 0.5098658800125122\n",
      "[Training Epoch 0] Batch 3498, Loss 0.49681973457336426\n",
      "[Training Epoch 0] Batch 3499, Loss 0.5086885690689087\n",
      "[Training Epoch 0] Batch 3500, Loss 0.47059667110443115\n",
      "[Training Epoch 0] Batch 3501, Loss 0.5179224610328674\n",
      "[Training Epoch 0] Batch 3502, Loss 0.4864341616630554\n",
      "[Training Epoch 0] Batch 3503, Loss 0.4876050055027008\n",
      "[Training Epoch 0] Batch 3504, Loss 0.5046063661575317\n",
      "[Training Epoch 0] Batch 3505, Loss 0.5084863305091858\n",
      "[Training Epoch 0] Batch 3506, Loss 0.5111429691314697\n",
      "[Training Epoch 0] Batch 3507, Loss 0.5034656524658203\n",
      "[Training Epoch 0] Batch 3508, Loss 0.493773877620697\n",
      "[Training Epoch 0] Batch 3509, Loss 0.5244845747947693\n",
      "[Training Epoch 0] Batch 3510, Loss 0.5139738321304321\n",
      "[Training Epoch 0] Batch 3511, Loss 0.5298559069633484\n",
      "[Training Epoch 0] Batch 3512, Loss 0.503609299659729\n",
      "[Training Epoch 0] Batch 3513, Loss 0.5141227841377258\n",
      "[Training Epoch 0] Batch 3514, Loss 0.5142138004302979\n",
      "[Training Epoch 0] Batch 3515, Loss 0.4970560371875763\n",
      "[Training Epoch 0] Batch 3516, Loss 0.5009287595748901\n",
      "[Training Epoch 0] Batch 3517, Loss 0.4769520163536072\n",
      "[Training Epoch 0] Batch 3518, Loss 0.5232707858085632\n",
      "[Training Epoch 0] Batch 3519, Loss 0.4980020225048065\n",
      "[Training Epoch 0] Batch 3520, Loss 0.4899570941925049\n",
      "[Training Epoch 0] Batch 3521, Loss 0.535142719745636\n",
      "[Training Epoch 0] Batch 3522, Loss 0.48896363377571106\n",
      "[Training Epoch 0] Batch 3523, Loss 0.5141329169273376\n",
      "[Training Epoch 0] Batch 3524, Loss 0.49003884196281433\n",
      "[Training Epoch 0] Batch 3525, Loss 0.47799092531204224\n",
      "[Training Epoch 0] Batch 3526, Loss 0.5340639352798462\n",
      "[Training Epoch 0] Batch 3527, Loss 0.4875771403312683\n",
      "[Training Epoch 0] Batch 3528, Loss 0.5286493301391602\n",
      "[Training Epoch 0] Batch 3529, Loss 0.5009134411811829\n",
      "[Training Epoch 0] Batch 3530, Loss 0.5060296654701233\n",
      "[Training Epoch 0] Batch 3531, Loss 0.5044721364974976\n",
      "[Training Epoch 0] Batch 3532, Loss 0.48351043462753296\n",
      "[Training Epoch 0] Batch 3533, Loss 0.4755048155784607\n",
      "[Training Epoch 0] Batch 3534, Loss 0.4849436283111572\n",
      "[Training Epoch 0] Batch 3535, Loss 0.4779864549636841\n",
      "[Training Epoch 0] Batch 3536, Loss 0.5034686326980591\n",
      "[Training Epoch 0] Batch 3537, Loss 0.48714426159858704\n",
      "[Training Epoch 0] Batch 3538, Loss 0.4935070872306824\n",
      "[Training Epoch 0] Batch 3539, Loss 0.4726991057395935\n",
      "[Training Epoch 0] Batch 3540, Loss 0.5366235375404358\n",
      "[Training Epoch 0] Batch 3541, Loss 0.4833836555480957\n",
      "[Training Epoch 0] Batch 3542, Loss 0.47543203830718994\n",
      "[Training Epoch 0] Batch 3543, Loss 0.4821089208126068\n",
      "[Training Epoch 0] Batch 3544, Loss 0.48988014459609985\n",
      "[Training Epoch 0] Batch 3545, Loss 0.47801804542541504\n",
      "[Training Epoch 0] Batch 3546, Loss 0.5103005170822144\n",
      "[Training Epoch 0] Batch 3547, Loss 0.5097929835319519\n",
      "[Training Epoch 0] Batch 3548, Loss 0.491612046957016\n",
      "[Training Epoch 0] Batch 3549, Loss 0.4872574806213379\n",
      "[Training Epoch 0] Batch 3550, Loss 0.4755958616733551\n",
      "[Training Epoch 0] Batch 3551, Loss 0.4888588488101959\n",
      "[Training Epoch 0] Batch 3552, Loss 0.4901687502861023\n",
      "[Training Epoch 0] Batch 3553, Loss 0.47650831937789917\n",
      "[Training Epoch 0] Batch 3554, Loss 0.4926658570766449\n",
      "[Training Epoch 0] Batch 3555, Loss 0.48585039377212524\n",
      "[Training Epoch 0] Batch 3556, Loss 0.48378223180770874\n",
      "[Training Epoch 0] Batch 3557, Loss 0.5061715245246887\n",
      "[Training Epoch 0] Batch 3558, Loss 0.5112473368644714\n",
      "[Training Epoch 0] Batch 3559, Loss 0.5136164426803589\n",
      "[Training Epoch 0] Batch 3560, Loss 0.4899557828903198\n",
      "[Training Epoch 0] Batch 3561, Loss 0.5059001445770264\n",
      "[Training Epoch 0] Batch 3562, Loss 0.5391037464141846\n",
      "[Training Epoch 0] Batch 3563, Loss 0.49382483959198\n",
      "[Training Epoch 0] Batch 3564, Loss 0.47146713733673096\n",
      "[Training Epoch 0] Batch 3565, Loss 0.4900447428226471\n",
      "[Training Epoch 0] Batch 3566, Loss 0.5222156643867493\n",
      "[Training Epoch 0] Batch 3567, Loss 0.5297771096229553\n",
      "[Training Epoch 0] Batch 3568, Loss 0.5011335611343384\n",
      "[Training Epoch 0] Batch 3569, Loss 0.49614417552948\n",
      "[Training Epoch 0] Batch 3570, Loss 0.5137796401977539\n",
      "[Training Epoch 0] Batch 3571, Loss 0.528485894203186\n",
      "[Training Epoch 0] Batch 3572, Loss 0.5112047791481018\n",
      "[Training Epoch 0] Batch 3573, Loss 0.4982130229473114\n",
      "[Training Epoch 0] Batch 3574, Loss 0.5146188735961914\n",
      "[Training Epoch 0] Batch 3575, Loss 0.49267980456352234\n",
      "[Training Epoch 0] Batch 3576, Loss 0.4945109784603119\n",
      "[Training Epoch 0] Batch 3577, Loss 0.5124213695526123\n",
      "[Training Epoch 0] Batch 3578, Loss 0.48065394163131714\n",
      "[Training Epoch 0] Batch 3579, Loss 0.4979039430618286\n",
      "[Training Epoch 0] Batch 3580, Loss 0.4997466504573822\n",
      "[Training Epoch 0] Batch 3581, Loss 0.4953905940055847\n",
      "[Training Epoch 0] Batch 3582, Loss 0.5095541477203369\n",
      "[Training Epoch 0] Batch 3583, Loss 0.5104411840438843\n",
      "[Training Epoch 0] Batch 3584, Loss 0.4931684732437134\n",
      "[Training Epoch 0] Batch 3585, Loss 0.5420562624931335\n",
      "[Training Epoch 0] Batch 3586, Loss 0.48718228936195374\n",
      "[Training Epoch 0] Batch 3587, Loss 0.5081583261489868\n",
      "[Training Epoch 0] Batch 3588, Loss 0.5073538422584534\n",
      "[Training Epoch 0] Batch 3589, Loss 0.4930345416069031\n",
      "[Training Epoch 0] Batch 3590, Loss 0.4928801655769348\n",
      "[Training Epoch 0] Batch 3591, Loss 0.49683353304862976\n",
      "[Training Epoch 0] Batch 3592, Loss 0.5248750448226929\n",
      "[Training Epoch 0] Batch 3593, Loss 0.5289193987846375\n",
      "[Training Epoch 0] Batch 3594, Loss 0.48718777298927307\n",
      "[Training Epoch 0] Batch 3595, Loss 0.4836234152317047\n",
      "[Training Epoch 0] Batch 3596, Loss 0.4821757674217224\n",
      "[Training Epoch 0] Batch 3597, Loss 0.4860537648200989\n",
      "[Training Epoch 0] Batch 3598, Loss 0.5344488620758057\n",
      "[Training Epoch 0] Batch 3599, Loss 0.49660220742225647\n",
      "[Training Epoch 0] Batch 3600, Loss 0.5273749232292175\n",
      "[Training Epoch 0] Batch 3601, Loss 0.5088850259780884\n",
      "[Training Epoch 0] Batch 3602, Loss 0.4980268180370331\n",
      "[Training Epoch 0] Batch 3603, Loss 0.4781557321548462\n",
      "[Training Epoch 0] Batch 3604, Loss 0.4926098585128784\n",
      "[Training Epoch 0] Batch 3605, Loss 0.5002885460853577\n",
      "[Training Epoch 0] Batch 3606, Loss 0.5258718729019165\n",
      "[Training Epoch 0] Batch 3607, Loss 0.5025421977043152\n",
      "[Training Epoch 0] Batch 3608, Loss 0.4850185513496399\n",
      "[Training Epoch 0] Batch 3609, Loss 0.4858194589614868\n",
      "[Training Epoch 0] Batch 3610, Loss 0.4846121370792389\n",
      "[Training Epoch 0] Batch 3611, Loss 0.49281448125839233\n",
      "[Training Epoch 0] Batch 3612, Loss 0.4966871738433838\n",
      "[Training Epoch 0] Batch 3613, Loss 0.48464369773864746\n",
      "[Training Epoch 0] Batch 3614, Loss 0.47006163001060486\n",
      "[Training Epoch 0] Batch 3615, Loss 0.48176681995391846\n",
      "[Training Epoch 0] Batch 3616, Loss 0.47928494215011597\n",
      "[Training Epoch 0] Batch 3617, Loss 0.49570605158805847\n",
      "[Training Epoch 0] Batch 3618, Loss 0.5129877328872681\n",
      "[Training Epoch 0] Batch 3619, Loss 0.48204031586647034\n",
      "[Training Epoch 0] Batch 3620, Loss 0.4660381078720093\n",
      "[Training Epoch 0] Batch 3621, Loss 0.5044704675674438\n",
      "[Training Epoch 0] Batch 3622, Loss 0.5125744938850403\n",
      "[Training Epoch 0] Batch 3623, Loss 0.46066856384277344\n",
      "[Training Epoch 0] Batch 3624, Loss 0.509996771812439\n",
      "[Training Epoch 0] Batch 3625, Loss 0.4992327094078064\n",
      "[Training Epoch 0] Batch 3626, Loss 0.49902868270874023\n",
      "[Training Epoch 0] Batch 3627, Loss 0.5166510939598083\n",
      "[Training Epoch 0] Batch 3628, Loss 0.5234285593032837\n",
      "[Training Epoch 0] Batch 3629, Loss 0.5087875723838806\n",
      "[Training Epoch 0] Batch 3630, Loss 0.47366708517074585\n",
      "[Training Epoch 0] Batch 3631, Loss 0.5101912617683411\n",
      "[Training Epoch 0] Batch 3632, Loss 0.5209779739379883\n",
      "[Training Epoch 0] Batch 3633, Loss 0.5263946652412415\n",
      "[Training Epoch 0] Batch 3634, Loss 0.4975847005844116\n",
      "[Training Epoch 0] Batch 3635, Loss 0.4805852472782135\n",
      "[Training Epoch 0] Batch 3636, Loss 0.507280707359314\n",
      "[Training Epoch 0] Batch 3637, Loss 0.5151197910308838\n",
      "[Training Epoch 0] Batch 3638, Loss 0.5158013105392456\n",
      "[Training Epoch 0] Batch 3639, Loss 0.5192627906799316\n",
      "[Training Epoch 0] Batch 3640, Loss 0.481721967458725\n",
      "[Training Epoch 0] Batch 3641, Loss 0.515498697757721\n",
      "[Training Epoch 0] Batch 3642, Loss 0.4876191318035126\n",
      "[Training Epoch 0] Batch 3643, Loss 0.5005658268928528\n",
      "[Training Epoch 0] Batch 3644, Loss 0.49281221628189087\n",
      "[Training Epoch 0] Batch 3645, Loss 0.4826076328754425\n",
      "[Training Epoch 0] Batch 3646, Loss 0.4943595826625824\n",
      "[Training Epoch 0] Batch 3647, Loss 0.4647826552391052\n",
      "[Training Epoch 0] Batch 3648, Loss 0.5168102383613586\n",
      "[Training Epoch 0] Batch 3649, Loss 0.5139574408531189\n",
      "[Training Epoch 0] Batch 3650, Loss 0.4905075430870056\n",
      "[Training Epoch 0] Batch 3651, Loss 0.5232928991317749\n",
      "[Training Epoch 0] Batch 3652, Loss 0.5257934927940369\n",
      "[Training Epoch 0] Batch 3653, Loss 0.5059959292411804\n",
      "[Training Epoch 0] Batch 3654, Loss 0.5073680877685547\n",
      "[Training Epoch 0] Batch 3655, Loss 0.48195505142211914\n",
      "[Training Epoch 0] Batch 3656, Loss 0.5073475241661072\n",
      "[Training Epoch 0] Batch 3657, Loss 0.4887828230857849\n",
      "[Training Epoch 0] Batch 3658, Loss 0.5221251249313354\n",
      "[Training Epoch 0] Batch 3659, Loss 0.49960094690322876\n",
      "[Training Epoch 0] Batch 3660, Loss 0.49243658781051636\n",
      "[Training Epoch 0] Batch 3661, Loss 0.4968774616718292\n",
      "[Training Epoch 0] Batch 3662, Loss 0.4885243773460388\n",
      "[Training Epoch 0] Batch 3663, Loss 0.5221205949783325\n",
      "[Training Epoch 0] Batch 3664, Loss 0.47038209438323975\n",
      "[Training Epoch 0] Batch 3665, Loss 0.5156198143959045\n",
      "[Training Epoch 0] Batch 3666, Loss 0.4991702139377594\n",
      "[Training Epoch 0] Batch 3667, Loss 0.48627883195877075\n",
      "[Training Epoch 0] Batch 3668, Loss 0.5101259350776672\n",
      "[Training Epoch 0] Batch 3669, Loss 0.48051556944847107\n",
      "[Training Epoch 0] Batch 3670, Loss 0.5088015198707581\n",
      "[Training Epoch 0] Batch 3671, Loss 0.5059953927993774\n",
      "[Training Epoch 0] Batch 3672, Loss 0.5036765336990356\n",
      "[Training Epoch 0] Batch 3673, Loss 0.5057565569877625\n",
      "[Training Epoch 0] Batch 3674, Loss 0.5217968225479126\n",
      "[Training Epoch 0] Batch 3675, Loss 0.49254950881004333\n",
      "[Training Epoch 0] Batch 3676, Loss 0.5220835208892822\n",
      "[Training Epoch 0] Batch 3677, Loss 0.5178322792053223\n",
      "[Training Epoch 0] Batch 3678, Loss 0.4977153539657593\n",
      "[Training Epoch 0] Batch 3679, Loss 0.4832931160926819\n",
      "[Training Epoch 0] Batch 3680, Loss 0.49266788363456726\n",
      "[Training Epoch 0] Batch 3681, Loss 0.4912806451320648\n",
      "[Training Epoch 0] Batch 3682, Loss 0.4812430143356323\n",
      "[Training Epoch 0] Batch 3683, Loss 0.48994356393814087\n",
      "[Training Epoch 0] Batch 3684, Loss 0.5087602734565735\n",
      "[Training Epoch 0] Batch 3685, Loss 0.4835525453090668\n",
      "[Training Epoch 0] Batch 3686, Loss 0.5021485090255737\n",
      "[Training Epoch 0] Batch 3687, Loss 0.520329475402832\n",
      "[Training Epoch 0] Batch 3688, Loss 0.4808553457260132\n",
      "[Training Epoch 0] Batch 3689, Loss 0.5271977782249451\n",
      "[Training Epoch 0] Batch 3690, Loss 0.5141432285308838\n",
      "[Training Epoch 0] Batch 3691, Loss 0.48885083198547363\n",
      "[Training Epoch 0] Batch 3692, Loss 0.47797754406929016\n",
      "[Training Epoch 0] Batch 3693, Loss 0.5035229921340942\n",
      "[Training Epoch 0] Batch 3694, Loss 0.5034656524658203\n",
      "[Training Epoch 0] Batch 3695, Loss 0.5031673908233643\n",
      "[Training Epoch 0] Batch 3696, Loss 0.5558144450187683\n",
      "[Training Epoch 0] Batch 3697, Loss 0.4941086769104004\n",
      "[Training Epoch 0] Batch 3698, Loss 0.5155705213546753\n",
      "[Training Epoch 0] Batch 3699, Loss 0.5142906904220581\n",
      "[Training Epoch 0] Batch 3700, Loss 0.5155494809150696\n",
      "[Training Epoch 0] Batch 3701, Loss 0.47940054535865784\n",
      "[Training Epoch 0] Batch 3702, Loss 0.5084977746009827\n",
      "[Training Epoch 0] Batch 3703, Loss 0.5115810632705688\n",
      "[Training Epoch 0] Batch 3704, Loss 0.4834769666194916\n",
      "[Training Epoch 0] Batch 3705, Loss 0.5061873197555542\n",
      "[Training Epoch 0] Batch 3706, Loss 0.5035428404808044\n",
      "[Training Epoch 0] Batch 3707, Loss 0.4844020903110504\n",
      "[Training Epoch 0] Batch 3708, Loss 0.5145633816719055\n",
      "[Training Epoch 0] Batch 3709, Loss 0.49935922026634216\n",
      "[Training Epoch 0] Batch 3710, Loss 0.5059127807617188\n",
      "[Training Epoch 0] Batch 3711, Loss 0.5297206044197083\n",
      "[Training Epoch 0] Batch 3712, Loss 0.4939776659011841\n",
      "[Training Epoch 0] Batch 3713, Loss 0.5100005269050598\n",
      "[Training Epoch 0] Batch 3714, Loss 0.4966464638710022\n",
      "[Training Epoch 0] Batch 3715, Loss 0.496755450963974\n",
      "[Training Epoch 0] Batch 3716, Loss 0.5100472569465637\n",
      "[Training Epoch 0] Batch 3717, Loss 0.4832940995693207\n",
      "[Training Epoch 0] Batch 3718, Loss 0.5195621252059937\n",
      "[Training Epoch 0] Batch 3719, Loss 0.4751133322715759\n",
      "[Training Epoch 0] Batch 3720, Loss 0.48334163427352905\n",
      "[Training Epoch 0] Batch 3721, Loss 0.47269123792648315\n",
      "[Training Epoch 0] Batch 3722, Loss 0.47665533423423767\n",
      "[Training Epoch 0] Batch 3723, Loss 0.4989761412143707\n",
      "[Training Epoch 0] Batch 3724, Loss 0.5036854147911072\n",
      "[Training Epoch 0] Batch 3725, Loss 0.49107009172439575\n",
      "[Training Epoch 0] Batch 3726, Loss 0.507224977016449\n",
      "[Training Epoch 0] Batch 3727, Loss 0.5271541476249695\n",
      "[Training Epoch 0] Batch 3728, Loss 0.49131229519844055\n",
      "[Training Epoch 0] Batch 3729, Loss 0.49925974011421204\n",
      "[Training Epoch 0] Batch 3730, Loss 0.47819483280181885\n",
      "[Training Epoch 0] Batch 3731, Loss 0.47975772619247437\n",
      "[Training Epoch 0] Batch 3732, Loss 0.5032520890235901\n",
      "[Training Epoch 0] Batch 3733, Loss 0.5166768431663513\n",
      "[Training Epoch 0] Batch 3734, Loss 0.5058433413505554\n",
      "[Training Epoch 0] Batch 3735, Loss 0.47630465030670166\n",
      "[Training Epoch 0] Batch 3736, Loss 0.4965045154094696\n",
      "[Training Epoch 0] Batch 3737, Loss 0.49919188022613525\n",
      "[Training Epoch 0] Batch 3738, Loss 0.508520245552063\n",
      "[Training Epoch 0] Batch 3739, Loss 0.4957214295864105\n",
      "[Training Epoch 0] Batch 3740, Loss 0.5086796283721924\n",
      "[Training Epoch 0] Batch 3741, Loss 0.4854317307472229\n",
      "[Training Epoch 0] Batch 3742, Loss 0.5142844915390015\n",
      "[Training Epoch 0] Batch 3743, Loss 0.5192521214485168\n",
      "[Training Epoch 0] Batch 3744, Loss 0.4969273805618286\n",
      "[Training Epoch 0] Batch 3745, Loss 0.48615217208862305\n",
      "[Training Epoch 0] Batch 3746, Loss 0.49726802110671997\n",
      "[Training Epoch 0] Batch 3747, Loss 0.4940178394317627\n",
      "[Training Epoch 0] Batch 3748, Loss 0.49065110087394714\n",
      "[Training Epoch 0] Batch 3749, Loss 0.5109140276908875\n",
      "[Training Epoch 0] Batch 3750, Loss 0.5153149962425232\n",
      "[Training Epoch 0] Batch 3751, Loss 0.49219661951065063\n",
      "[Training Epoch 0] Batch 3752, Loss 0.5182325839996338\n",
      "[Training Epoch 0] Batch 3753, Loss 0.5125324726104736\n",
      "[Training Epoch 0] Batch 3754, Loss 0.48213472962379456\n",
      "[Training Epoch 0] Batch 3755, Loss 0.5075854659080505\n",
      "[Training Epoch 0] Batch 3756, Loss 0.47526341676712036\n",
      "[Training Epoch 0] Batch 3757, Loss 0.5019824504852295\n",
      "[Training Epoch 0] Batch 3758, Loss 0.5116592049598694\n",
      "[Training Epoch 0] Batch 3759, Loss 0.4803360402584076\n",
      "[Training Epoch 0] Batch 3760, Loss 0.522236704826355\n",
      "[Training Epoch 0] Batch 3761, Loss 0.5043849349021912\n",
      "[Training Epoch 0] Batch 3762, Loss 0.5085681080818176\n",
      "[Training Epoch 0] Batch 3763, Loss 0.5377538800239563\n",
      "[Training Epoch 0] Batch 3764, Loss 0.5087183713912964\n",
      "[Training Epoch 0] Batch 3765, Loss 0.5180726051330566\n",
      "[Training Epoch 0] Batch 3766, Loss 0.5035529136657715\n",
      "[Training Epoch 0] Batch 3767, Loss 0.5388748645782471\n",
      "[Training Epoch 0] Batch 3768, Loss 0.4794580936431885\n",
      "[Training Epoch 0] Batch 3769, Loss 0.5010021924972534\n",
      "[Training Epoch 0] Batch 3770, Loss 0.48139530420303345\n",
      "[Training Epoch 0] Batch 3771, Loss 0.5004099011421204\n",
      "[Training Epoch 0] Batch 3772, Loss 0.509791374206543\n",
      "[Training Epoch 0] Batch 3773, Loss 0.5059019327163696\n",
      "[Training Epoch 0] Batch 3774, Loss 0.5114313364028931\n",
      "[Training Epoch 0] Batch 3775, Loss 0.48762232065200806\n",
      "[Training Epoch 0] Batch 3776, Loss 0.4927343726158142\n",
      "[Training Epoch 0] Batch 3777, Loss 0.5011419057846069\n",
      "[Training Epoch 0] Batch 3778, Loss 0.524666965007782\n",
      "[Training Epoch 0] Batch 3779, Loss 0.4683871865272522\n",
      "[Training Epoch 0] Batch 3780, Loss 0.4938163161277771\n",
      "[Training Epoch 0] Batch 3781, Loss 0.5208724737167358\n",
      "[Training Epoch 0] Batch 3782, Loss 0.4891025424003601\n",
      "[Training Epoch 0] Batch 3783, Loss 0.48479127883911133\n",
      "[Training Epoch 0] Batch 3784, Loss 0.48866701126098633\n",
      "[Training Epoch 0] Batch 3785, Loss 0.5163596868515015\n",
      "[Training Epoch 0] Batch 3786, Loss 0.4915066063404083\n",
      "[Training Epoch 0] Batch 3787, Loss 0.5158181190490723\n",
      "[Training Epoch 0] Batch 3788, Loss 0.5407955050468445\n",
      "[Training Epoch 0] Batch 3789, Loss 0.49933260679244995\n",
      "[Training Epoch 0] Batch 3790, Loss 0.5195682048797607\n",
      "[Training Epoch 0] Batch 3791, Loss 0.4698704183101654\n",
      "[Training Epoch 0] Batch 3792, Loss 0.49947524070739746\n",
      "[Training Epoch 0] Batch 3793, Loss 0.4968206584453583\n",
      "[Training Epoch 0] Batch 3794, Loss 0.49010035395622253\n",
      "[Training Epoch 0] Batch 3795, Loss 0.5061227679252625\n",
      "[Training Epoch 0] Batch 3796, Loss 0.496992826461792\n",
      "[Training Epoch 0] Batch 3797, Loss 0.5187112092971802\n",
      "[Training Epoch 0] Batch 3798, Loss 0.48579588532447815\n",
      "[Training Epoch 0] Batch 3799, Loss 0.5143460035324097\n",
      "[Training Epoch 0] Batch 3800, Loss 0.49262312054634094\n",
      "[Training Epoch 0] Batch 3801, Loss 0.472876638174057\n",
      "[Training Epoch 0] Batch 3802, Loss 0.4993637204170227\n",
      "[Training Epoch 0] Batch 3803, Loss 0.523676335811615\n",
      "[Training Epoch 0] Batch 3804, Loss 0.4898998737335205\n",
      "[Training Epoch 0] Batch 3805, Loss 0.5005720853805542\n",
      "[Training Epoch 0] Batch 3806, Loss 0.5193449854850769\n",
      "[Training Epoch 0] Batch 3807, Loss 0.5063186287879944\n",
      "[Training Epoch 0] Batch 3808, Loss 0.5232565999031067\n",
      "[Training Epoch 0] Batch 3809, Loss 0.5247253179550171\n",
      "[Training Epoch 0] Batch 3810, Loss 0.49946731328964233\n",
      "[Training Epoch 0] Batch 3811, Loss 0.5194557309150696\n",
      "[Training Epoch 0] Batch 3812, Loss 0.4914069175720215\n",
      "[Training Epoch 0] Batch 3813, Loss 0.472516804933548\n",
      "[Training Epoch 0] Batch 3814, Loss 0.4769030809402466\n",
      "[Training Epoch 0] Batch 3815, Loss 0.4831923544406891\n",
      "[Training Epoch 0] Batch 3816, Loss 0.5005837082862854\n",
      "[Training Epoch 0] Batch 3817, Loss 0.4967609643936157\n",
      "[Training Epoch 0] Batch 3818, Loss 0.47771915793418884\n",
      "[Training Epoch 0] Batch 3819, Loss 0.4967738091945648\n",
      "[Training Epoch 0] Batch 3820, Loss 0.5180326104164124\n",
      "[Training Epoch 0] Batch 3821, Loss 0.4952429533004761\n",
      "[Training Epoch 0] Batch 3822, Loss 0.47531580924987793\n",
      "[Training Epoch 0] Batch 3823, Loss 0.5151113867759705\n",
      "[Training Epoch 0] Batch 3824, Loss 0.5130323767662048\n",
      "[Training Epoch 0] Batch 3825, Loss 0.5032863616943359\n",
      "[Training Epoch 0] Batch 3826, Loss 0.5114387273788452\n",
      "[Training Epoch 0] Batch 3827, Loss 0.5144273638725281\n",
      "[Training Epoch 0] Batch 3828, Loss 0.4898653030395508\n",
      "[Training Epoch 0] Batch 3829, Loss 0.5218640565872192\n",
      "[Training Epoch 0] Batch 3830, Loss 0.5234723687171936\n",
      "[Training Epoch 0] Batch 3831, Loss 0.4995590150356293\n",
      "[Training Epoch 0] Batch 3832, Loss 0.5179149508476257\n",
      "[Training Epoch 0] Batch 3833, Loss 0.5260184407234192\n",
      "[Training Epoch 0] Batch 3834, Loss 0.5328296422958374\n",
      "[Training Epoch 0] Batch 3835, Loss 0.5191646218299866\n",
      "[Training Epoch 0] Batch 3836, Loss 0.5276395082473755\n",
      "[Training Epoch 0] Batch 3837, Loss 0.4994053244590759\n",
      "[Training Epoch 0] Batch 3838, Loss 0.4782526195049286\n",
      "[Training Epoch 0] Batch 3839, Loss 0.5088932514190674\n",
      "[Training Epoch 0] Batch 3840, Loss 0.4900193214416504\n",
      "[Training Epoch 0] Batch 3841, Loss 0.4843766987323761\n",
      "[Training Epoch 0] Batch 3842, Loss 0.519252598285675\n",
      "[Training Epoch 0] Batch 3843, Loss 0.4752082824707031\n",
      "[Training Epoch 0] Batch 3844, Loss 0.49149182438850403\n",
      "[Training Epoch 0] Batch 3845, Loss 0.4524652063846588\n",
      "[Training Epoch 0] Batch 3846, Loss 0.5047711730003357\n",
      "[Training Epoch 0] Batch 3847, Loss 0.5032403469085693\n",
      "[Training Epoch 0] Batch 3848, Loss 0.49261146783828735\n",
      "[Training Epoch 0] Batch 3849, Loss 0.4901507794857025\n",
      "[Training Epoch 0] Batch 3850, Loss 0.5042442083358765\n",
      "[Training Epoch 0] Batch 3851, Loss 0.49253740906715393\n",
      "[Training Epoch 0] Batch 3852, Loss 0.4835723042488098\n",
      "[Training Epoch 0] Batch 3853, Loss 0.508593738079071\n",
      "[Training Epoch 0] Batch 3854, Loss 0.4896388351917267\n",
      "[Training Epoch 0] Batch 3855, Loss 0.490205317735672\n",
      "[Training Epoch 0] Batch 3856, Loss 0.5139119029045105\n",
      "[Training Epoch 0] Batch 3857, Loss 0.4926881194114685\n",
      "[Training Epoch 0] Batch 3858, Loss 0.5220454931259155\n",
      "[Training Epoch 0] Batch 3859, Loss 0.5214630365371704\n",
      "[Training Epoch 0] Batch 3860, Loss 0.49401232600212097\n",
      "[Training Epoch 0] Batch 3861, Loss 0.4926626980304718\n",
      "[Training Epoch 0] Batch 3862, Loss 0.48760801553726196\n",
      "[Training Epoch 0] Batch 3863, Loss 0.5090063214302063\n",
      "[Training Epoch 0] Batch 3864, Loss 0.48435354232788086\n",
      "[Training Epoch 0] Batch 3865, Loss 0.5543534159660339\n",
      "[Training Epoch 0] Batch 3866, Loss 0.5169901847839355\n",
      "[Training Epoch 0] Batch 3867, Loss 0.5086429119110107\n",
      "[Training Epoch 0] Batch 3868, Loss 0.5144997835159302\n",
      "[Training Epoch 0] Batch 3869, Loss 0.49095696210861206\n",
      "[Training Epoch 0] Batch 3870, Loss 0.5181532502174377\n",
      "[Training Epoch 0] Batch 3871, Loss 0.49839910864830017\n",
      "[Training Epoch 0] Batch 3872, Loss 0.5061023831367493\n",
      "[Training Epoch 0] Batch 3873, Loss 0.4874284565448761\n",
      "[Training Epoch 0] Batch 3874, Loss 0.49269598722457886\n",
      "[Training Epoch 0] Batch 3875, Loss 0.46764522790908813\n",
      "[Training Epoch 0] Batch 3876, Loss 0.4991621673107147\n",
      "[Training Epoch 0] Batch 3877, Loss 0.5002450942993164\n",
      "[Training Epoch 0] Batch 3878, Loss 0.5020120739936829\n",
      "[Training Epoch 0] Batch 3879, Loss 0.49870577454566956\n",
      "[Training Epoch 0] Batch 3880, Loss 0.5302174687385559\n",
      "[Training Epoch 0] Batch 3881, Loss 0.4779720902442932\n",
      "[Training Epoch 0] Batch 3882, Loss 0.47916796803474426\n",
      "[Training Epoch 0] Batch 3883, Loss 0.4646994173526764\n",
      "[Training Epoch 0] Batch 3884, Loss 0.5006644129753113\n",
      "[Training Epoch 0] Batch 3885, Loss 0.4672604203224182\n",
      "[Training Epoch 0] Batch 3886, Loss 0.519385039806366\n",
      "[Training Epoch 0] Batch 3887, Loss 0.5182322263717651\n",
      "[Training Epoch 0] Batch 3888, Loss 0.5312475562095642\n",
      "[Training Epoch 0] Batch 3889, Loss 0.49537235498428345\n",
      "[Training Epoch 0] Batch 3890, Loss 0.523583173751831\n",
      "[Training Epoch 0] Batch 3891, Loss 0.48719674348831177\n",
      "[Training Epoch 0] Batch 3892, Loss 0.4911791682243347\n",
      "[Training Epoch 0] Batch 3893, Loss 0.4886571168899536\n",
      "[Training Epoch 0] Batch 3894, Loss 0.5076280236244202\n",
      "[Training Epoch 0] Batch 3895, Loss 0.49645790457725525\n",
      "[Training Epoch 0] Batch 3896, Loss 0.5194607973098755\n",
      "[Training Epoch 0] Batch 3897, Loss 0.531406044960022\n",
      "[Training Epoch 0] Batch 3898, Loss 0.4738476276397705\n",
      "[Training Epoch 0] Batch 3899, Loss 0.4990116357803345\n",
      "[Training Epoch 0] Batch 3900, Loss 0.49239790439605713\n",
      "[Training Epoch 0] Batch 3901, Loss 0.5143754482269287\n",
      "[Training Epoch 0] Batch 3902, Loss 0.5239917635917664\n",
      "[Training Epoch 0] Batch 3903, Loss 0.5366252660751343\n",
      "[Training Epoch 0] Batch 3904, Loss 0.503544270992279\n",
      "[Training Epoch 0] Batch 3905, Loss 0.49549630284309387\n",
      "[Training Epoch 0] Batch 3906, Loss 0.4937947988510132\n",
      "[Training Epoch 0] Batch 3907, Loss 0.4928216338157654\n",
      "[Training Epoch 0] Batch 3908, Loss 0.48911598324775696\n",
      "[Training Epoch 0] Batch 3909, Loss 0.46094128489494324\n",
      "[Training Epoch 0] Batch 3910, Loss 0.5021811723709106\n",
      "[Training Epoch 0] Batch 3911, Loss 0.46137434244155884\n",
      "[Training Epoch 0] Batch 3912, Loss 0.5035247206687927\n",
      "[Training Epoch 0] Batch 3913, Loss 0.47753310203552246\n",
      "[Training Epoch 0] Batch 3914, Loss 0.5316191911697388\n",
      "[Training Epoch 0] Batch 3915, Loss 0.49411410093307495\n",
      "[Training Epoch 0] Batch 3916, Loss 0.5179352760314941\n",
      "[Training Epoch 0] Batch 3917, Loss 0.5004234910011292\n",
      "[Training Epoch 0] Batch 3918, Loss 0.5047909617424011\n",
      "[Training Epoch 0] Batch 3919, Loss 0.5006632804870605\n",
      "[Training Epoch 0] Batch 3920, Loss 0.5024596452713013\n",
      "[Training Epoch 0] Batch 3921, Loss 0.4861666262149811\n",
      "[Training Epoch 0] Batch 3922, Loss 0.5074739456176758\n",
      "[Training Epoch 0] Batch 3923, Loss 0.5076149702072144\n",
      "[Training Epoch 0] Batch 3924, Loss 0.545056939125061\n",
      "[Training Epoch 0] Batch 3925, Loss 0.5116615295410156\n",
      "[Training Epoch 0] Batch 3926, Loss 0.5077463388442993\n",
      "[Training Epoch 0] Batch 3927, Loss 0.48423945903778076\n",
      "[Training Epoch 0] Batch 3928, Loss 0.5097439289093018\n",
      "[Training Epoch 0] Batch 3929, Loss 0.4832879900932312\n",
      "[Training Epoch 0] Batch 3930, Loss 0.48178350925445557\n",
      "[Training Epoch 0] Batch 3931, Loss 0.49145954847335815\n",
      "[Training Epoch 0] Batch 3932, Loss 0.5148259997367859\n",
      "[Training Epoch 0] Batch 3933, Loss 0.48735907673835754\n",
      "[Training Epoch 0] Batch 3934, Loss 0.4977926015853882\n",
      "[Training Epoch 0] Batch 3935, Loss 0.5150455236434937\n",
      "[Training Epoch 0] Batch 3936, Loss 0.48988714814186096\n",
      "[Training Epoch 0] Batch 3937, Loss 0.518243670463562\n",
      "[Training Epoch 0] Batch 3938, Loss 0.45637863874435425\n",
      "[Training Epoch 0] Batch 3939, Loss 0.48484814167022705\n",
      "[Training Epoch 0] Batch 3940, Loss 0.528734564781189\n",
      "[Training Epoch 0] Batch 3941, Loss 0.49156734347343445\n",
      "[Training Epoch 0] Batch 3942, Loss 0.5225334763526917\n",
      "[Training Epoch 0] Batch 3943, Loss 0.49368366599082947\n",
      "[Training Epoch 0] Batch 3944, Loss 0.4573497474193573\n",
      "[Training Epoch 0] Batch 3945, Loss 0.5165372490882874\n",
      "[Training Epoch 0] Batch 3946, Loss 0.48713913559913635\n",
      "[Training Epoch 0] Batch 3947, Loss 0.4910252094268799\n",
      "[Training Epoch 0] Batch 3948, Loss 0.5153563618659973\n",
      "[Training Epoch 0] Batch 3949, Loss 0.48481181263923645\n",
      "[Training Epoch 0] Batch 3950, Loss 0.519890546798706\n",
      "[Training Epoch 0] Batch 3951, Loss 0.5088561773300171\n",
      "[Training Epoch 0] Batch 3952, Loss 0.5007318258285522\n",
      "[Training Epoch 0] Batch 3953, Loss 0.5276514887809753\n",
      "[Training Epoch 0] Batch 3954, Loss 0.5052100419998169\n",
      "[Training Epoch 0] Batch 3955, Loss 0.4929041862487793\n",
      "[Training Epoch 0] Batch 3956, Loss 0.5247107148170471\n",
      "[Training Epoch 0] Batch 3957, Loss 0.48564833402633667\n",
      "[Training Epoch 0] Batch 3958, Loss 0.5209733843803406\n",
      "[Training Epoch 0] Batch 3959, Loss 0.49675941467285156\n",
      "[Training Epoch 0] Batch 3960, Loss 0.5166974067687988\n",
      "[Training Epoch 0] Batch 3961, Loss 0.5007458925247192\n",
      "[Training Epoch 0] Batch 3962, Loss 0.4832226634025574\n",
      "[Training Epoch 0] Batch 3963, Loss 0.4671896696090698\n",
      "[Training Epoch 0] Batch 3964, Loss 0.4952061176300049\n",
      "[Training Epoch 0] Batch 3965, Loss 0.5059393048286438\n",
      "[Training Epoch 0] Batch 3966, Loss 0.493731826543808\n",
      "[Training Epoch 0] Batch 3967, Loss 0.5112687349319458\n",
      "[Training Epoch 0] Batch 3968, Loss 0.5045305490493774\n",
      "[Training Epoch 0] Batch 3969, Loss 0.534307062625885\n",
      "[Training Epoch 0] Batch 3970, Loss 0.4941960573196411\n",
      "[Training Epoch 0] Batch 3971, Loss 0.5306155681610107\n",
      "[Training Epoch 0] Batch 3972, Loss 0.5022692084312439\n",
      "[Training Epoch 0] Batch 3973, Loss 0.4951012432575226\n",
      "[Training Epoch 0] Batch 3974, Loss 0.47814488410949707\n",
      "[Training Epoch 0] Batch 3975, Loss 0.4951067268848419\n",
      "[Training Epoch 0] Batch 3976, Loss 0.468432754278183\n",
      "[Training Epoch 0] Batch 3977, Loss 0.4950929284095764\n",
      "[Training Epoch 0] Batch 3978, Loss 0.5076844692230225\n",
      "[Training Epoch 0] Batch 3979, Loss 0.5045605897903442\n",
      "[Training Epoch 0] Batch 3980, Loss 0.4684754014015198\n",
      "[Training Epoch 0] Batch 3981, Loss 0.5542723536491394\n",
      "[Training Epoch 0] Batch 3982, Loss 0.5049106478691101\n",
      "[Training Epoch 0] Batch 3983, Loss 0.5103591680526733\n",
      "[Training Epoch 0] Batch 3984, Loss 0.5142356157302856\n",
      "[Training Epoch 0] Batch 3985, Loss 0.5035929083824158\n",
      "[Training Epoch 0] Batch 3986, Loss 0.5196574926376343\n",
      "[Training Epoch 0] Batch 3987, Loss 0.49559086561203003\n",
      "[Training Epoch 0] Batch 3988, Loss 0.5359563231468201\n",
      "[Training Epoch 0] Batch 3989, Loss 0.4966737627983093\n",
      "[Training Epoch 0] Batch 3990, Loss 0.47372645139694214\n",
      "[Training Epoch 0] Batch 3991, Loss 0.5047407150268555\n",
      "[Training Epoch 0] Batch 3992, Loss 0.5032660365104675\n",
      "[Training Epoch 0] Batch 3993, Loss 0.48727744817733765\n",
      "[Training Epoch 0] Batch 3994, Loss 0.48193666338920593\n",
      "[Training Epoch 0] Batch 3995, Loss 0.49796801805496216\n",
      "[Training Epoch 0] Batch 3996, Loss 0.4874109923839569\n",
      "[Training Epoch 0] Batch 3997, Loss 0.48297926783561707\n",
      "[Training Epoch 0] Batch 3998, Loss 0.5061294436454773\n",
      "[Training Epoch 0] Batch 3999, Loss 0.501748263835907\n",
      "[Training Epoch 0] Batch 4000, Loss 0.5071067810058594\n",
      "[Training Epoch 0] Batch 4001, Loss 0.49779748916625977\n",
      "[Training Epoch 0] Batch 4002, Loss 0.5045158863067627\n",
      "[Training Epoch 0] Batch 4003, Loss 0.47641095519065857\n",
      "[Training Epoch 0] Batch 4004, Loss 0.47109317779541016\n",
      "[Training Epoch 0] Batch 4005, Loss 0.5160371661186218\n",
      "[Training Epoch 0] Batch 4006, Loss 0.4844505190849304\n",
      "[Training Epoch 0] Batch 4007, Loss 0.5100334882736206\n",
      "[Training Epoch 0] Batch 4008, Loss 0.5008694529533386\n",
      "[Training Epoch 0] Batch 4009, Loss 0.5059905648231506\n",
      "[Training Epoch 0] Batch 4010, Loss 0.4818316698074341\n",
      "[Training Epoch 0] Batch 4011, Loss 0.49369654059410095\n",
      "[Training Epoch 0] Batch 4012, Loss 0.5207486152648926\n",
      "[Training Epoch 0] Batch 4013, Loss 0.5021275281906128\n",
      "[Training Epoch 0] Batch 4014, Loss 0.5327122211456299\n",
      "[Training Epoch 0] Batch 4015, Loss 0.5303978323936462\n",
      "[Training Epoch 0] Batch 4016, Loss 0.5223633050918579\n",
      "[Training Epoch 0] Batch 4017, Loss 0.5047734379768372\n",
      "[Training Epoch 0] Batch 4018, Loss 0.48887088894844055\n",
      "[Training Epoch 0] Batch 4019, Loss 0.5023757815361023\n",
      "[Training Epoch 0] Batch 4020, Loss 0.5100671648979187\n",
      "[Training Epoch 0] Batch 4021, Loss 0.5194928646087646\n",
      "[Training Epoch 0] Batch 4022, Loss 0.526168942451477\n",
      "[Training Epoch 0] Batch 4023, Loss 0.47220614552497864\n",
      "[Training Epoch 0] Batch 4024, Loss 0.49654796719551086\n",
      "[Training Epoch 0] Batch 4025, Loss 0.49530157446861267\n",
      "[Training Epoch 0] Batch 4026, Loss 0.5063994526863098\n",
      "[Training Epoch 0] Batch 4027, Loss 0.49561938643455505\n",
      "[Training Epoch 0] Batch 4028, Loss 0.4885421693325043\n",
      "[Training Epoch 0] Batch 4029, Loss 0.49805963039398193\n",
      "[Training Epoch 0] Batch 4030, Loss 0.5142471790313721\n",
      "[Training Epoch 0] Batch 4031, Loss 0.507439136505127\n",
      "[Training Epoch 0] Batch 4032, Loss 0.5155093669891357\n",
      "[Training Epoch 0] Batch 4033, Loss 0.5155304074287415\n",
      "[Training Epoch 0] Batch 4034, Loss 0.5140501856803894\n",
      "[Training Epoch 0] Batch 4035, Loss 0.5181431174278259\n",
      "[Training Epoch 0] Batch 4036, Loss 0.5036261081695557\n",
      "[Training Epoch 0] Batch 4037, Loss 0.5046864151954651\n",
      "[Training Epoch 0] Batch 4038, Loss 0.4929116666316986\n",
      "[Training Epoch 0] Batch 4039, Loss 0.5287430286407471\n",
      "[Training Epoch 0] Batch 4040, Loss 0.5163271427154541\n",
      "[Training Epoch 0] Batch 4041, Loss 0.5223835110664368\n",
      "[Training Epoch 0] Batch 4042, Loss 0.5207306146621704\n",
      "[Training Epoch 0] Batch 4043, Loss 0.47077512741088867\n",
      "[Training Epoch 0] Batch 4044, Loss 0.4658876657485962\n",
      "[Training Epoch 0] Batch 4045, Loss 0.47351425886154175\n",
      "[Training Epoch 0] Batch 4046, Loss 0.477790504693985\n",
      "[Training Epoch 0] Batch 4047, Loss 0.502167284488678\n",
      "[Training Epoch 0] Batch 4048, Loss 0.5221309065818787\n",
      "[Training Epoch 0] Batch 4049, Loss 0.49534082412719727\n",
      "[Training Epoch 0] Batch 4050, Loss 0.5126773715019226\n",
      "[Training Epoch 0] Batch 4051, Loss 0.5046900510787964\n",
      "[Training Epoch 0] Batch 4052, Loss 0.5086154937744141\n",
      "[Training Epoch 0] Batch 4053, Loss 0.4821060001850128\n",
      "[Training Epoch 0] Batch 4054, Loss 0.4793895184993744\n",
      "[Training Epoch 0] Batch 4055, Loss 0.47005563974380493\n",
      "[Training Epoch 0] Batch 4056, Loss 0.5112054944038391\n",
      "[Training Epoch 0] Batch 4057, Loss 0.5007740259170532\n",
      "[Training Epoch 0] Batch 4058, Loss 0.5104401111602783\n",
      "[Training Epoch 0] Batch 4059, Loss 0.4884023368358612\n",
      "[Training Epoch 0] Batch 4060, Loss 0.5055813193321228\n",
      "[Training Epoch 0] Batch 4061, Loss 0.4801567792892456\n",
      "[Training Epoch 0] Batch 4062, Loss 0.528706967830658\n",
      "[Training Epoch 0] Batch 4063, Loss 0.5083867907524109\n",
      "[Training Epoch 0] Batch 4064, Loss 0.5061807632446289\n",
      "[Training Epoch 0] Batch 4065, Loss 0.4885109066963196\n",
      "[Training Epoch 0] Batch 4066, Loss 0.4953089952468872\n",
      "[Training Epoch 0] Batch 4067, Loss 0.5144827365875244\n",
      "[Training Epoch 0] Batch 4068, Loss 0.5152685642242432\n",
      "[Training Epoch 0] Batch 4069, Loss 0.547738254070282\n",
      "[Training Epoch 0] Batch 4070, Loss 0.5033751130104065\n",
      "[Training Epoch 0] Batch 4071, Loss 0.527976930141449\n",
      "[Training Epoch 0] Batch 4072, Loss 0.4887472093105316\n",
      "[Training Epoch 0] Batch 4073, Loss 0.5035818815231323\n",
      "[Training Epoch 0] Batch 4074, Loss 0.5233442783355713\n",
      "[Training Epoch 0] Batch 4075, Loss 0.5021264553070068\n",
      "[Training Epoch 0] Batch 4076, Loss 0.5035030841827393\n",
      "[Training Epoch 0] Batch 4077, Loss 0.48780497908592224\n",
      "[Training Epoch 0] Batch 4078, Loss 0.5050603151321411\n",
      "[Training Epoch 0] Batch 4079, Loss 0.4916769862174988\n",
      "[Training Epoch 0] Batch 4080, Loss 0.4993680417537689\n",
      "[Training Epoch 0] Batch 4081, Loss 0.48063355684280396\n",
      "[Training Epoch 0] Batch 4082, Loss 0.49815016984939575\n",
      "[Training Epoch 0] Batch 4083, Loss 0.49335891008377075\n",
      "[Training Epoch 0] Batch 4084, Loss 0.4914722144603729\n",
      "[Training Epoch 0] Batch 4085, Loss 0.49687594175338745\n",
      "[Training Epoch 0] Batch 4086, Loss 0.5024295449256897\n",
      "[Training Epoch 0] Batch 4087, Loss 0.5223957300186157\n",
      "[Training Epoch 0] Batch 4088, Loss 0.5329241752624512\n",
      "[Training Epoch 0] Batch 4089, Loss 0.49674952030181885\n",
      "[Training Epoch 0] Batch 4090, Loss 0.504526674747467\n",
      "[Training Epoch 0] Batch 4091, Loss 0.49113309383392334\n",
      "[Training Epoch 0] Batch 4092, Loss 0.5008081197738647\n",
      "[Training Epoch 0] Batch 4093, Loss 0.523740291595459\n",
      "[Training Epoch 0] Batch 4094, Loss 0.5210822820663452\n",
      "[Training Epoch 0] Batch 4095, Loss 0.4748636782169342\n",
      "[Training Epoch 0] Batch 4096, Loss 0.4966610372066498\n",
      "[Training Epoch 0] Batch 4097, Loss 0.5128071904182434\n",
      "[Training Epoch 0] Batch 4098, Loss 0.48439621925354004\n",
      "[Training Epoch 0] Batch 4099, Loss 0.5220741033554077\n",
      "[Training Epoch 0] Batch 4100, Loss 0.49388387799263\n",
      "[Training Epoch 0] Batch 4101, Loss 0.5102909207344055\n",
      "[Training Epoch 0] Batch 4102, Loss 0.5010903477668762\n",
      "[Training Epoch 0] Batch 4103, Loss 0.4765886068344116\n",
      "[Training Epoch 0] Batch 4104, Loss 0.48960840702056885\n",
      "[Training Epoch 0] Batch 4105, Loss 0.503253698348999\n",
      "[Training Epoch 0] Batch 4106, Loss 0.4737318456172943\n",
      "[Training Epoch 0] Batch 4107, Loss 0.49250006675720215\n",
      "[Training Epoch 0] Batch 4108, Loss 0.49259358644485474\n",
      "[Training Epoch 0] Batch 4109, Loss 0.5251118540763855\n",
      "[Training Epoch 0] Batch 4110, Loss 0.48849886655807495\n",
      "[Training Epoch 0] Batch 4111, Loss 0.503449022769928\n",
      "[Training Epoch 0] Batch 4112, Loss 0.47495079040527344\n",
      "[Training Epoch 0] Batch 4113, Loss 0.5170761942863464\n",
      "[Training Epoch 0] Batch 4114, Loss 0.49909907579421997\n",
      "[Training Epoch 0] Batch 4115, Loss 0.5156359076499939\n",
      "[Training Epoch 0] Batch 4116, Loss 0.5062326192855835\n",
      "[Training Epoch 0] Batch 4117, Loss 0.4991266429424286\n",
      "[Training Epoch 0] Batch 4118, Loss 0.4998014271259308\n",
      "[Training Epoch 0] Batch 4119, Loss 0.4589956998825073\n",
      "[Training Epoch 0] Batch 4120, Loss 0.4902375042438507\n",
      "[Training Epoch 0] Batch 4121, Loss 0.5178746581077576\n",
      "[Training Epoch 0] Batch 4122, Loss 0.4899633526802063\n",
      "[Training Epoch 0] Batch 4123, Loss 0.483177125453949\n",
      "[Training Epoch 0] Batch 4124, Loss 0.4965980648994446\n",
      "[Training Epoch 0] Batch 4125, Loss 0.48724308609962463\n",
      "[Training Epoch 0] Batch 4126, Loss 0.47802451252937317\n",
      "[Training Epoch 0] Batch 4127, Loss 0.5260984897613525\n",
      "[Training Epoch 0] Batch 4128, Loss 0.5247467756271362\n",
      "[Training Epoch 0] Batch 4129, Loss 0.5020925402641296\n",
      "[Training Epoch 0] Batch 4130, Loss 0.4912148118019104\n",
      "[Training Epoch 0] Batch 4131, Loss 0.5033552050590515\n",
      "[Training Epoch 0] Batch 4132, Loss 0.502118706703186\n",
      "[Training Epoch 0] Batch 4133, Loss 0.5130055546760559\n",
      "[Training Epoch 0] Batch 4134, Loss 0.4763747453689575\n",
      "[Training Epoch 0] Batch 4135, Loss 0.5089916586875916\n",
      "[Training Epoch 0] Batch 4136, Loss 0.4911990165710449\n",
      "[Training Epoch 0] Batch 4137, Loss 0.4737061560153961\n",
      "[Training Epoch 0] Batch 4138, Loss 0.49518659710884094\n",
      "[Training Epoch 0] Batch 4139, Loss 0.4950120747089386\n",
      "[Training Epoch 0] Batch 4140, Loss 0.4765501022338867\n",
      "[Training Epoch 0] Batch 4141, Loss 0.4967387914657593\n",
      "[Training Epoch 0] Batch 4142, Loss 0.47649431228637695\n",
      "[Training Epoch 0] Batch 4143, Loss 0.506039559841156\n",
      "[Training Epoch 0] Batch 4144, Loss 0.5008190274238586\n",
      "[Training Epoch 0] Batch 4145, Loss 0.4997011423110962\n",
      "[Training Epoch 0] Batch 4146, Loss 0.4925537407398224\n",
      "[Training Epoch 0] Batch 4147, Loss 0.4614619016647339\n",
      "[Training Epoch 0] Batch 4148, Loss 0.503513753414154\n",
      "[Training Epoch 0] Batch 4149, Loss 0.47618943452835083\n",
      "[Training Epoch 0] Batch 4150, Loss 0.4739297926425934\n",
      "[Training Epoch 0] Batch 4151, Loss 0.5126447677612305\n",
      "[Training Epoch 0] Batch 4152, Loss 0.50648033618927\n",
      "[Training Epoch 0] Batch 4153, Loss 0.47904497385025024\n",
      "[Training Epoch 0] Batch 4154, Loss 0.4737190008163452\n",
      "[Training Epoch 0] Batch 4155, Loss 0.5021648406982422\n",
      "[Training Epoch 0] Batch 4156, Loss 0.5101945400238037\n",
      "[Training Epoch 0] Batch 4157, Loss 0.4928027391433716\n",
      "[Training Epoch 0] Batch 4158, Loss 0.505984902381897\n",
      "[Training Epoch 0] Batch 4159, Loss 0.5045204162597656\n",
      "[Training Epoch 0] Batch 4160, Loss 0.4886694848537445\n",
      "[Training Epoch 0] Batch 4161, Loss 0.48838499188423157\n",
      "[Training Epoch 0] Batch 4162, Loss 0.500754714012146\n",
      "[Training Epoch 0] Batch 4163, Loss 0.5274550914764404\n",
      "[Training Epoch 0] Batch 4164, Loss 0.4900859594345093\n",
      "[Training Epoch 0] Batch 4165, Loss 0.5088068246841431\n",
      "[Training Epoch 0] Batch 4166, Loss 0.5250338912010193\n",
      "[Training Epoch 0] Batch 4167, Loss 0.5251515507698059\n",
      "[Training Epoch 0] Batch 4168, Loss 0.49635428190231323\n",
      "[Training Epoch 0] Batch 4169, Loss 0.49505528807640076\n",
      "[Training Epoch 0] Batch 4170, Loss 0.5126758217811584\n",
      "[Training Epoch 0] Batch 4171, Loss 0.4590127468109131\n",
      "[Training Epoch 0] Batch 4172, Loss 0.5196959972381592\n",
      "[Training Epoch 0] Batch 4173, Loss 0.49297598004341125\n",
      "[Training Epoch 0] Batch 4174, Loss 0.500896692276001\n",
      "[Training Epoch 0] Batch 4175, Loss 0.5059233903884888\n",
      "[Training Epoch 0] Batch 4176, Loss 0.5236639976501465\n",
      "[Training Epoch 0] Batch 4177, Loss 0.5211111307144165\n",
      "[Training Epoch 0] Batch 4178, Loss 0.49935856461524963\n",
      "[Training Epoch 0] Batch 4179, Loss 0.5075783133506775\n",
      "[Training Epoch 0] Batch 4180, Loss 0.5220729112625122\n",
      "[Training Epoch 0] Batch 4181, Loss 0.5236642956733704\n",
      "[Training Epoch 0] Batch 4182, Loss 0.5060834884643555\n",
      "[Training Epoch 0] Batch 4183, Loss 0.5114583373069763\n",
      "[Training Epoch 0] Batch 4184, Loss 0.5128185749053955\n",
      "[Training Epoch 0] Batch 4185, Loss 0.5211586952209473\n",
      "[Training Epoch 0] Batch 4186, Loss 0.5195720195770264\n",
      "[Training Epoch 0] Batch 4187, Loss 0.5195121169090271\n",
      "[Training Epoch 0] Batch 4188, Loss 0.5142128467559814\n",
      "[Training Epoch 0] Batch 4189, Loss 0.5065140724182129\n",
      "[Training Epoch 0] Batch 4190, Loss 0.5102901458740234\n",
      "[Training Epoch 0] Batch 4191, Loss 0.5005218386650085\n",
      "[Training Epoch 0] Batch 4192, Loss 0.508618950843811\n",
      "[Training Epoch 0] Batch 4193, Loss 0.5062139630317688\n",
      "[Training Epoch 0] Batch 4194, Loss 0.4967449903488159\n",
      "[Training Epoch 0] Batch 4195, Loss 0.4830572009086609\n",
      "[Training Epoch 0] Batch 4196, Loss 0.5046272277832031\n",
      "[Training Epoch 0] Batch 4197, Loss 0.4802985191345215\n",
      "[Training Epoch 0] Batch 4198, Loss 0.48995479941368103\n",
      "[Training Epoch 0] Batch 4199, Loss 0.46434158086776733\n",
      "[Training Epoch 0] Batch 4200, Loss 0.4687731862068176\n",
      "[Training Epoch 0] Batch 4201, Loss 0.5126771330833435\n",
      "[Training Epoch 0] Batch 4202, Loss 0.4898424446582794\n",
      "[Training Epoch 0] Batch 4203, Loss 0.5126189589500427\n",
      "[Training Epoch 0] Batch 4204, Loss 0.5220464468002319\n",
      "[Training Epoch 0] Batch 4205, Loss 0.5197167992591858\n",
      "[Training Epoch 0] Batch 4206, Loss 0.491294264793396\n",
      "[Training Epoch 0] Batch 4207, Loss 0.4901650846004486\n",
      "[Training Epoch 0] Batch 4208, Loss 0.48297804594039917\n",
      "[Training Epoch 0] Batch 4209, Loss 0.4817189574241638\n",
      "[Training Epoch 0] Batch 4210, Loss 0.5088951587677002\n",
      "[Training Epoch 0] Batch 4211, Loss 0.5032860636711121\n",
      "[Training Epoch 0] Batch 4212, Loss 0.5157138705253601\n",
      "[Training Epoch 0] Batch 4213, Loss 0.48738738894462585\n",
      "[Training Epoch 0] Batch 4214, Loss 0.49397045373916626\n",
      "[Training Epoch 0] Batch 4215, Loss 0.47386008501052856\n",
      "[Training Epoch 0] Batch 4216, Loss 0.4898998737335205\n",
      "[Training Epoch 0] Batch 4217, Loss 0.5249123573303223\n",
      "[Training Epoch 0] Batch 4218, Loss 0.4980095624923706\n",
      "[Training Epoch 0] Batch 4219, Loss 0.47659534215927124\n",
      "[Training Epoch 0] Batch 4220, Loss 0.4856794476509094\n",
      "[Training Epoch 0] Batch 4221, Loss 0.4899997413158417\n",
      "[Training Epoch 0] Batch 4222, Loss 0.49269169569015503\n",
      "[Training Epoch 0] Batch 4223, Loss 0.4861598014831543\n",
      "[Training Epoch 0] Batch 4224, Loss 0.4872272312641144\n",
      "[Training Epoch 0] Batch 4225, Loss 0.5007500052452087\n",
      "[Training Epoch 0] Batch 4226, Loss 0.49788111448287964\n",
      "[Training Epoch 0] Batch 4227, Loss 0.4884277284145355\n",
      "[Training Epoch 0] Batch 4228, Loss 0.5129871964454651\n",
      "[Training Epoch 0] Batch 4229, Loss 0.5048571228981018\n",
      "[Training Epoch 0] Batch 4230, Loss 0.5142363905906677\n",
      "[Training Epoch 0] Batch 4231, Loss 0.4670726954936981\n",
      "[Training Epoch 0] Batch 4232, Loss 0.4993802309036255\n",
      "[Training Epoch 0] Batch 4233, Loss 0.5043867826461792\n",
      "[Training Epoch 0] Batch 4234, Loss 0.5168721675872803\n",
      "[Training Epoch 0] Batch 4235, Loss 0.5099865198135376\n",
      "[Training Epoch 0] Batch 4236, Loss 0.46828386187553406\n",
      "[Training Epoch 0] Batch 4237, Loss 0.5020009279251099\n",
      "[Training Epoch 0] Batch 4238, Loss 0.4980694651603699\n",
      "[Training Epoch 0] Batch 4239, Loss 0.49943557381629944\n",
      "[Training Epoch 0] Batch 4240, Loss 0.5224269032478333\n",
      "[Training Epoch 0] Batch 4241, Loss 0.49271562695503235\n",
      "[Training Epoch 0] Batch 4242, Loss 0.47784510254859924\n",
      "[Training Epoch 0] Batch 4243, Loss 0.5181501507759094\n",
      "[Training Epoch 0] Batch 4244, Loss 0.4978999197483063\n",
      "[Training Epoch 0] Batch 4245, Loss 0.4804879426956177\n",
      "[Training Epoch 0] Batch 4246, Loss 0.4789866507053375\n",
      "[Training Epoch 0] Batch 4247, Loss 0.5127637386322021\n",
      "[Training Epoch 0] Batch 4248, Loss 0.48720306158065796\n",
      "[Training Epoch 0] Batch 4249, Loss 0.48995810747146606\n",
      "[Training Epoch 0] Batch 4250, Loss 0.5019678473472595\n",
      "[Training Epoch 0] Batch 4251, Loss 0.49776241183280945\n",
      "[Training Epoch 0] Batch 4252, Loss 0.49682584404945374\n",
      "[Training Epoch 0] Batch 4253, Loss 0.5073391795158386\n",
      "[Training Epoch 0] Batch 4254, Loss 0.49002787470817566\n",
      "[Training Epoch 0] Batch 4255, Loss 0.4939035475254059\n",
      "[Training Epoch 0] Batch 4256, Loss 0.4938400685787201\n",
      "[Training Epoch 0] Batch 4257, Loss 0.49111923575401306\n",
      "[Training Epoch 0] Batch 4258, Loss 0.472262978553772\n",
      "[Training Epoch 0] Batch 4259, Loss 0.5073535442352295\n",
      "[Training Epoch 0] Batch 4260, Loss 0.5413544178009033\n",
      "[Training Epoch 0] Batch 4261, Loss 0.4668203890323639\n",
      "[Training Epoch 0] Batch 4262, Loss 0.49137580394744873\n",
      "[Training Epoch 0] Batch 4263, Loss 0.501975953578949\n",
      "[Training Epoch 0] Batch 4264, Loss 0.5048123598098755\n",
      "[Training Epoch 0] Batch 4265, Loss 0.4696812629699707\n",
      "[Training Epoch 0] Batch 4266, Loss 0.48434725403785706\n",
      "[Training Epoch 0] Batch 4267, Loss 0.4815693795681\n",
      "[Training Epoch 0] Batch 4268, Loss 0.5157533288002014\n",
      "[Training Epoch 0] Batch 4269, Loss 0.5356664657592773\n",
      "[Training Epoch 0] Batch 4270, Loss 0.5088073015213013\n",
      "[Training Epoch 0] Batch 4271, Loss 0.4980727732181549\n",
      "[Training Epoch 0] Batch 4272, Loss 0.5029700398445129\n",
      "[Training Epoch 0] Batch 4273, Loss 0.4859324097633362\n",
      "[Training Epoch 0] Batch 4274, Loss 0.5208260416984558\n",
      "[Training Epoch 0] Batch 4275, Loss 0.4845104217529297\n",
      "[Training Epoch 0] Batch 4276, Loss 0.49662792682647705\n",
      "[Training Epoch 0] Batch 4277, Loss 0.5114305019378662\n",
      "[Training Epoch 0] Batch 4278, Loss 0.46837642788887024\n",
      "[Training Epoch 0] Batch 4279, Loss 0.5023038983345032\n",
      "[Training Epoch 0] Batch 4280, Loss 0.4886358380317688\n",
      "[Training Epoch 0] Batch 4281, Loss 0.5141948461532593\n",
      "[Training Epoch 0] Batch 4282, Loss 0.4924994111061096\n",
      "[Training Epoch 0] Batch 4283, Loss 0.4996950626373291\n",
      "[Training Epoch 0] Batch 4284, Loss 0.5088560581207275\n",
      "[Training Epoch 0] Batch 4285, Loss 0.49644872546195984\n",
      "[Training Epoch 0] Batch 4286, Loss 0.4982544779777527\n",
      "[Training Epoch 0] Batch 4287, Loss 0.5398744344711304\n",
      "[Training Epoch 0] Batch 4288, Loss 0.47776925563812256\n",
      "[Training Epoch 0] Batch 4289, Loss 0.5152134895324707\n",
      "[Training Epoch 0] Batch 4290, Loss 0.5086633563041687\n",
      "[Training Epoch 0] Batch 4291, Loss 0.5321050882339478\n",
      "[Training Epoch 0] Batch 4292, Loss 0.495190292596817\n",
      "[Training Epoch 0] Batch 4293, Loss 0.5263537168502808\n",
      "[Training Epoch 0] Batch 4294, Loss 0.5276306867599487\n",
      "[Training Epoch 0] Batch 4295, Loss 0.48854413628578186\n",
      "[Training Epoch 0] Batch 4296, Loss 0.4979441165924072\n",
      "[Training Epoch 0] Batch 4297, Loss 0.5155615210533142\n",
      "[Training Epoch 0] Batch 4298, Loss 0.5315831303596497\n",
      "[Training Epoch 0] Batch 4299, Loss 0.484335720539093\n",
      "[Training Epoch 0] Batch 4300, Loss 0.5138579607009888\n",
      "[Training Epoch 0] Batch 4301, Loss 0.4832741320133209\n",
      "[Training Epoch 0] Batch 4302, Loss 0.49124157428741455\n",
      "[Training Epoch 0] Batch 4303, Loss 0.48461082577705383\n",
      "[Training Epoch 0] Batch 4304, Loss 0.5001124143600464\n",
      "[Training Epoch 0] Batch 4305, Loss 0.5436484813690186\n",
      "[Training Epoch 0] Batch 4306, Loss 0.5076494812965393\n",
      "[Training Epoch 0] Batch 4307, Loss 0.510344922542572\n",
      "[Training Epoch 0] Batch 4308, Loss 0.5019010305404663\n",
      "[Training Epoch 0] Batch 4309, Loss 0.48028966784477234\n",
      "[Training Epoch 0] Batch 4310, Loss 0.49781912565231323\n",
      "[Training Epoch 0] Batch 4311, Loss 0.4639876186847687\n",
      "[Training Epoch 0] Batch 4312, Loss 0.5005719661712646\n",
      "[Training Epoch 0] Batch 4313, Loss 0.45993563532829285\n",
      "[Training Epoch 0] Batch 4314, Loss 0.5073535442352295\n",
      "[Training Epoch 0] Batch 4315, Loss 0.5088242888450623\n",
      "[Training Epoch 0] Batch 4316, Loss 0.5249796509742737\n",
      "[Training Epoch 0] Batch 4317, Loss 0.51024329662323\n",
      "[Training Epoch 0] Batch 4318, Loss 0.49655812978744507\n",
      "[Training Epoch 0] Batch 4319, Loss 0.48609021306037903\n",
      "[Training Epoch 0] Batch 4320, Loss 0.5104621648788452\n",
      "[Training Epoch 0] Batch 4321, Loss 0.49372339248657227\n",
      "[Training Epoch 0] Batch 4322, Loss 0.5171691179275513\n",
      "[Training Epoch 0] Batch 4323, Loss 0.5005851984024048\n",
      "[Training Epoch 0] Batch 4324, Loss 0.5143715739250183\n",
      "[Training Epoch 0] Batch 4325, Loss 0.5033703446388245\n",
      "[Training Epoch 0] Batch 4326, Loss 0.5112687349319458\n",
      "[Training Epoch 0] Batch 4327, Loss 0.49123600125312805\n",
      "[Training Epoch 0] Batch 4328, Loss 0.4902050495147705\n",
      "[Training Epoch 0] Batch 4329, Loss 0.5304587483406067\n",
      "[Training Epoch 0] Batch 4330, Loss 0.5035401582717896\n",
      "[Training Epoch 0] Batch 4331, Loss 0.5048620104789734\n",
      "[Training Epoch 0] Batch 4332, Loss 0.5008979439735413\n",
      "[Training Epoch 0] Batch 4333, Loss 0.49382200837135315\n",
      "[Training Epoch 0] Batch 4334, Loss 0.4915098249912262\n",
      "[Training Epoch 0] Batch 4335, Loss 0.5006226897239685\n",
      "[Training Epoch 0] Batch 4336, Loss 0.5103863477706909\n",
      "[Training Epoch 0] Batch 4337, Loss 0.4831525683403015\n",
      "[Training Epoch 0] Batch 4338, Loss 0.5199856758117676\n",
      "[Training Epoch 0] Batch 4339, Loss 0.4995534420013428\n",
      "[Training Epoch 0] Batch 4340, Loss 0.5021343231201172\n",
      "[Training Epoch 0] Batch 4341, Loss 0.5063846111297607\n",
      "[Training Epoch 0] Batch 4342, Loss 0.5157374739646912\n",
      "[Training Epoch 0] Batch 4343, Loss 0.48469075560569763\n",
      "[Training Epoch 0] Batch 4344, Loss 0.5017622113227844\n",
      "[Training Epoch 0] Batch 4345, Loss 0.5059266686439514\n",
      "[Training Epoch 0] Batch 4346, Loss 0.5126264095306396\n",
      "[Training Epoch 0] Batch 4347, Loss 0.4978841543197632\n",
      "[Training Epoch 0] Batch 4348, Loss 0.5059980750083923\n",
      "[Training Epoch 0] Batch 4349, Loss 0.5072451829910278\n",
      "[Training Epoch 0] Batch 4350, Loss 0.5153747797012329\n",
      "[Training Epoch 0] Batch 4351, Loss 0.521048367023468\n",
      "[Training Epoch 0] Batch 4352, Loss 0.5465773344039917\n",
      "[Training Epoch 0] Batch 4353, Loss 0.5195586681365967\n",
      "[Training Epoch 0] Batch 4354, Loss 0.5048103928565979\n",
      "[Training Epoch 0] Batch 4355, Loss 0.510394811630249\n",
      "[Training Epoch 0] Batch 4356, Loss 0.5321667194366455\n",
      "[Training Epoch 0] Batch 4357, Loss 0.497903972864151\n",
      "[Training Epoch 0] Batch 4358, Loss 0.48704731464385986\n",
      "[Training Epoch 0] Batch 4359, Loss 0.5049407482147217\n",
      "[Training Epoch 0] Batch 4360, Loss 0.499377578496933\n",
      "[Training Epoch 0] Batch 4361, Loss 0.48863765597343445\n",
      "[Training Epoch 0] Batch 4362, Loss 0.49395298957824707\n",
      "[Training Epoch 0] Batch 4363, Loss 0.5049059987068176\n",
      "[Training Epoch 0] Batch 4364, Loss 0.5170300602912903\n",
      "[Training Epoch 0] Batch 4365, Loss 0.491150438785553\n",
      "[Training Epoch 0] Batch 4366, Loss 0.5184848308563232\n",
      "[Training Epoch 0] Batch 4367, Loss 0.4885159730911255\n",
      "[Training Epoch 0] Batch 4368, Loss 0.5007150173187256\n",
      "[Training Epoch 0] Batch 4369, Loss 0.47770220041275024\n",
      "[Training Epoch 0] Batch 4370, Loss 0.4801495373249054\n",
      "[Training Epoch 0] Batch 4371, Loss 0.5130941271781921\n",
      "[Training Epoch 0] Batch 4372, Loss 0.49383360147476196\n",
      "[Training Epoch 0] Batch 4373, Loss 0.5155083537101746\n",
      "[Training Epoch 0] Batch 4374, Loss 0.5087477564811707\n",
      "[Training Epoch 0] Batch 4375, Loss 0.5303128361701965\n",
      "[Training Epoch 0] Batch 4376, Loss 0.518097996711731\n",
      "[Training Epoch 0] Batch 4377, Loss 0.47895923256874084\n",
      "[Training Epoch 0] Batch 4378, Loss 0.47126901149749756\n",
      "[Training Epoch 0] Batch 4379, Loss 0.47224628925323486\n",
      "[Training Epoch 0] Batch 4380, Loss 0.4939623475074768\n",
      "[Training Epoch 0] Batch 4381, Loss 0.5248897671699524\n",
      "[Training Epoch 0] Batch 4382, Loss 0.47075924277305603\n",
      "[Training Epoch 0] Batch 4383, Loss 0.5205942988395691\n",
      "[Training Epoch 0] Batch 4384, Loss 0.5088169574737549\n",
      "[Training Epoch 0] Batch 4385, Loss 0.5071696043014526\n",
      "[Training Epoch 0] Batch 4386, Loss 0.49802446365356445\n",
      "[Training Epoch 0] Batch 4387, Loss 0.5210753679275513\n",
      "[Training Epoch 0] Batch 4388, Loss 0.4858682453632355\n",
      "[Training Epoch 0] Batch 4389, Loss 0.5223127603530884\n",
      "[Training Epoch 0] Batch 4390, Loss 0.48434513807296753\n",
      "[Training Epoch 0] Batch 4391, Loss 0.5093820691108704\n",
      "[Training Epoch 0] Batch 4392, Loss 0.5180686712265015\n",
      "[Training Epoch 0] Batch 4393, Loss 0.5020470023155212\n",
      "[Training Epoch 0] Batch 4394, Loss 0.5141071081161499\n",
      "[Training Epoch 0] Batch 4395, Loss 0.48590564727783203\n",
      "[Training Epoch 0] Batch 4396, Loss 0.4927573800086975\n",
      "[Training Epoch 0] Batch 4397, Loss 0.515950083732605\n",
      "[Training Epoch 0] Batch 4398, Loss 0.5262585878372192\n",
      "[Training Epoch 0] Batch 4399, Loss 0.5099132657051086\n",
      "[Training Epoch 0] Batch 4400, Loss 0.5057022571563721\n",
      "[Training Epoch 0] Batch 4401, Loss 0.49805736541748047\n",
      "[Training Epoch 0] Batch 4402, Loss 0.4776133894920349\n",
      "[Training Epoch 0] Batch 4403, Loss 0.4953731596469879\n",
      "[Training Epoch 0] Batch 4404, Loss 0.4937986731529236\n",
      "[Training Epoch 0] Batch 4405, Loss 0.4844093918800354\n",
      "[Training Epoch 0] Batch 4406, Loss 0.4724939167499542\n",
      "[Training Epoch 0] Batch 4407, Loss 0.537453830242157\n",
      "[Training Epoch 0] Batch 4408, Loss 0.4910907447338104\n",
      "[Training Epoch 0] Batch 4409, Loss 0.5131708383560181\n",
      "[Training Epoch 0] Batch 4410, Loss 0.4710127115249634\n",
      "[Training Epoch 0] Batch 4411, Loss 0.5048185586929321\n",
      "[Training Epoch 0] Batch 4412, Loss 0.49818527698516846\n",
      "[Training Epoch 0] Batch 4413, Loss 0.47874632477760315\n",
      "[Training Epoch 0] Batch 4414, Loss 0.4869396686553955\n",
      "[Training Epoch 0] Batch 4415, Loss 0.5212891697883606\n",
      "[Training Epoch 0] Batch 4416, Loss 0.5221288800239563\n",
      "[Training Epoch 0] Batch 4417, Loss 0.49413976073265076\n",
      "[Training Epoch 0] Batch 4418, Loss 0.5353693962097168\n",
      "[Training Epoch 0] Batch 4419, Loss 0.5061807036399841\n",
      "[Training Epoch 0] Batch 4420, Loss 0.4907408654689789\n",
      "[Training Epoch 0] Batch 4421, Loss 0.4747125804424286\n",
      "[Training Epoch 0] Batch 4422, Loss 0.5046004056930542\n",
      "[Training Epoch 0] Batch 4423, Loss 0.5064270496368408\n",
      "[Training Epoch 0] Batch 4424, Loss 0.5331313014030457\n",
      "[Training Epoch 0] Batch 4425, Loss 0.5249389410018921\n",
      "[Training Epoch 0] Batch 4426, Loss 0.5099383592605591\n",
      "[Training Epoch 0] Batch 4427, Loss 0.4991653263568878\n",
      "[Training Epoch 0] Batch 4428, Loss 0.4964313507080078\n",
      "[Training Epoch 0] Batch 4429, Loss 0.5143105983734131\n",
      "[Training Epoch 0] Batch 4430, Loss 0.5239332914352417\n",
      "[Training Epoch 0] Batch 4431, Loss 0.4951619505882263\n",
      "[Training Epoch 0] Batch 4432, Loss 0.4953787326812744\n",
      "[Training Epoch 0] Batch 4433, Loss 0.5277181267738342\n",
      "[Training Epoch 0] Batch 4434, Loss 0.5141062140464783\n",
      "[Training Epoch 0] Batch 4435, Loss 0.4900851547718048\n",
      "[Training Epoch 0] Batch 4436, Loss 0.500800609588623\n",
      "[Training Epoch 0] Batch 4437, Loss 0.4913244843482971\n",
      "[Training Epoch 0] Batch 4438, Loss 0.5402240753173828\n",
      "[Training Epoch 0] Batch 4439, Loss 0.4997687339782715\n",
      "[Training Epoch 0] Batch 4440, Loss 0.5145831108093262\n",
      "[Training Epoch 0] Batch 4441, Loss 0.4847324788570404\n",
      "[Training Epoch 0] Batch 4442, Loss 0.4883590638637543\n",
      "[Training Epoch 0] Batch 4443, Loss 0.4859834313392639\n",
      "[Training Epoch 0] Batch 4444, Loss 0.5007758736610413\n",
      "[Training Epoch 0] Batch 4445, Loss 0.5290462970733643\n",
      "[Training Epoch 0] Batch 4446, Loss 0.49287039041519165\n",
      "[Training Epoch 0] Batch 4447, Loss 0.5102320909500122\n",
      "[Training Epoch 0] Batch 4448, Loss 0.48467791080474854\n",
      "[Training Epoch 0] Batch 4449, Loss 0.5005391240119934\n",
      "[Training Epoch 0] Batch 4450, Loss 0.4915657341480255\n",
      "[Training Epoch 0] Batch 4451, Loss 0.5100680589675903\n",
      "[Training Epoch 0] Batch 4452, Loss 0.4858475923538208\n",
      "[Training Epoch 0] Batch 4453, Loss 0.49527209997177124\n",
      "[Training Epoch 0] Batch 4454, Loss 0.5116606950759888\n",
      "[Training Epoch 0] Batch 4455, Loss 0.5021612048149109\n",
      "[Training Epoch 0] Batch 4456, Loss 0.49664291739463806\n",
      "[Training Epoch 0] Batch 4457, Loss 0.5153980255126953\n",
      "[Training Epoch 0] Batch 4458, Loss 0.49538755416870117\n",
      "[Training Epoch 0] Batch 4459, Loss 0.48033562302589417\n",
      "[Training Epoch 0] Batch 4460, Loss 0.4845131039619446\n",
      "[Training Epoch 0] Batch 4461, Loss 0.5006377100944519\n",
      "[Training Epoch 0] Batch 4462, Loss 0.5251001119613647\n",
      "[Training Epoch 0] Batch 4463, Loss 0.4993073344230652\n",
      "[Training Epoch 0] Batch 4464, Loss 0.5060637593269348\n",
      "[Training Epoch 0] Batch 4465, Loss 0.48973986506462097\n",
      "[Training Epoch 0] Batch 4466, Loss 0.4764845669269562\n",
      "[Training Epoch 0] Batch 4467, Loss 0.4979590177536011\n",
      "[Training Epoch 0] Batch 4468, Loss 0.5071816444396973\n",
      "[Training Epoch 0] Batch 4469, Loss 0.49802061915397644\n",
      "[Training Epoch 0] Batch 4470, Loss 0.5168198943138123\n",
      "[Training Epoch 0] Batch 4471, Loss 0.49672114849090576\n",
      "[Training Epoch 0] Batch 4472, Loss 0.49533385038375854\n",
      "[Training Epoch 0] Batch 4473, Loss 0.5020084977149963\n",
      "[Training Epoch 0] Batch 4474, Loss 0.51268070936203\n",
      "[Training Epoch 0] Batch 4475, Loss 0.4645194411277771\n",
      "[Training Epoch 0] Batch 4476, Loss 0.49655482172966003\n",
      "[Training Epoch 0] Batch 4477, Loss 0.4872090816497803\n",
      "[Training Epoch 0] Batch 4478, Loss 0.5289788246154785\n",
      "[Training Epoch 0] Batch 4479, Loss 0.4914478659629822\n",
      "[Training Epoch 0] Batch 4480, Loss 0.4804406464099884\n",
      "[Training Epoch 0] Batch 4481, Loss 0.5154768824577332\n",
      "[Training Epoch 0] Batch 4482, Loss 0.5046926140785217\n",
      "[Training Epoch 0] Batch 4483, Loss 0.5116467475891113\n",
      "[Training Epoch 0] Batch 4484, Loss 0.4870567321777344\n",
      "[Training Epoch 0] Batch 4485, Loss 0.5357760190963745\n",
      "[Training Epoch 0] Batch 4486, Loss 0.5169819593429565\n",
      "[Training Epoch 0] Batch 4487, Loss 0.5007013082504272\n",
      "[Training Epoch 0] Batch 4488, Loss 0.535782516002655\n",
      "[Training Epoch 0] Batch 4489, Loss 0.47770822048187256\n",
      "[Training Epoch 0] Batch 4490, Loss 0.500754714012146\n",
      "[Training Epoch 0] Batch 4491, Loss 0.5114428400993347\n",
      "[Training Epoch 0] Batch 4492, Loss 0.5318607091903687\n",
      "[Training Epoch 0] Batch 4493, Loss 0.49771273136138916\n",
      "[Training Epoch 0] Batch 4494, Loss 0.48573702573776245\n",
      "[Training Epoch 0] Batch 4495, Loss 0.4641133248806\n",
      "[Training Epoch 0] Batch 4496, Loss 0.48421233892440796\n",
      "[Training Epoch 0] Batch 4497, Loss 0.5005706548690796\n",
      "[Training Epoch 0] Batch 4498, Loss 0.48616987466812134\n",
      "[Training Epoch 0] Batch 4499, Loss 0.4939633309841156\n",
      "[Training Epoch 0] Batch 4500, Loss 0.5005264282226562\n",
      "[Training Epoch 0] Batch 4501, Loss 0.5264054536819458\n",
      "[Training Epoch 0] Batch 4502, Loss 0.5155455470085144\n",
      "[Training Epoch 0] Batch 4503, Loss 0.48418787121772766\n",
      "[Training Epoch 0] Batch 4504, Loss 0.4694947302341461\n",
      "[Training Epoch 0] Batch 4505, Loss 0.5194839239120483\n",
      "[Training Epoch 0] Batch 4506, Loss 0.51456618309021\n",
      "[Training Epoch 0] Batch 4507, Loss 0.5048853158950806\n",
      "[Training Epoch 0] Batch 4508, Loss 0.5328652262687683\n",
      "[Training Epoch 0] Batch 4509, Loss 0.4737006425857544\n",
      "[Training Epoch 0] Batch 4510, Loss 0.4792864918708801\n",
      "[Training Epoch 0] Batch 4511, Loss 0.561382532119751\n",
      "[Training Epoch 0] Batch 4512, Loss 0.4980134069919586\n",
      "[Training Epoch 0] Batch 4513, Loss 0.4954878091812134\n",
      "[Training Epoch 0] Batch 4514, Loss 0.4914413094520569\n",
      "[Training Epoch 0] Batch 4515, Loss 0.5004720687866211\n",
      "[Training Epoch 0] Batch 4516, Loss 0.495186448097229\n",
      "[Training Epoch 0] Batch 4517, Loss 0.48744022846221924\n",
      "[Training Epoch 0] Batch 4518, Loss 0.5222700238227844\n",
      "[Training Epoch 0] Batch 4519, Loss 0.4982542395591736\n",
      "[Training Epoch 0] Batch 4520, Loss 0.49156835675239563\n",
      "[Training Epoch 0] Batch 4521, Loss 0.5060701966285706\n",
      "[Training Epoch 0] Batch 4522, Loss 0.4707866609096527\n",
      "[Training Epoch 0] Batch 4523, Loss 0.4963380694389343\n",
      "[Training Epoch 0] Batch 4524, Loss 0.5141422748565674\n",
      "[Training Epoch 0] Batch 4525, Loss 0.5158893465995789\n",
      "[Training Epoch 0] Batch 4526, Loss 0.4967940151691437\n",
      "[Training Epoch 0] Batch 4527, Loss 0.5142843127250671\n",
      "[Training Epoch 0] Batch 4528, Loss 0.4913666546344757\n",
      "[Training Epoch 0] Batch 4529, Loss 0.4871894121170044\n",
      "[Training Epoch 0] Batch 4530, Loss 0.5019010901451111\n",
      "[Training Epoch 0] Batch 4531, Loss 0.47366708517074585\n",
      "[Training Epoch 0] Batch 4532, Loss 0.5097700953483582\n",
      "[Training Epoch 0] Batch 4533, Loss 0.5102171301841736\n",
      "[Training Epoch 0] Batch 4534, Loss 0.48456674814224243\n",
      "[Training Epoch 0] Batch 4535, Loss 0.4888210892677307\n",
      "[Training Epoch 0] Batch 4536, Loss 0.4749376177787781\n",
      "[Training Epoch 0] Batch 4537, Loss 0.5059858560562134\n",
      "[Training Epoch 0] Batch 4538, Loss 0.5022209286689758\n",
      "[Training Epoch 0] Batch 4539, Loss 0.5114066004753113\n",
      "[Training Epoch 0] Batch 4540, Loss 0.5167983174324036\n",
      "[Training Epoch 0] Batch 4541, Loss 0.4937075972557068\n",
      "[Training Epoch 0] Batch 4542, Loss 0.4613008201122284\n",
      "[Training Epoch 0] Batch 4543, Loss 0.4680621922016144\n",
      "[Training Epoch 0] Batch 4544, Loss 0.4868217408657074\n",
      "[Training Epoch 0] Batch 4545, Loss 0.47759610414505005\n",
      "[Training Epoch 0] Batch 4546, Loss 0.5073220729827881\n",
      "[Training Epoch 0] Batch 4547, Loss 0.5263862609863281\n",
      "[Training Epoch 0] Batch 4548, Loss 0.48577749729156494\n",
      "[Training Epoch 0] Batch 4549, Loss 0.49791470170021057\n",
      "[Training Epoch 0] Batch 4550, Loss 0.48583704233169556\n",
      "[Training Epoch 0] Batch 4551, Loss 0.5034210681915283\n",
      "[Training Epoch 0] Batch 4552, Loss 0.4680689871311188\n",
      "[Training Epoch 0] Batch 4553, Loss 0.5064905881881714\n",
      "[Training Epoch 0] Batch 4554, Loss 0.47351279854774475\n",
      "[Training Epoch 0] Batch 4555, Loss 0.52341628074646\n",
      "[Training Epoch 0] Batch 4556, Loss 0.5237733721733093\n",
      "[Training Epoch 0] Batch 4557, Loss 0.4926193058490753\n",
      "[Training Epoch 0] Batch 4558, Loss 0.5009315013885498\n",
      "[Training Epoch 0] Batch 4559, Loss 0.5076221823692322\n",
      "[Training Epoch 0] Batch 4560, Loss 0.5060541033744812\n",
      "[Training Epoch 0] Batch 4561, Loss 0.49891626834869385\n",
      "[Training Epoch 0] Batch 4562, Loss 0.48927178978919983\n",
      "[Training Epoch 0] Batch 4563, Loss 0.527815043926239\n",
      "[Training Epoch 0] Batch 4564, Loss 0.5031012296676636\n",
      "[Training Epoch 0] Batch 4565, Loss 0.5127196907997131\n",
      "[Training Epoch 0] Batch 4566, Loss 0.5192105174064636\n",
      "[Training Epoch 0] Batch 4567, Loss 0.5129469633102417\n",
      "[Training Epoch 0] Batch 4568, Loss 0.4817533493041992\n",
      "[Training Epoch 0] Batch 4569, Loss 0.5106542706489563\n",
      "[Training Epoch 0] Batch 4570, Loss 0.47068706154823303\n",
      "[Training Epoch 0] Batch 4571, Loss 0.4826333522796631\n",
      "[Training Epoch 0] Batch 4572, Loss 0.4954131245613098\n",
      "[Training Epoch 0] Batch 4573, Loss 0.4962643086910248\n",
      "[Training Epoch 0] Batch 4574, Loss 0.5155035257339478\n",
      "[Training Epoch 0] Batch 4575, Loss 0.5249173045158386\n",
      "[Training Epoch 0] Batch 4576, Loss 0.49169614911079407\n",
      "[Training Epoch 0] Batch 4577, Loss 0.5072691440582275\n",
      "[Training Epoch 0] Batch 4578, Loss 0.4911732077598572\n",
      "[Training Epoch 0] Batch 4579, Loss 0.4809330105781555\n",
      "[Training Epoch 0] Batch 4580, Loss 0.4753495454788208\n",
      "[Training Epoch 0] Batch 4581, Loss 0.5467371344566345\n",
      "[Training Epoch 0] Batch 4582, Loss 0.525809109210968\n",
      "[Training Epoch 0] Batch 4583, Loss 0.5029474496841431\n",
      "[Training Epoch 0] Batch 4584, Loss 0.5127742886543274\n",
      "[Training Epoch 0] Batch 4585, Loss 0.5038874745368958\n",
      "[Training Epoch 0] Batch 4586, Loss 0.5258763432502747\n",
      "[Training Epoch 0] Batch 4587, Loss 0.4754430949687958\n",
      "[Training Epoch 0] Batch 4588, Loss 0.5093177556991577\n",
      "[Training Epoch 0] Batch 4589, Loss 0.5154354572296143\n",
      "[Training Epoch 0] Batch 4590, Loss 0.4663943648338318\n",
      "[Training Epoch 0] Batch 4591, Loss 0.49663490056991577\n",
      "[Training Epoch 0] Batch 4592, Loss 0.5193386673927307\n",
      "[Training Epoch 0] Batch 4593, Loss 0.5163989067077637\n",
      "[Training Epoch 0] Batch 4594, Loss 0.4697790741920471\n",
      "[Training Epoch 0] Batch 4595, Loss 0.49501660466194153\n",
      "[Training Epoch 0] Batch 4596, Loss 0.4794003665447235\n",
      "[Training Epoch 0] Batch 4597, Loss 0.48946475982666016\n",
      "[Training Epoch 0] Batch 4598, Loss 0.5171492099761963\n",
      "[Training Epoch 0] Batch 4599, Loss 0.5220917463302612\n",
      "[Training Epoch 0] Batch 4600, Loss 0.49136999249458313\n",
      "[Training Epoch 0] Batch 4601, Loss 0.5251337885856628\n",
      "[Training Epoch 0] Batch 4602, Loss 0.5253384709358215\n",
      "[Training Epoch 0] Batch 4603, Loss 0.502358615398407\n",
      "[Training Epoch 0] Batch 4604, Loss 0.49246448278427124\n",
      "[Training Epoch 0] Batch 4605, Loss 0.5075727701187134\n",
      "[Training Epoch 0] Batch 4606, Loss 0.5265040993690491\n",
      "[Training Epoch 0] Batch 4607, Loss 0.4843272864818573\n",
      "[Training Epoch 0] Batch 4608, Loss 0.48334720730781555\n",
      "[Training Epoch 0] Batch 4609, Loss 0.48024415969848633\n",
      "[Training Epoch 0] Batch 4610, Loss 0.5277593731880188\n",
      "[Training Epoch 0] Batch 4611, Loss 0.4777231812477112\n",
      "[Training Epoch 0] Batch 4612, Loss 0.5104749798774719\n",
      "[Training Epoch 0] Batch 4613, Loss 0.4847105145454407\n",
      "[Training Epoch 0] Batch 4614, Loss 0.4886397123336792\n",
      "[Training Epoch 0] Batch 4615, Loss 0.488743394613266\n",
      "[Training Epoch 0] Batch 4616, Loss 0.5064003467559814\n",
      "[Training Epoch 0] Batch 4617, Loss 0.5060997009277344\n",
      "[Training Epoch 0] Batch 4618, Loss 0.5238372683525085\n",
      "[Training Epoch 0] Batch 4619, Loss 0.5140563249588013\n",
      "[Training Epoch 0] Batch 4620, Loss 0.510031521320343\n",
      "[Training Epoch 0] Batch 4621, Loss 0.49667561054229736\n",
      "[Training Epoch 0] Batch 4622, Loss 0.48566120862960815\n",
      "[Training Epoch 0] Batch 4623, Loss 0.5234453082084656\n",
      "[Training Epoch 0] Batch 4624, Loss 0.4966902732849121\n",
      "[Training Epoch 0] Batch 4625, Loss 0.5006714463233948\n",
      "[Training Epoch 0] Batch 4626, Loss 0.5234010815620422\n",
      "[Training Epoch 0] Batch 4627, Loss 0.4845646619796753\n",
      "[Training Epoch 0] Batch 4628, Loss 0.4897382855415344\n",
      "[Training Epoch 0] Batch 4629, Loss 0.5276079773902893\n",
      "[Training Epoch 0] Batch 4630, Loss 0.49387794733047485\n",
      "[Training Epoch 0] Batch 4631, Loss 0.5057063102722168\n",
      "[Training Epoch 0] Batch 4632, Loss 0.5118128061294556\n",
      "[Training Epoch 0] Batch 4633, Loss 0.4869900345802307\n",
      "[Training Epoch 0] Batch 4634, Loss 0.49372783303260803\n",
      "[Training Epoch 0] Batch 4635, Loss 0.5305010676383972\n",
      "[Training Epoch 0] Batch 4636, Loss 0.4966510236263275\n",
      "[Training Epoch 0] Batch 4637, Loss 0.5046575665473938\n",
      "[Training Epoch 0] Batch 4638, Loss 0.4950938820838928\n",
      "[Training Epoch 0] Batch 4639, Loss 0.5158299207687378\n",
      "[Training Epoch 0] Batch 4640, Loss 0.48059239983558655\n",
      "[Training Epoch 0] Batch 4641, Loss 0.4762146472930908\n",
      "[Training Epoch 0] Batch 4642, Loss 0.5048964023590088\n",
      "[Training Epoch 0] Batch 4643, Loss 0.5116961598396301\n",
      "[Training Epoch 0] Batch 4644, Loss 0.49608534574508667\n",
      "[Training Epoch 0] Batch 4645, Loss 0.5166976451873779\n",
      "[Training Epoch 0] Batch 4646, Loss 0.49299687147140503\n",
      "[Training Epoch 0] Batch 4647, Loss 0.4859546422958374\n",
      "[Training Epoch 0] Batch 4648, Loss 0.46968016028404236\n",
      "[Training Epoch 0] Batch 4649, Loss 0.4976845383644104\n",
      "[Training Epoch 0] Batch 4650, Loss 0.5159016847610474\n",
      "[Training Epoch 0] Batch 4651, Loss 0.5167236328125\n",
      "[Training Epoch 0] Batch 4652, Loss 0.5263397097587585\n",
      "[Training Epoch 0] Batch 4653, Loss 0.5117062926292419\n",
      "[Training Epoch 0] Batch 4654, Loss 0.4913279116153717\n",
      "[Training Epoch 0] Batch 4655, Loss 0.5140503644943237\n",
      "[Training Epoch 0] Batch 4656, Loss 0.4916548728942871\n",
      "[Training Epoch 0] Batch 4657, Loss 0.5224255919456482\n",
      "[Training Epoch 0] Batch 4658, Loss 0.5085611939430237\n",
      "[Training Epoch 0] Batch 4659, Loss 0.5303288698196411\n",
      "[Training Epoch 0] Batch 4660, Loss 0.5139425992965698\n",
      "[Training Epoch 0] Batch 4661, Loss 0.5411909818649292\n",
      "[Training Epoch 0] Batch 4662, Loss 0.49800655245780945\n",
      "[Training Epoch 0] Batch 4663, Loss 0.49368441104888916\n",
      "[Training Epoch 0] Batch 4664, Loss 0.5061924457550049\n",
      "[Training Epoch 0] Batch 4665, Loss 0.5138815641403198\n",
      "[Training Epoch 0] Batch 4666, Loss 0.5051279664039612\n",
      "[Training Epoch 0] Batch 4667, Loss 0.5207613110542297\n",
      "[Training Epoch 0] Batch 4668, Loss 0.5064612627029419\n",
      "[Training Epoch 0] Batch 4669, Loss 0.4777679443359375\n",
      "[Training Epoch 0] Batch 4670, Loss 0.5064586400985718\n",
      "[Training Epoch 0] Batch 4671, Loss 0.5144868493080139\n",
      "[Training Epoch 0] Batch 4672, Loss 0.49823036789894104\n",
      "[Training Epoch 0] Batch 4673, Loss 0.49389615654945374\n",
      "[Training Epoch 0] Batch 4674, Loss 0.5003497004508972\n",
      "[Training Epoch 0] Batch 4675, Loss 0.5143666863441467\n",
      "[Training Epoch 0] Batch 4676, Loss 0.49785494804382324\n",
      "[Training Epoch 0] Batch 4677, Loss 0.5251871347427368\n",
      "[Training Epoch 0] Batch 4678, Loss 0.4971977174282074\n",
      "[Training Epoch 0] Batch 4679, Loss 0.5019200444221497\n",
      "[Training Epoch 0] Batch 4680, Loss 0.5602183938026428\n",
      "[Training Epoch 0] Batch 4681, Loss 0.497739315032959\n",
      "[Training Epoch 0] Batch 4682, Loss 0.47237056493759155\n",
      "[Training Epoch 0] Batch 4683, Loss 0.49684637784957886\n",
      "[Training Epoch 0] Batch 4684, Loss 0.49672505259513855\n",
      "[Training Epoch 0] Batch 4685, Loss 0.4903835654258728\n",
      "[Training Epoch 0] Batch 4686, Loss 0.4912755489349365\n",
      "[Training Epoch 0] Batch 4687, Loss 0.5030534267425537\n",
      "[Training Epoch 0] Batch 4688, Loss 0.48829931020736694\n",
      "[Training Epoch 0] Batch 4689, Loss 0.48832014203071594\n",
      "[Training Epoch 0] Batch 4690, Loss 0.5204832553863525\n",
      "[Training Epoch 0] Batch 4691, Loss 0.5183691382408142\n",
      "[Training Epoch 0] Batch 4692, Loss 0.5220743417739868\n",
      "[Training Epoch 0] Batch 4693, Loss 0.5171051025390625\n",
      "[Training Epoch 0] Batch 4694, Loss 0.48333439230918884\n",
      "[Training Epoch 0] Batch 4695, Loss 0.5038147568702698\n",
      "[Training Epoch 0] Batch 4696, Loss 0.5208970904350281\n",
      "[Training Epoch 0] Batch 4697, Loss 0.4807088077068329\n",
      "[Training Epoch 0] Batch 4698, Loss 0.5033194422721863\n",
      "[Training Epoch 0] Batch 4699, Loss 0.44945216178894043\n",
      "[Training Epoch 0] Batch 4700, Loss 0.5007708668708801\n",
      "[Training Epoch 0] Batch 4701, Loss 0.47479668259620667\n",
      "[Training Epoch 0] Batch 4702, Loss 0.48326969146728516\n",
      "[Training Epoch 0] Batch 4703, Loss 0.4565802216529846\n",
      "[Training Epoch 0] Batch 4704, Loss 0.5047915577888489\n",
      "[Training Epoch 0] Batch 4705, Loss 0.5300072431564331\n",
      "[Training Epoch 0] Batch 4706, Loss 0.4681084454059601\n",
      "[Training Epoch 0] Batch 4707, Loss 0.5278461575508118\n",
      "[Training Epoch 0] Batch 4708, Loss 0.49797242879867554\n",
      "[Training Epoch 0] Batch 4709, Loss 0.5399129986763\n",
      "[Training Epoch 0] Batch 4710, Loss 0.46172207593917847\n",
      "[Training Epoch 0] Batch 4711, Loss 0.5100016593933105\n",
      "[Training Epoch 0] Batch 4712, Loss 0.5072383880615234\n",
      "[Training Epoch 0] Batch 4713, Loss 0.5210642218589783\n",
      "[Training Epoch 0] Batch 4714, Loss 0.49648573994636536\n",
      "[Training Epoch 0] Batch 4715, Loss 0.5158985257148743\n",
      "[Training Epoch 0] Batch 4716, Loss 0.48056572675704956\n",
      "[Training Epoch 0] Batch 4717, Loss 0.5030848979949951\n",
      "[Training Epoch 0] Batch 4718, Loss 0.512898862361908\n",
      "[Training Epoch 0] Batch 4719, Loss 0.4777773916721344\n",
      "[Training Epoch 0] Batch 4720, Loss 0.5140511393547058\n",
      "[Training Epoch 0] Batch 4721, Loss 0.5047071576118469\n",
      "[Training Epoch 0] Batch 4722, Loss 0.49792325496673584\n",
      "[Training Epoch 0] Batch 4723, Loss 0.47600480914115906\n",
      "[Training Epoch 0] Batch 4724, Loss 0.494365394115448\n",
      "[Training Epoch 0] Batch 4725, Loss 0.48719969391822815\n",
      "[Training Epoch 0] Batch 4726, Loss 0.5140001773834229\n",
      "[Training Epoch 0] Batch 4727, Loss 0.5208463668823242\n",
      "[Training Epoch 0] Batch 4728, Loss 0.48745229840278625\n",
      "[Training Epoch 0] Batch 4729, Loss 0.5223854184150696\n",
      "[Training Epoch 0] Batch 4730, Loss 0.49277734756469727\n",
      "[Training Epoch 0] Batch 4731, Loss 0.4726649224758148\n",
      "[Training Epoch 0] Batch 4732, Loss 0.5222263336181641\n",
      "[Training Epoch 0] Batch 4733, Loss 0.48447924852371216\n",
      "[Training Epoch 0] Batch 4734, Loss 0.4872719943523407\n",
      "[Training Epoch 0] Batch 4735, Loss 0.5129048824310303\n",
      "[Training Epoch 0] Batch 4736, Loss 0.5100952982902527\n",
      "[Training Epoch 0] Batch 4737, Loss 0.48994022607803345\n",
      "[Training Epoch 0] Batch 4738, Loss 0.5143461227416992\n",
      "[Training Epoch 0] Batch 4739, Loss 0.5263228416442871\n",
      "[Training Epoch 0] Batch 4740, Loss 0.4844546318054199\n",
      "[Training Epoch 0] Batch 4741, Loss 0.4966539144515991\n",
      "[Training Epoch 0] Batch 4742, Loss 0.5275691151618958\n",
      "[Training Epoch 0] Batch 4743, Loss 0.5192072987556458\n",
      "[Training Epoch 0] Batch 4744, Loss 0.4967726469039917\n",
      "[Training Epoch 0] Batch 4745, Loss 0.5275806188583374\n",
      "[Training Epoch 0] Batch 4746, Loss 0.4911240041255951\n",
      "[Training Epoch 0] Batch 4747, Loss 0.5101838111877441\n",
      "[Training Epoch 0] Batch 4748, Loss 0.4762135148048401\n",
      "[Training Epoch 0] Batch 4749, Loss 0.5235876441001892\n",
      "[Training Epoch 0] Batch 4750, Loss 0.4939306080341339\n",
      "[Training Epoch 0] Batch 4751, Loss 0.492308109998703\n",
      "[Training Epoch 0] Batch 4752, Loss 0.49611660838127136\n",
      "[Training Epoch 0] Batch 4753, Loss 0.5047289133071899\n",
      "[Training Epoch 0] Batch 4754, Loss 0.5110586881637573\n",
      "[Training Epoch 0] Batch 4755, Loss 0.516938328742981\n",
      "[Training Epoch 0] Batch 4756, Loss 0.5127707719802856\n",
      "[Training Epoch 0] Batch 4757, Loss 0.5413181185722351\n",
      "[Training Epoch 0] Batch 4758, Loss 0.5238086581230164\n",
      "[Training Epoch 0] Batch 4759, Loss 0.4993807077407837\n",
      "[Training Epoch 0] Batch 4760, Loss 0.5251601338386536\n",
      "[Training Epoch 0] Batch 4761, Loss 0.4940037131309509\n",
      "[Training Epoch 0] Batch 4762, Loss 0.48588991165161133\n",
      "[Training Epoch 0] Batch 4763, Loss 0.5183393359184265\n",
      "[Training Epoch 0] Batch 4764, Loss 0.4953664243221283\n",
      "[Training Epoch 0] Batch 4765, Loss 0.5275574922561646\n",
      "[Training Epoch 0] Batch 4766, Loss 0.5060542821884155\n",
      "[Training Epoch 0] Batch 4767, Loss 0.5007297992706299\n",
      "[Training Epoch 0] Batch 4768, Loss 0.5152963399887085\n",
      "[Training Epoch 0] Batch 4769, Loss 0.49357694387435913\n",
      "[Training Epoch 0] Batch 4770, Loss 0.4914379119873047\n",
      "[Training Epoch 0] Batch 4771, Loss 0.4658464789390564\n",
      "[Training Epoch 0] Batch 4772, Loss 0.501991868019104\n",
      "[Training Epoch 0] Batch 4773, Loss 0.4926891624927521\n",
      "[Training Epoch 0] Batch 4774, Loss 0.49505093693733215\n",
      "[Training Epoch 0] Batch 4775, Loss 0.5192006826400757\n",
      "[Training Epoch 0] Batch 4776, Loss 0.4950951933860779\n",
      "[Training Epoch 0] Batch 4777, Loss 0.5078424215316772\n",
      "[Training Epoch 0] Batch 4778, Loss 0.5072023272514343\n",
      "[Training Epoch 0] Batch 4779, Loss 0.5087637901306152\n",
      "[Training Epoch 0] Batch 4780, Loss 0.4859851598739624\n",
      "[Training Epoch 0] Batch 4781, Loss 0.49441012740135193\n",
      "[Training Epoch 0] Batch 4782, Loss 0.5158441662788391\n",
      "[Training Epoch 0] Batch 4783, Loss 0.48853954672813416\n",
      "[Training Epoch 0] Batch 4784, Loss 0.5234713554382324\n",
      "[Training Epoch 0] Batch 4785, Loss 0.4871447682380676\n",
      "[Training Epoch 0] Batch 4786, Loss 0.48731186985969543\n",
      "[Training Epoch 0] Batch 4787, Loss 0.4979614019393921\n",
      "[Training Epoch 0] Batch 4788, Loss 0.4816688895225525\n",
      "[Training Epoch 0] Batch 4789, Loss 0.502968430519104\n",
      "[Training Epoch 0] Batch 4790, Loss 0.4751060903072357\n",
      "[Training Epoch 0] Batch 4791, Loss 0.521984875202179\n",
      "[Training Epoch 0] Batch 4792, Loss 0.4763558506965637\n",
      "[Training Epoch 0] Batch 4793, Loss 0.5061764121055603\n",
      "[Training Epoch 0] Batch 4794, Loss 0.5169022679328918\n",
      "[Training Epoch 0] Batch 4795, Loss 0.5222471356391907\n",
      "[Training Epoch 0] Batch 4796, Loss 0.5061759948730469\n",
      "[Training Epoch 0] Batch 4797, Loss 0.5143511891365051\n",
      "[Training Epoch 0] Batch 4798, Loss 0.47649669647216797\n",
      "[Training Epoch 0] Batch 4799, Loss 0.5090027451515198\n",
      "[Training Epoch 0] Batch 4800, Loss 0.49662819504737854\n",
      "[Training Epoch 0] Batch 4801, Loss 0.5227289199829102\n",
      "[Training Epoch 0] Batch 4802, Loss 0.4847693145275116\n",
      "[Training Epoch 0] Batch 4803, Loss 0.5102148652076721\n",
      "[Training Epoch 0] Batch 4804, Loss 0.4887470304965973\n",
      "[Training Epoch 0] Batch 4805, Loss 0.4856407940387726\n",
      "[Training Epoch 0] Batch 4806, Loss 0.4997124671936035\n",
      "[Training Epoch 0] Batch 4807, Loss 0.5194380283355713\n",
      "[Training Epoch 0] Batch 4808, Loss 0.499439001083374\n",
      "[Training Epoch 0] Batch 4809, Loss 0.46899741888046265\n",
      "[Training Epoch 0] Batch 4810, Loss 0.5305643081665039\n",
      "[Training Epoch 0] Batch 4811, Loss 0.5366532802581787\n",
      "[Training Epoch 0] Batch 4812, Loss 0.48340386152267456\n",
      "[Training Epoch 0] Batch 4813, Loss 0.4817752540111542\n",
      "[Training Epoch 0] Batch 4814, Loss 0.48438000679016113\n",
      "[Training Epoch 0] Batch 4815, Loss 0.5159463286399841\n",
      "[Training Epoch 0] Batch 4816, Loss 0.5101821422576904\n",
      "[Training Epoch 0] Batch 4817, Loss 0.515641450881958\n",
      "[Training Epoch 0] Batch 4818, Loss 0.4888269305229187\n",
      "[Training Epoch 0] Batch 4819, Loss 0.4953036308288574\n",
      "[Training Epoch 0] Batch 4820, Loss 0.4575085937976837\n",
      "[Training Epoch 0] Batch 4821, Loss 0.4844651520252228\n",
      "[Training Epoch 0] Batch 4822, Loss 0.46845343708992004\n",
      "[Training Epoch 0] Batch 4823, Loss 0.5036796927452087\n",
      "[Training Epoch 0] Batch 4824, Loss 0.477569580078125\n",
      "[Training Epoch 0] Batch 4825, Loss 0.48739391565322876\n",
      "[Training Epoch 0] Batch 4826, Loss 0.4952915608882904\n",
      "[Training Epoch 0] Batch 4827, Loss 0.5049028396606445\n",
      "[Training Epoch 0] Batch 4828, Loss 0.4928280711174011\n",
      "[Training Epoch 0] Batch 4829, Loss 0.4871762692928314\n",
      "[Training Epoch 0] Batch 4830, Loss 0.5034539699554443\n",
      "[Training Epoch 0] Batch 4831, Loss 0.49102550745010376\n",
      "[Training Epoch 0] Batch 4832, Loss 0.49123087525367737\n",
      "[Training Epoch 0] Batch 4833, Loss 0.5061012506484985\n",
      "[Training Epoch 0] Batch 4834, Loss 0.48166802525520325\n",
      "[Training Epoch 0] Batch 4835, Loss 0.47505202889442444\n",
      "[Training Epoch 0] Batch 4836, Loss 0.5278792977333069\n",
      "[Training Epoch 0] Batch 4837, Loss 0.5075961351394653\n",
      "[Training Epoch 0] Batch 4838, Loss 0.5220634937286377\n",
      "[Training Epoch 0] Batch 4839, Loss 0.5197622776031494\n",
      "[Training Epoch 0] Batch 4840, Loss 0.4992113411426544\n",
      "[Training Epoch 0] Batch 4841, Loss 0.5050844550132751\n",
      "[Training Epoch 0] Batch 4842, Loss 0.4965800642967224\n",
      "[Training Epoch 0] Batch 4843, Loss 0.5141388177871704\n",
      "[Training Epoch 0] Batch 4844, Loss 0.5032720565795898\n",
      "[Training Epoch 0] Batch 4845, Loss 0.49374130368232727\n",
      "[Training Epoch 0] Batch 4846, Loss 0.5345858335494995\n",
      "[Training Epoch 0] Batch 4847, Loss 0.5020352602005005\n",
      "[Training Epoch 0] Batch 4848, Loss 0.5058990716934204\n",
      "[Training Epoch 0] Batch 4849, Loss 0.4790011942386627\n",
      "[Training Epoch 0] Batch 4850, Loss 0.4925442337989807\n",
      "[Training Epoch 0] Batch 4851, Loss 0.510138750076294\n",
      "[Training Epoch 0] Batch 4852, Loss 0.5141649842262268\n",
      "[Training Epoch 0] Batch 4853, Loss 0.4954230785369873\n",
      "[Training Epoch 0] Batch 4854, Loss 0.517113208770752\n",
      "[Evluating Epoch 0] HR = 0.0982, NDCG = 0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan\\Desktop\\GitHub_public\\neural-collaborative-filtering\\src\\metrics.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config['num_epoch']):\n",
    "    print('Epoch {} starts !'.format(epoch))\n",
    "    print('-' * 80)\n",
    "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
    "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "    engine.save(config['alias'], epoch, hit_ratio, ndcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
